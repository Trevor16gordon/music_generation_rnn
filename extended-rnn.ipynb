{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Disable all GPUS\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "visible_devices = tf.config.get_visible_devices()\n",
    "for device in visible_devices:\n",
    "    assert device.device_type != 'GPU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import pdb\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.basic_rnn import RNNMusicExperiment\n",
    "from src.utils.midi_support import MidiSupport, load_midi_objs, load_just_that_one_test_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trevorgordon/.venvs/sandbox_tf/lib/python3.8/site-packages/pretty_midi/pretty_midi.py:97: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# loaded_data = load_midi_objs(num_files=1, seq_length=1)\n",
    "loaded_data = load_just_that_one_test_song()\n",
    "note_vicinity = 24\n",
    "prepared_data = MidiSupport().prepare_song_note_invariant_plus_beats_and_more(loaded_data, vicinity=note_vicinity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'zip' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sw/31vsr7sd3dggcqkkgn59gsl80000gn/T/ipykernel_87118/2127923169.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequences2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhelper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'zip' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "zip(sequences, sequences2).map(helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 69\n",
    "X_tensor = tf.convert_to_tensor(prepared_data[0])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_tensor)\n",
    "# dataset_y = tf.data.Dataset.from_tensor_slices(y_tensor)\n",
    "# vocab_size = 128\n",
    "\n",
    "# Take 1 extra for the labels\n",
    "seq_length = seq_length + 1\n",
    "windows = dataset.window(seq_length,\n",
    "                            shift=1,\n",
    "                            stride=1,\n",
    "                            drop_remainder=True)\n",
    "\n",
    "sequences = windows.flat_map(lambda x: x.batch(seq_length, drop_remainder=True))\n",
    "\n",
    "# Split the labels\n",
    "def split_labels(sequences):\n",
    "    \n",
    "    inputs = sequences[:-1, :, :]\n",
    "    # Note 12:14 comes from architecture. first 24 elements are the vicinity. Label is in the middle\n",
    "    labels = sequences[-1, :, 12:14]\n",
    "    return inputs, labels\n",
    "\n",
    "seq_ds = sequences.map(split_labels,\n",
    "                        num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((69, 128, 53), (128, 2)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<TakeDataset shapes: (69, 128, 53), types: tf.int64>,\n",
       " <TakeDataset shapes: (1, 128, 53), types: tf.int64>)"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.take(1), sequences2.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to create a dataset preparer that will shift along the input sequence giving a multi to one predictor:\n",
    "\n",
    "def prepare_windowed(self, X, y, seq_length=15):\n",
    "    \"\"\"\n",
    "    X.shape should be ((time, num_notes, architecture)\n",
    "    y.shape should be ((time, num_notes, note_or_artic) # Not the y label is already shifted by one in time and corresponds to the 'next' note\n",
    "\n",
    "\n",
    "    returns:\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # all_song_dfs = all_song_dfs.T\n",
    "\n",
    "    print(f\"all_song_dfs.shape in is {all_song_dfs.shape}\")\n",
    "    X_tensor = tf.convert_to_tensor(X)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X_tensor)\n",
    "    vocab_size = 128\n",
    "\n",
    "    # Take 1 extra for the labels\n",
    "    windows = dataset.window(seq_length,\n",
    "                                shift=1,\n",
    "                                stride=1,\n",
    "                                drop_remainder=True)\n",
    "\n",
    "    # `flat_map` flattens the\" dataset of datasets\" into a dataset of tensors\n",
    "    flatten = lambda x: x.batch(seq_length, drop_remainder=True)\n",
    "    sequences = windows.flat_map(flatten)\n",
    "\n",
    "    # Normalize note pitch\n",
    "    def scale_pitch(x):\n",
    "        x = x / vocab_size\n",
    "        return x\n",
    "\n",
    "    # Split the labels\n",
    "    def split_labels(sequences, label_tensor):\n",
    "        inputs = sequences[:-1]\n",
    "        print(f\"sequences.shape is {sequences.shape}\")\n",
    "        # labels_dense = sequences[-1][0]\n",
    "        labels_dense = tf.reshape(sequences[-1][0:256], (256, 1))\n",
    "        return inputs, {\"pitch\": labels_dense}\n",
    "\n",
    "    seq_ds = sequences.map(split_labels,\n",
    "                            num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    X_tst, y_tst = list(seq_ds.take(1))[0]\n",
    "    print(f\"X_tst.shape out is {X_tst.shape}\")\n",
    "    print(f\"y_tst.shape out is {X_tst.shape}\")\n",
    "\n",
    "    return seq_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1206, 128, 53), (1206, 128, 2), numpy.ndarray)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_data[0].shape, prepared_data[1].shape, type(prepared_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwapLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(SwapLayer, self).__init__()\n",
    "\n",
    "    def call(self, orig_tensor, training=None):\n",
    "        print(f\" Swaplayer input tensor shape is {orig_tensor.get_shape()}\")\n",
    "        swapped = tf.transpose(orig_tensor, [1, 0, 2])\n",
    "        # swapped = orig_tensor\n",
    "        print(f\" Swaplayer output tensor shape is {swapped.get_shape()}\")\n",
    "        return swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetModelState(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        if batch % 4 == 0:\n",
    "            pass\n",
    "            # # print(\"...Training: end of batch {}\".format(batch))\n",
    "            # self.model.layers[1].reset_states()\n",
    "            # self.model.layers[5].reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_new(learning_rate=0.001, total_vicinity=28, dropout=0, recurrent_dropout=0):\n",
    "\n",
    "    elements_per_time_step = 4\n",
    "\n",
    "    # Time layers get input:\n",
    "    # [Batch(notes total 128 batches), beats(whatever our total song length is), architecture dimension]\n",
    "    input_shape = (elements_per_time_step, total_vicinity)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Time layers get input:\n",
    "    # Recursion along time dimension\n",
    "    # \" # time_inputs is a matrix (time, batch/note, input_per_note)\" \n",
    "    #                         =>  (batch/note, time, input_per_note)\n",
    "    #                         batch dimension is where all our extra data goes. Notes are added here\n",
    "    # For a batch_amount of notes, we take an arbitrarly length time segment and train. In the batch dimensions we have the other notes for this segment and the other segments\n",
    "    # [batch_amount of notes (should be 128), beats(arbitrary time sequence), architecture dimension]\n",
    "    model.add(tf.keras.Input(shape=input_shape, batch_size=4))\n",
    "    model.add(tf.keras.layers.LSTM(200,  return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout, stateful=True))\n",
    "    # model.add(tf.keras.layers.LSTM(200,  return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout, stateful=True))\n",
    "    model.add(SwapLayer())\n",
    "\n",
    "    # Note layers get input:\n",
    "    # Recursion along the note dimension\n",
    "    # \"# Transpose to be (note, batch/time, hidden_states)\"\n",
    "    #               =>  (batch/time, note, input_per_note)\n",
    "    #                  batch dimension is where all our extra data goes. All changes in time within temporal sequences and across different temporal sequences go here.\n",
    "    # For a single temporal sequence step, we take all notes and train. In the batch dimensions all other changes go there\n",
    "    # [beats(arbitrary time sequence), Batch(notes total 128 batches), 300 node time layer output]\n",
    "    # model.add(tf.keras.layers.LSTM(100,  return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout, stateful=True))\n",
    "    model.add(tf.keras.layers.LSTM(100,  return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout, stateful=True))\n",
    "    model.add(tf.keras.layers.Dense(2, activation=\"sigmoid\"))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    model.compile(\n",
    "            loss=loss,\n",
    "            optimizer=optimizer,\n",
    "            # metrics=[tf.keras.metrics.BinaryCrossentropy()\n",
    "            metrics=[\"mse\"\n",
    "            ]\n",
    "        )\n",
    "    return model, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_new_2(learning_rate=0.001, total_vicinity=28, dropout=0, recurrent_dropout=0):\n",
    "\n",
    "    elements_per_time_step = 128\n",
    "\n",
    "    # Time layers get input:\n",
    "    # [Batch(notes total 128 batches), beats(whatever our total song length is), architecture dimension]\n",
    "    input_shape = (elements_per_time_step, total_vicinity)\n",
    "    input_shape_b = (elements_per_time_step, 2)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Time layers get input:\n",
    "    # Recursion along time dimension\n",
    "    # Note from Daniel Johnson Code: \" # time_inputs is a matrix (time, batch/note, input_per_note)\" \n",
    "    #                 Tensorflow wants batch as 0th dimension=>  (batch/note, time, input_per_note)\n",
    "    #                         batch dimension is where all our extra data goes. Notes are added here\n",
    "    # For a batch_amount of notes, we take an arbitrarly length time segment and train. In the batch dimensions we have the other notes for this segment and the other segments\n",
    "    # [batch_amount of notes (should be 128), beats(arbitrary time sequence), architecture dimension]\n",
    "    input_a = tf.keras.Input(shape=input_shape,  name=\"input_a\")\n",
    "    input_b = tf.keras.Input(shape=input_shape_b, name=\"input_b\")\n",
    "\n",
    "    mod_time = SwapLayer()(input_a)\n",
    "    mod_time = tf.keras.layers.LSTM(200, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout, stateful=False)(mod_time)\n",
    "    mod_time = SwapLayer()(mod_time)\n",
    "    mod_time = tf.keras.Model(inputs=input_a, outputs=mod_time)\n",
    "    # model.add(tf.keras.layers.LSTM(200,  unroll=True, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout, stateful=True))\n",
    "    \n",
    "\n",
    "    combined = tf.keras.layers.concatenate([mod_time.output, input_b])\n",
    "\n",
    "    \n",
    "\n",
    "    mod_notes = tf.keras.layers.LSTM(100, return_sequences=True, dropout=dropout, stateful=False)(combined)\n",
    "    mod_notes = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(2, activation=\"sigmoid\"))(mod_notes)\n",
    "    # mod_notes = SwapLayer()(mod_notes)\n",
    "    \n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_a, input_b], outputs=[mod_notes])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    model.compile(\n",
    "            loss=loss,\n",
    "            optimizer=optimizer,\n",
    "            # metrics=[tf.keras.metrics.BinaryCrossentropy()\n",
    "            metrics=[\"mse\"\n",
    "            ]\n",
    "        )\n",
    "    return model, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements_per_time_step = 128\n",
    "\n",
    "input_shape = (128, 200)\n",
    "\n",
    "sub_model = tf.keras.Sequential()\n",
    "new_input = tf.keras.Input(shape=input_shape,  name=\"new_input\")\n",
    "old_input_b = model.layers[3]\n",
    "\n",
    "concat_lay = model.layers[4]\n",
    "concat_lay.input = [new_input, old_input_b]\n",
    "\n",
    "mod_notes = model.layers[5](concat_lay)\n",
    "mod_notes = model.layers[6](mod_notes)\n",
    "\n",
    "sub_model = tf.keras.Model(inputs=[new_input, old_input_b], outputs=[mod_notes])\n",
    "\n",
    "sub_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_new_2_debug(learning_rate=0.001, total_vicinity=28, dropout=0, recurrent_dropout=0):\n",
    "\n",
    "    input_shape_b = (128, 2)\n",
    "    input_b = tf.keras.Input(shape=input_shape_b, name=\"input_b\")\n",
    "    mod_notes = tf.keras.layers.LSTM(25, return_sequences=True, dropout=dropout, stateful=False)(input_b)\n",
    "    mod_notes = tf.keras.layers.LSTM(25, return_sequences=True, dropout=dropout, stateful=False)(mod_notes)\n",
    "    mod_notes = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(2, activation=\"sigmoid\"))(mod_notes)\n",
    "    # mod_notes = tf.keras.layers.Dense(2, activation=\"sigmoid\")(mod_notes)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_b], outputs=[mod_notes])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    model.compile(\n",
    "            loss=loss,\n",
    "            optimizer=optimizer,\n",
    "            # metrics=[tf.keras.metrics.BinaryCrossentropy()\n",
    "            metrics=[\"mse\"\n",
    "            ]\n",
    "        )\n",
    "    return model, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to swap time and note dimension\n",
    "\n",
    "# Need to take 128 in the time dimension at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147456, 2)\n"
     ]
    }
   ],
   "source": [
    "full_len = prepared_data[0].shape[0]\n",
    "even_mul_length = (full_len//128) * 128\n",
    "xxx = prepared_data[0][:even_mul_length, :, :]\n",
    "yyy = prepared_data[1][:even_mul_length, :, :]\n",
    "yyy_other = prepared_data[1][:even_mul_length, :, :].reshape((1152*128, 2))\n",
    "print(yyy_other.shape)\n",
    "yyy_other = np.concatenate([np.zeros((2,2)), yyy_other[2:, :]])\n",
    "yyy_other = yyy_other.reshape((1152, 128, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1152, 128, 2)\n",
      "(4608, 32, 2)\n",
      "(4608, 32, 2)\n"
     ]
    }
   ],
   "source": [
    "print(yyy.shape)\n",
    "yyy_2 = yyy.reshape((1152*4, 32, 2))\n",
    "yyy_other_2 = yyy_other.reshape((1152*4, 32, 2))\n",
    "print(yyy_2.shape)\n",
    "print(yyy_other_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(xxx_2[31, :, :]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1152, 128, 53)\n",
      "(4608, 32, 53)\n"
     ]
    }
   ],
   "source": [
    "print(xxx.shape)\n",
    "xxx_2 = xxx.reshape((1152*4, 32, 53))\n",
    "print(xxx_2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "9/9 [==============================] - 4s 483ms/step - loss: 0.0388 - mse: 0.0105\n",
      "Epoch 2/200\n",
      "9/9 [==============================] - 4s 487ms/step - loss: 0.0386 - mse: 0.0104\n",
      "Epoch 3/200\n",
      "9/9 [==============================] - 4s 458ms/step - loss: 0.0383 - mse: 0.0103\n",
      "Epoch 4/200\n",
      "9/9 [==============================] - 4s 482ms/step - loss: 0.0381 - mse: 0.0103\n",
      "Epoch 5/200\n",
      "9/9 [==============================] - 4s 474ms/step - loss: 0.0378 - mse: 0.0102\n",
      "Epoch 6/200\n",
      "9/9 [==============================] - 4s 480ms/step - loss: 0.0374 - mse: 0.0101\n",
      "Epoch 7/200\n",
      "9/9 [==============================] - 4s 461ms/step - loss: 0.0372 - mse: 0.0100\n",
      "Epoch 8/200\n",
      "9/9 [==============================] - 4s 453ms/step - loss: 0.0370 - mse: 0.0099\n",
      "Epoch 9/200\n",
      "9/9 [==============================] - 4s 485ms/step - loss: 0.0368 - mse: 0.0099\n",
      "Epoch 10/200\n",
      "9/9 [==============================] - 4s 472ms/step - loss: 0.0365 - mse: 0.0099\n",
      "Epoch 11/200\n",
      "9/9 [==============================] - 4s 486ms/step - loss: 0.0361 - mse: 0.0097\n",
      "Epoch 12/200\n",
      "9/9 [==============================] - 4s 473ms/step - loss: 0.0359 - mse: 0.0096\n",
      "Epoch 13/200\n",
      "9/9 [==============================] - 4s 462ms/step - loss: 0.0356 - mse: 0.0096\n",
      "Epoch 14/200\n",
      "9/9 [==============================] - 4s 462ms/step - loss: 0.0353 - mse: 0.0095\n",
      "Epoch 15/200\n",
      "9/9 [==============================] - 4s 474ms/step - loss: 0.0351 - mse: 0.0094\n",
      "Epoch 16/200\n",
      "9/9 [==============================] - 4s 449ms/step - loss: 0.0349 - mse: 0.0094\n",
      "Epoch 17/200\n",
      "9/9 [==============================] - 4s 498ms/step - loss: 0.0347 - mse: 0.0093\n",
      "Epoch 18/200\n",
      "9/9 [==============================] - 4s 476ms/step - loss: 0.0344 - mse: 0.0092\n",
      "Epoch 19/200\n",
      "9/9 [==============================] - 4s 493ms/step - loss: 0.0341 - mse: 0.0091\n",
      "Epoch 20/200\n",
      "9/9 [==============================] - 4s 467ms/step - loss: 0.0341 - mse: 0.0092\n",
      "Epoch 21/200\n",
      "9/9 [==============================] - 4s 485ms/step - loss: 0.0338 - mse: 0.0090\n",
      "Epoch 22/200\n",
      "9/9 [==============================] - 4s 486ms/step - loss: 0.0335 - mse: 0.0090\n",
      "Epoch 23/200\n",
      "9/9 [==============================] - 4s 476ms/step - loss: 0.0333 - mse: 0.0089\n",
      "Epoch 24/200\n",
      "9/9 [==============================] - 4s 462ms/step - loss: 0.0329 - mse: 0.0088\n",
      "Epoch 25/200\n",
      "9/9 [==============================] - 4s 475ms/step - loss: 0.0327 - mse: 0.0087\n",
      "Epoch 26/200\n",
      "9/9 [==============================] - 4s 497ms/step - loss: 0.0326 - mse: 0.0087\n",
      "Epoch 27/200\n",
      "9/9 [==============================] - 4s 480ms/step - loss: 0.0324 - mse: 0.0087\n",
      "Epoch 28/200\n",
      "9/9 [==============================] - 4s 477ms/step - loss: 0.0321 - mse: 0.0085\n",
      "Epoch 29/200\n",
      "9/9 [==============================] - 4s 491ms/step - loss: 0.0319 - mse: 0.0085\n",
      "Epoch 30/200\n",
      "9/9 [==============================] - 4s 493ms/step - loss: 0.0316 - mse: 0.0084\n",
      "Epoch 31/200\n",
      "9/9 [==============================] - 4s 484ms/step - loss: 0.0314 - mse: 0.0083\n",
      "Epoch 32/200\n",
      "9/9 [==============================] - 4s 498ms/step - loss: 0.0311 - mse: 0.0083\n",
      "Epoch 33/200\n",
      "9/9 [==============================] - 4s 474ms/step - loss: 0.0311 - mse: 0.0083\n",
      "Epoch 34/200\n",
      "9/9 [==============================] - 4s 493ms/step - loss: 0.0308 - mse: 0.0082\n",
      "Epoch 35/200\n",
      "9/9 [==============================] - 4s 472ms/step - loss: 0.0305 - mse: 0.0081\n",
      "Epoch 36/200\n",
      "9/9 [==============================] - 4s 453ms/step - loss: 0.0303 - mse: 0.0080\n",
      "Epoch 37/200\n",
      "9/9 [==============================] - 4s 460ms/step - loss: 0.0299 - mse: 0.0079\n",
      "Epoch 38/200\n",
      "9/9 [==============================] - 4s 476ms/step - loss: 0.0298 - mse: 0.0079\n",
      "Epoch 39/200\n",
      "9/9 [==============================] - 4s 499ms/step - loss: 0.0296 - mse: 0.0078\n",
      "Epoch 40/200\n",
      "9/9 [==============================] - 5s 511ms/step - loss: 0.0293 - mse: 0.0078\n",
      "Epoch 41/200\n",
      "9/9 [==============================] - 4s 479ms/step - loss: 0.0291 - mse: 0.0077\n",
      "Epoch 42/200\n",
      "9/9 [==============================] - 4s 470ms/step - loss: 0.0289 - mse: 0.0076\n",
      "Epoch 43/200\n",
      "9/9 [==============================] - 4s 462ms/step - loss: 0.0287 - mse: 0.0076\n",
      "Epoch 44/200\n",
      "9/9 [==============================] - 4s 461ms/step - loss: 0.0286 - mse: 0.0075\n",
      "Epoch 45/200\n",
      "9/9 [==============================] - 4s 466ms/step - loss: 0.0282 - mse: 0.0074\n",
      "Epoch 46/200\n",
      "9/9 [==============================] - 5s 499ms/step - loss: 0.0281 - mse: 0.0074\n",
      "Epoch 47/200\n",
      "9/9 [==============================] - 4s 469ms/step - loss: 0.0279 - mse: 0.0073\n",
      "Epoch 48/200\n",
      "9/9 [==============================] - 4s 498ms/step - loss: 0.0276 - mse: 0.0072\n",
      "Epoch 49/200\n",
      "9/9 [==============================] - 4s 468ms/step - loss: 0.0274 - mse: 0.0072\n",
      "Epoch 50/200\n",
      "9/9 [==============================] - 4s 467ms/step - loss: 0.0272 - mse: 0.0071\n",
      "Epoch 51/200\n",
      "9/9 [==============================] - 5s 511ms/step - loss: 0.0272 - mse: 0.0071\n",
      "Epoch 52/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sw/31vsr7sd3dggcqkkgn59gsl80000gn/T/ipykernel_25748/3891073431.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# model, callbakcs = model_new_2(total_vicinity=53, learning_rate=0.0001, dropout=0, recurrent_dropout=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m history = model.fit([xxx, yyy_other], [yyy],\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/sandbox_tf/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/sandbox_tf/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/sandbox_tf/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/sandbox_tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/sandbox_tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/sandbox_tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3128\u001b[0m       (graph_function,\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/sandbox_tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.venvs/sandbox_tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/sandbox_tf/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# model, callbakcs = model_new(total_vicinity=53, learning_rate=0.0001, dropout=0, recurrent_dropout=0)\n",
    "# model, callbakcs = model_new_2(total_vicinity=53, learning_rate=0.0001, dropout=0, recurrent_dropout=0)\n",
    "\n",
    "history = model.fit([xxx, yyy_other], [yyy],\n",
    "    epochs=200,\n",
    "    batch_size=128,\n",
    "    callbacks=[ResetModelState()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1152, 128, 53), (1152, 128, 2), (1152, 2))"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxx.shape, yyy_other.shape, yyy_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the whole thing!\n",
    "yyy_pred = model.predict([xxx[:512, :, :], yyy_other[:512, :, :]], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1152, 128, 2), (1152, 128, 2), (1152, 128, 2), (1152, 128, 2))"
      ]
     },
     "execution_count": 685,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yyy_other_rand = np.random.uniform(size=(1152, 128, 2))\n",
    "yyy_rand = np.random.uniform(size=(1152, 128, 2))\n",
    "yyy_other.shape, yyy.shape, yyy_other_rand.shape, yyy_rand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_87\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_b (InputLayer)        [(None, 128, 2)]          0         \n",
      "                                                                 \n",
      " lstm_161 (LSTM)             (None, 128, 25)           2800      \n",
      "                                                                 \n",
      " lstm_162 (LSTM)             (None, 128, 25)           5100      \n",
      "                                                                 \n",
      " time_distributed_32 (TimeDi  (None, 128, 2)           52        \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,952\n",
      "Trainable params: 7,952\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-10 17:40:30.112093: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-10 17:40:30.323738: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-10 17:40:30.558776: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-10 17:40:30.954848: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-10 17:40:31.317661: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 3s 103ms/step - loss: 0.6932 - mse: 0.0833\n"
     ]
    }
   ],
   "source": [
    "# model, _ = model_new_2_debug()\n",
    "\n",
    "# history = model.fit([np.random.uniform(size=(1152, 128, 2))], [np.random.uniform(size=(1152, 128, 2))],\n",
    "#     epochs=1,\n",
    "#     batch_size=128,\n",
    "#     callbacks=[ResetModelState()],\n",
    "# )\n",
    "\n",
    "# # history = model.fit([yyy_other], [yyy],\n",
    "# #     epochs=1,\n",
    "# #     batch_size=128,\n",
    "# #     callbacks=[ResetModelState()],\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 128, 2), (1152, 128, 2))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yyy_pred_128.shape, yyy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    10   11   12   13   14   \\\n",
       "0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "1    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   15   16   17   18   19   20   21   22   23   24   25   26   27   28   29   \\\n",
       "0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "1    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   30   31   32   33   34   35   36   37   38   39   40   41   42   43   44   \\\n",
       "0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "1    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   45   46   47   48   49   50   51   52   53   54   55   56   57   58   59   \\\n",
       "0    0    0    0    0    0    0    1    0    0    1    0    0    0    1    0   \n",
       "1    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   60   61   62   63   64   65   66   67   68   69   70   71   72   73   74   \\\n",
       "0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "1    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   75   76   77   78   79   80   81   82   83   84   85   86   87   88   89   \\\n",
       "0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "1    0    0    0    1    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   90   91   92   93   94   95   96   97   98   99   100  101  102  103  104  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "1    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "1    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   120  121  122  123  124  125  126  127  \n",
       "0    0    0    0    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0    0    0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "t = pd.DataFrame(yyy[127, :, :]).T\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003455</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>0.001099</td>\n",
       "      <td>0.001371</td>\n",
       "      <td>0.001620</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.002707</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.00325</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>0.003593</td>\n",
       "      <td>0.003653</td>\n",
       "      <td>0.003893</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.004233</td>\n",
       "      <td>0.004351</td>\n",
       "      <td>0.004775</td>\n",
       "      <td>0.005037</td>\n",
       "      <td>0.005745</td>\n",
       "      <td>0.006429</td>\n",
       "      <td>0.007825</td>\n",
       "      <td>0.009268</td>\n",
       "      <td>0.012610</td>\n",
       "      <td>0.015461</td>\n",
       "      <td>0.019474</td>\n",
       "      <td>0.771081</td>\n",
       "      <td>0.012818</td>\n",
       "      <td>0.023800</td>\n",
       "      <td>0.823598</td>\n",
       "      <td>0.013893</td>\n",
       "      <td>0.025136</td>\n",
       "      <td>0.029468</td>\n",
       "      <td>0.806719</td>\n",
       "      <td>0.011963</td>\n",
       "      <td>0.020853</td>\n",
       "      <td>0.024359</td>\n",
       "      <td>0.02333</td>\n",
       "      <td>0.020775</td>\n",
       "      <td>0.021442</td>\n",
       "      <td>0.019884</td>\n",
       "      <td>0.019658</td>\n",
       "      <td>0.018230</td>\n",
       "      <td>0.018131</td>\n",
       "      <td>0.017005</td>\n",
       "      <td>0.016957</td>\n",
       "      <td>0.015818</td>\n",
       "      <td>0.015885</td>\n",
       "      <td>0.014813</td>\n",
       "      <td>0.014653</td>\n",
       "      <td>0.013551</td>\n",
       "      <td>0.013303</td>\n",
       "      <td>0.01232</td>\n",
       "      <td>0.052666</td>\n",
       "      <td>0.009616</td>\n",
       "      <td>0.012735</td>\n",
       "      <td>0.013125</td>\n",
       "      <td>0.012946</td>\n",
       "      <td>0.011379</td>\n",
       "      <td>0.010342</td>\n",
       "      <td>0.008569</td>\n",
       "      <td>0.007170</td>\n",
       "      <td>0.005312</td>\n",
       "      <td>0.003897</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005437</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>0.00149</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.001842</td>\n",
       "      <td>0.001968</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.002323</td>\n",
       "      <td>0.002542</td>\n",
       "      <td>0.002913</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>0.004028</td>\n",
       "      <td>0.004874</td>\n",
       "      <td>0.006421</td>\n",
       "      <td>0.008129</td>\n",
       "      <td>0.010483</td>\n",
       "      <td>0.048248</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.012628</td>\n",
       "      <td>0.058450</td>\n",
       "      <td>0.010595</td>\n",
       "      <td>0.011812</td>\n",
       "      <td>0.013388</td>\n",
       "      <td>0.051362</td>\n",
       "      <td>0.008835</td>\n",
       "      <td>0.009499</td>\n",
       "      <td>0.010522</td>\n",
       "      <td>0.01040</td>\n",
       "      <td>0.009857</td>\n",
       "      <td>0.010077</td>\n",
       "      <td>0.009749</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.009416</td>\n",
       "      <td>0.009301</td>\n",
       "      <td>0.008806</td>\n",
       "      <td>0.008667</td>\n",
       "      <td>0.008195</td>\n",
       "      <td>0.007996</td>\n",
       "      <td>0.007569</td>\n",
       "      <td>0.007417</td>\n",
       "      <td>0.006968</td>\n",
       "      <td>0.006685</td>\n",
       "      <td>0.00616</td>\n",
       "      <td>0.322173</td>\n",
       "      <td>0.012857</td>\n",
       "      <td>0.010412</td>\n",
       "      <td>0.009373</td>\n",
       "      <td>0.008224</td>\n",
       "      <td>0.006838</td>\n",
       "      <td>0.005724</td>\n",
       "      <td>0.004457</td>\n",
       "      <td>0.003425</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.003455  0.000441  0.000202  0.000136  0.000113  0.000099  0.000094   \n",
       "1  0.005437  0.000594  0.000229  0.000129  0.000091  0.000074  0.000067   \n",
       "\n",
       "        7         8         9         10        11        12        13   \\\n",
       "0  0.000090  0.000093  0.000093  0.000097  0.000099  0.000106  0.000110   \n",
       "1  0.000063  0.000063  0.000065  0.000069  0.000073  0.000078  0.000083   \n",
       "\n",
       "        14        15        16        17        18        19        20   \\\n",
       "0  0.000119  0.000124  0.000143  0.000155  0.000177  0.000199  0.000241   \n",
       "1  0.000090  0.000097  0.000109  0.000119  0.000134  0.000150  0.000174   \n",
       "\n",
       "        21        22        23        24        25        26        27   \\\n",
       "0  0.000290  0.000371  0.000456  0.000595  0.000729  0.000922  0.001099   \n",
       "1  0.000203  0.000247  0.000299  0.000373  0.000456  0.000564  0.000667   \n",
       "\n",
       "        28        29        30        31        32        33       34   \\\n",
       "0  0.001371  0.001620  0.001972  0.002222  0.002707  0.002927  0.00325   \n",
       "1  0.000789  0.000896  0.001028  0.001124  0.001274  0.001366  0.00149   \n",
       "\n",
       "        35        36        37        38        39        40        41   \\\n",
       "0  0.003342  0.003593  0.003653  0.003893  0.003923  0.004233  0.004351   \n",
       "1  0.001553  0.001646  0.001687  0.001788  0.001842  0.001968  0.002087   \n",
       "\n",
       "        42        43        44        45        46        47        48   \\\n",
       "0  0.004775  0.005037  0.005745  0.006429  0.007825  0.009268  0.012610   \n",
       "1  0.002323  0.002542  0.002913  0.003319  0.004028  0.004874  0.006421   \n",
       "\n",
       "        49        50        51        52        53        54        55   \\\n",
       "0  0.015461  0.019474  0.771081  0.012818  0.023800  0.823598  0.013893   \n",
       "1  0.008129  0.010483  0.048248  0.010642  0.012628  0.058450  0.010595   \n",
       "\n",
       "        56        57        58        59        60        61       62   \\\n",
       "0  0.025136  0.029468  0.806719  0.011963  0.020853  0.024359  0.02333   \n",
       "1  0.011812  0.013388  0.051362  0.008835  0.009499  0.010522  0.01040   \n",
       "\n",
       "        63        64        65        66        67        68        69   \\\n",
       "0  0.020775  0.021442  0.019884  0.019658  0.018230  0.018131  0.017005   \n",
       "1  0.009857  0.010077  0.009749  0.009787  0.009416  0.009301  0.008806   \n",
       "\n",
       "        70        71        72        73        74        75        76   \\\n",
       "0  0.016957  0.015818  0.015885  0.014813  0.014653  0.013551  0.013303   \n",
       "1  0.008667  0.008195  0.007996  0.007569  0.007417  0.006968  0.006685   \n",
       "\n",
       "       77        78        79        80        81        82        83   \\\n",
       "0  0.01232  0.052666  0.009616  0.012735  0.013125  0.012946  0.011379   \n",
       "1  0.00616  0.322173  0.012857  0.010412  0.009373  0.008224  0.006838   \n",
       "\n",
       "        84        85        86        87        88        89        90   \\\n",
       "0  0.010342  0.008569  0.007170  0.005312  0.003897  0.002434  0.001464   \n",
       "1  0.005724  0.004457  0.003425  0.002389  0.001591  0.000949  0.000537   \n",
       "\n",
       "        91        92        93        94        95        96        97   \\\n",
       "0  0.000814  0.000501  0.000336  0.000263  0.000216  0.000204  0.000182   \n",
       "1  0.000285  0.000159  0.000098  0.000070  0.000055  0.000048  0.000043   \n",
       "\n",
       "        98        99        100       101       102       103       104  \\\n",
       "0  0.000168  0.000148  0.000133  0.000115  0.000104  0.000093  0.000091   \n",
       "1  0.000040  0.000038  0.000036  0.000034  0.000033  0.000032  0.000031   \n",
       "\n",
       "        105       106       107       108       109       110       111  \\\n",
       "0  0.000086  0.000085  0.000082  0.000083  0.000081  0.000082  0.000081   \n",
       "1  0.000030  0.000030  0.000030  0.000029  0.000029  0.000029  0.000029   \n",
       "\n",
       "        112       113       114       115       116       117       118  \\\n",
       "0  0.000086  0.000085  0.000087  0.000086  0.000088  0.000088  0.000090   \n",
       "1  0.000030  0.000030  0.000030  0.000030  0.000030  0.000030  0.000031   \n",
       "\n",
       "        119       120       121       122       123       124       125  \\\n",
       "0  0.000090  0.000094  0.000093  0.000096  0.000095  0.000099  0.000099   \n",
       "1  0.000031  0.000032  0.000032  0.000033  0.000033  0.000034  0.000034   \n",
       "\n",
       "        126       127  \n",
       "0  0.000103  0.000104  \n",
       "1  0.000035  0.000036  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting with part of the sequence as zero, This is reference\n",
    "yyy_other_test = np.copy(yyy_other[:128, :, :])\n",
    "yyy_pred_128 = model.predict([xxx[:128, :, :], yyy_other_test], batch_size=128)\n",
    "t1 = pd.DataFrame(yyy_pred_128[-1, :, :]).T\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.013516</td>\n",
       "      <td>0.001920</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.001742</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002855</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.003844</td>\n",
       "      <td>0.004266</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.005180</td>\n",
       "      <td>0.005686</td>\n",
       "      <td>0.005927</td>\n",
       "      <td>0.006330</td>\n",
       "      <td>0.006462</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>0.006884</td>\n",
       "      <td>0.007271</td>\n",
       "      <td>0.007441</td>\n",
       "      <td>0.007996</td>\n",
       "      <td>0.008410</td>\n",
       "      <td>0.009403</td>\n",
       "      <td>0.010432</td>\n",
       "      <td>0.012879</td>\n",
       "      <td>0.015649</td>\n",
       "      <td>0.020598</td>\n",
       "      <td>0.287240</td>\n",
       "      <td>0.981540</td>\n",
       "      <td>0.976948</td>\n",
       "      <td>0.976850</td>\n",
       "      <td>0.976348</td>\n",
       "      <td>0.976222</td>\n",
       "      <td>0.975877</td>\n",
       "      <td>0.975867</td>\n",
       "      <td>0.975297</td>\n",
       "      <td>0.974479</td>\n",
       "      <td>0.97219</td>\n",
       "      <td>0.968473</td>\n",
       "      <td>0.963202</td>\n",
       "      <td>0.959208</td>\n",
       "      <td>0.956486</td>\n",
       "      <td>0.955500</td>\n",
       "      <td>0.954275</td>\n",
       "      <td>0.953876</td>\n",
       "      <td>0.953130</td>\n",
       "      <td>0.953202</td>\n",
       "      <td>0.952538</td>\n",
       "      <td>0.952609</td>\n",
       "      <td>0.952060</td>\n",
       "      <td>0.95231</td>\n",
       "      <td>0.951746</td>\n",
       "      <td>0.951879</td>\n",
       "      <td>0.951508</td>\n",
       "      <td>0.951915</td>\n",
       "      <td>0.951496</td>\n",
       "      <td>0.951599</td>\n",
       "      <td>0.951208</td>\n",
       "      <td>0.951598</td>\n",
       "      <td>0.951123</td>\n",
       "      <td>0.951319</td>\n",
       "      <td>0.951007</td>\n",
       "      <td>0.951467</td>\n",
       "      <td>0.951085</td>\n",
       "      <td>0.951367</td>\n",
       "      <td>0.950986</td>\n",
       "      <td>0.951380</td>\n",
       "      <td>0.950927</td>\n",
       "      <td>0.951159</td>\n",
       "      <td>0.950881</td>\n",
       "      <td>0.951364</td>\n",
       "      <td>0.951007</td>\n",
       "      <td>0.951168</td>\n",
       "      <td>0.950827</td>\n",
       "      <td>0.951258</td>\n",
       "      <td>0.950816</td>\n",
       "      <td>0.951043</td>\n",
       "      <td>0.950763</td>\n",
       "      <td>0.951251</td>\n",
       "      <td>0.950891</td>\n",
       "      <td>0.951187</td>\n",
       "      <td>0.950822</td>\n",
       "      <td>0.951227</td>\n",
       "      <td>0.950786</td>\n",
       "      <td>0.951032</td>\n",
       "      <td>0.950769</td>\n",
       "      <td>0.951265</td>\n",
       "      <td>0.950917</td>\n",
       "      <td>0.951086</td>\n",
       "      <td>0.950755</td>\n",
       "      <td>0.951194</td>\n",
       "      <td>0.950760</td>\n",
       "      <td>0.950993</td>\n",
       "      <td>0.950722</td>\n",
       "      <td>0.951215</td>\n",
       "      <td>0.950861</td>\n",
       "      <td>0.951160</td>\n",
       "      <td>0.950798</td>\n",
       "      <td>0.951208</td>\n",
       "      <td>0.950768</td>\n",
       "      <td>0.951016</td>\n",
       "      <td>0.950754</td>\n",
       "      <td>0.951251</td>\n",
       "      <td>0.950904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.015516</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>0.001421</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>0.001727</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>0.002065</td>\n",
       "      <td>0.002182</td>\n",
       "      <td>0.002229</td>\n",
       "      <td>0.002317</td>\n",
       "      <td>0.002373</td>\n",
       "      <td>0.002511</td>\n",
       "      <td>0.002607</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>0.002956</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.003669</td>\n",
       "      <td>0.004446</td>\n",
       "      <td>0.005464</td>\n",
       "      <td>0.007217</td>\n",
       "      <td>0.021917</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.922962</td>\n",
       "      <td>0.932851</td>\n",
       "      <td>0.936260</td>\n",
       "      <td>0.937315</td>\n",
       "      <td>0.937854</td>\n",
       "      <td>0.937777</td>\n",
       "      <td>0.937677</td>\n",
       "      <td>0.936573</td>\n",
       "      <td>0.93526</td>\n",
       "      <td>0.932748</td>\n",
       "      <td>0.929680</td>\n",
       "      <td>0.926123</td>\n",
       "      <td>0.923310</td>\n",
       "      <td>0.920749</td>\n",
       "      <td>0.918694</td>\n",
       "      <td>0.916136</td>\n",
       "      <td>0.914293</td>\n",
       "      <td>0.912346</td>\n",
       "      <td>0.910989</td>\n",
       "      <td>0.909617</td>\n",
       "      <td>0.908761</td>\n",
       "      <td>0.90772</td>\n",
       "      <td>0.907274</td>\n",
       "      <td>0.906113</td>\n",
       "      <td>0.905732</td>\n",
       "      <td>0.905077</td>\n",
       "      <td>0.904950</td>\n",
       "      <td>0.904459</td>\n",
       "      <td>0.904338</td>\n",
       "      <td>0.903937</td>\n",
       "      <td>0.904053</td>\n",
       "      <td>0.903336</td>\n",
       "      <td>0.903306</td>\n",
       "      <td>0.902948</td>\n",
       "      <td>0.903066</td>\n",
       "      <td>0.902984</td>\n",
       "      <td>0.903211</td>\n",
       "      <td>0.903063</td>\n",
       "      <td>0.903376</td>\n",
       "      <td>0.902784</td>\n",
       "      <td>0.902869</td>\n",
       "      <td>0.902611</td>\n",
       "      <td>0.902822</td>\n",
       "      <td>0.902633</td>\n",
       "      <td>0.902773</td>\n",
       "      <td>0.902600</td>\n",
       "      <td>0.902902</td>\n",
       "      <td>0.902326</td>\n",
       "      <td>0.902408</td>\n",
       "      <td>0.902158</td>\n",
       "      <td>0.902360</td>\n",
       "      <td>0.902367</td>\n",
       "      <td>0.902662</td>\n",
       "      <td>0.902586</td>\n",
       "      <td>0.902955</td>\n",
       "      <td>0.902401</td>\n",
       "      <td>0.902516</td>\n",
       "      <td>0.902288</td>\n",
       "      <td>0.902530</td>\n",
       "      <td>0.902373</td>\n",
       "      <td>0.902539</td>\n",
       "      <td>0.902389</td>\n",
       "      <td>0.902713</td>\n",
       "      <td>0.902155</td>\n",
       "      <td>0.902248</td>\n",
       "      <td>0.902010</td>\n",
       "      <td>0.902224</td>\n",
       "      <td>0.902242</td>\n",
       "      <td>0.902549</td>\n",
       "      <td>0.902475</td>\n",
       "      <td>0.902854</td>\n",
       "      <td>0.902305</td>\n",
       "      <td>0.902428</td>\n",
       "      <td>0.902206</td>\n",
       "      <td>0.902454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.013516  0.001920  0.000770  0.000470  0.000362  0.000304  0.000279   \n",
       "1  0.015516  0.001709  0.000618  0.000358  0.000262  0.000214  0.000191   \n",
       "\n",
       "        7         8         9         10        11        12        13   \\\n",
       "0  0.000261  0.000260  0.000256  0.000260  0.000261  0.000272  0.000278   \n",
       "1  0.000178  0.000174  0.000173  0.000177  0.000180  0.000186  0.000192   \n",
       "\n",
       "        14        15        16        17        18        19        20   \\\n",
       "0  0.000293  0.000305  0.000338  0.000363  0.000408  0.000458  0.000542   \n",
       "1  0.000200  0.000208  0.000223  0.000236  0.000256  0.000277  0.000308   \n",
       "\n",
       "        21        22        23        24        25        26        27   \\\n",
       "0  0.000634  0.000765  0.000896  0.001084  0.001260  0.001506  0.001742   \n",
       "1  0.000343  0.000393  0.000444  0.000513  0.000586  0.000681  0.000772   \n",
       "\n",
       "        28        29        30        31        32        33        34   \\\n",
       "0  0.002085  0.002411  0.002855  0.003236  0.003844  0.004266  0.004829   \n",
       "1  0.000886  0.000994  0.001131  0.001246  0.001421  0.001554  0.001727   \n",
       "\n",
       "        35        36        37        38        39        40        41   \\\n",
       "0  0.005180  0.005686  0.005927  0.006330  0.006462  0.006803  0.006884   \n",
       "1  0.001845  0.001986  0.002065  0.002182  0.002229  0.002317  0.002373   \n",
       "\n",
       "        42        43        44        45        46        47        48   \\\n",
       "0  0.007271  0.007441  0.007996  0.008410  0.009403  0.010432  0.012879   \n",
       "1  0.002511  0.002607  0.002785  0.002956  0.003289  0.003669  0.004446   \n",
       "\n",
       "        49        50        51        52        53        54        55   \\\n",
       "0  0.015649  0.020598  0.287240  0.981540  0.976948  0.976850  0.976348   \n",
       "1  0.005464  0.007217  0.021917  0.860335  0.922962  0.932851  0.936260   \n",
       "\n",
       "        56        57        58        59        60       61        62   \\\n",
       "0  0.976222  0.975877  0.975867  0.975297  0.974479  0.97219  0.968473   \n",
       "1  0.937315  0.937854  0.937777  0.937677  0.936573  0.93526  0.932748   \n",
       "\n",
       "        63        64        65        66        67        68        69   \\\n",
       "0  0.963202  0.959208  0.956486  0.955500  0.954275  0.953876  0.953130   \n",
       "1  0.929680  0.926123  0.923310  0.920749  0.918694  0.916136  0.914293   \n",
       "\n",
       "        70        71        72        73       74        75        76   \\\n",
       "0  0.953202  0.952538  0.952609  0.952060  0.95231  0.951746  0.951879   \n",
       "1  0.912346  0.910989  0.909617  0.908761  0.90772  0.907274  0.906113   \n",
       "\n",
       "        77        78        79        80        81        82        83   \\\n",
       "0  0.951508  0.951915  0.951496  0.951599  0.951208  0.951598  0.951123   \n",
       "1  0.905732  0.905077  0.904950  0.904459  0.904338  0.903937  0.904053   \n",
       "\n",
       "        84        85        86        87        88        89        90   \\\n",
       "0  0.951319  0.951007  0.951467  0.951085  0.951367  0.950986  0.951380   \n",
       "1  0.903336  0.903306  0.902948  0.903066  0.902984  0.903211  0.903063   \n",
       "\n",
       "        91        92        93        94        95        96        97   \\\n",
       "0  0.950927  0.951159  0.950881  0.951364  0.951007  0.951168  0.950827   \n",
       "1  0.903376  0.902784  0.902869  0.902611  0.902822  0.902633  0.902773   \n",
       "\n",
       "        98        99        100       101       102       103       104  \\\n",
       "0  0.951258  0.950816  0.951043  0.950763  0.951251  0.950891  0.951187   \n",
       "1  0.902600  0.902902  0.902326  0.902408  0.902158  0.902360  0.902367   \n",
       "\n",
       "        105       106       107       108       109       110       111  \\\n",
       "0  0.950822  0.951227  0.950786  0.951032  0.950769  0.951265  0.950917   \n",
       "1  0.902662  0.902586  0.902955  0.902401  0.902516  0.902288  0.902530   \n",
       "\n",
       "        112       113       114       115       116       117       118  \\\n",
       "0  0.951086  0.950755  0.951194  0.950760  0.950993  0.950722  0.951215   \n",
       "1  0.902373  0.902539  0.902389  0.902713  0.902155  0.902248  0.902010   \n",
       "\n",
       "        119       120       121       122       123       124       125  \\\n",
       "0  0.950861  0.951160  0.950798  0.951208  0.950768  0.951016  0.950754   \n",
       "1  0.902224  0.902242  0.902549  0.902475  0.902854  0.902305  0.902428   \n",
       "\n",
       "        126       127  \n",
       "0  0.951251  0.950904  \n",
       "1  0.902206  0.902454  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting with part of the sequence as zero, Does this change the sequences after\n",
    "yyy_other_test = np.copy(yyy_other[:128, :, :])\n",
    "yyy_other_test[:, 52:, :] = 5\n",
    "yyy_pred_128 = model.predict([xxx[:128, :, :], yyy_other_test], batch_size=128)\n",
    "t2 = pd.DataFrame(yyy_pred_128[-1, :, :]).T\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.942166</td>\n",
       "      <td>-0.924320</td>\n",
       "      <td>-0.503686</td>\n",
       "      <td>-0.914381</td>\n",
       "      <td>-0.903161</td>\n",
       "      <td>-0.907305</td>\n",
       "      <td>-0.466340</td>\n",
       "      <td>-0.911909</td>\n",
       "      <td>-0.90426</td>\n",
       "      <td>-0.908847</td>\n",
       "      <td>-0.907846</td>\n",
       "      <td>-0.907426</td>\n",
       "      <td>-0.903568</td>\n",
       "      <td>-0.90466</td>\n",
       "      <td>-0.904835</td>\n",
       "      <td>-0.906935</td>\n",
       "      <td>-0.907176</td>\n",
       "      <td>-0.909284</td>\n",
       "      <td>-0.910064</td>\n",
       "      <td>-0.911900</td>\n",
       "      <td>-0.912475</td>\n",
       "      <td>-0.914351</td>\n",
       "      <td>-0.915120</td>\n",
       "      <td>-0.916727</td>\n",
       "      <td>-0.917313</td>\n",
       "      <td>-0.919093</td>\n",
       "      <td>-0.874049</td>\n",
       "      <td>-0.915646</td>\n",
       "      <td>-0.915291</td>\n",
       "      <td>-0.918842</td>\n",
       "      <td>-0.920998</td>\n",
       "      <td>-0.923962</td>\n",
       "      <td>-0.926514</td>\n",
       "      <td>-0.930080</td>\n",
       "      <td>-0.933799</td>\n",
       "      <td>-0.937600</td>\n",
       "      <td>-0.941405</td>\n",
       "      <td>-0.944488</td>\n",
       "      <td>-0.947195</td>\n",
       "      <td>-0.948339</td>\n",
       "      <td>-0.949467</td>\n",
       "      <td>-0.949753</td>\n",
       "      <td>-0.950542</td>\n",
       "      <td>-0.950378</td>\n",
       "      <td>-0.950625</td>\n",
       "      <td>-0.950354</td>\n",
       "      <td>-0.950819</td>\n",
       "      <td>-0.950406</td>\n",
       "      <td>-0.950645</td>\n",
       "      <td>-0.950378</td>\n",
       "      <td>-0.950865</td>\n",
       "      <td>-0.950505</td>\n",
       "      <td>-0.950787</td>\n",
       "      <td>-0.950418</td>\n",
       "      <td>-0.950811</td>\n",
       "      <td>-0.950364</td>\n",
       "      <td>-0.950595</td>\n",
       "      <td>-0.950326</td>\n",
       "      <td>-0.950811</td>\n",
       "      <td>-0.950460</td>\n",
       "      <td>-0.950607</td>\n",
       "      <td>-0.950274</td>\n",
       "      <td>-0.950703</td>\n",
       "      <td>-0.950267</td>\n",
       "      <td>-0.950489</td>\n",
       "      <td>-0.950215</td>\n",
       "      <td>-0.950699</td>\n",
       "      <td>-0.950345</td>\n",
       "      <td>-0.950629</td>\n",
       "      <td>-0.950268</td>\n",
       "      <td>-0.950668</td>\n",
       "      <td>-0.950227</td>\n",
       "      <td>-0.950460</td>\n",
       "      <td>-0.950194</td>\n",
       "      <td>-0.950677</td>\n",
       "      <td>-0.950326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.846774</td>\n",
       "      <td>-0.906295</td>\n",
       "      <td>-0.887555</td>\n",
       "      <td>-0.914317</td>\n",
       "      <td>-0.914335</td>\n",
       "      <td>-0.914022</td>\n",
       "      <td>-0.882885</td>\n",
       "      <td>-0.914148</td>\n",
       "      <td>-0.91409</td>\n",
       "      <td>-0.913497</td>\n",
       "      <td>-0.911285</td>\n",
       "      <td>-0.909111</td>\n",
       "      <td>-0.905453</td>\n",
       "      <td>-0.90337</td>\n",
       "      <td>-0.900949</td>\n",
       "      <td>-0.899674</td>\n",
       "      <td>-0.897516</td>\n",
       "      <td>-0.896617</td>\n",
       "      <td>-0.895062</td>\n",
       "      <td>-0.894573</td>\n",
       "      <td>-0.893651</td>\n",
       "      <td>-0.893513</td>\n",
       "      <td>-0.892695</td>\n",
       "      <td>-0.892914</td>\n",
       "      <td>-0.892189</td>\n",
       "      <td>-0.892650</td>\n",
       "      <td>-0.796739</td>\n",
       "      <td>-0.876548</td>\n",
       "      <td>-0.883601</td>\n",
       "      <td>-0.887398</td>\n",
       "      <td>-0.889228</td>\n",
       "      <td>-0.891595</td>\n",
       "      <td>-0.892707</td>\n",
       "      <td>-0.894714</td>\n",
       "      <td>-0.896109</td>\n",
       "      <td>-0.898063</td>\n",
       "      <td>-0.899507</td>\n",
       "      <td>-0.901001</td>\n",
       "      <td>-0.901698</td>\n",
       "      <td>-0.902557</td>\n",
       "      <td>-0.902267</td>\n",
       "      <td>-0.902527</td>\n",
       "      <td>-0.902363</td>\n",
       "      <td>-0.902631</td>\n",
       "      <td>-0.902473</td>\n",
       "      <td>-0.902632</td>\n",
       "      <td>-0.902469</td>\n",
       "      <td>-0.902777</td>\n",
       "      <td>-0.902203</td>\n",
       "      <td>-0.902287</td>\n",
       "      <td>-0.902035</td>\n",
       "      <td>-0.902237</td>\n",
       "      <td>-0.902241</td>\n",
       "      <td>-0.902534</td>\n",
       "      <td>-0.902452</td>\n",
       "      <td>-0.902818</td>\n",
       "      <td>-0.902261</td>\n",
       "      <td>-0.902375</td>\n",
       "      <td>-0.902142</td>\n",
       "      <td>-0.902383</td>\n",
       "      <td>-0.902220</td>\n",
       "      <td>-0.902385</td>\n",
       "      <td>-0.902231</td>\n",
       "      <td>-0.902554</td>\n",
       "      <td>-0.901993</td>\n",
       "      <td>-0.902087</td>\n",
       "      <td>-0.901845</td>\n",
       "      <td>-0.902059</td>\n",
       "      <td>-0.902075</td>\n",
       "      <td>-0.902380</td>\n",
       "      <td>-0.902301</td>\n",
       "      <td>-0.902679</td>\n",
       "      <td>-0.902127</td>\n",
       "      <td>-0.902249</td>\n",
       "      <td>-0.902022</td>\n",
       "      <td>-0.902269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    10   11   12   13   14   \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   15   16   17   18   19   20   21   22   23   24   25   26   27   28   29   \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   30   31   32   33   34   35   36   37   38   39   40   41   42   43   44   \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   45   46   47   48   49   50   51        52        53        54        55   \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 -0.942166 -0.924320 -0.503686 -0.914381   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0 -0.846774 -0.906295 -0.887555 -0.914317   \n",
       "\n",
       "        56        57        58        59       60        61        62   \\\n",
       "0 -0.903161 -0.907305 -0.466340 -0.911909 -0.90426 -0.908847 -0.907846   \n",
       "1 -0.914335 -0.914022 -0.882885 -0.914148 -0.91409 -0.913497 -0.911285   \n",
       "\n",
       "        63        64       65        66        67        68        69   \\\n",
       "0 -0.907426 -0.903568 -0.90466 -0.904835 -0.906935 -0.907176 -0.909284   \n",
       "1 -0.909111 -0.905453 -0.90337 -0.900949 -0.899674 -0.897516 -0.896617   \n",
       "\n",
       "        70        71        72        73        74        75        76   \\\n",
       "0 -0.910064 -0.911900 -0.912475 -0.914351 -0.915120 -0.916727 -0.917313   \n",
       "1 -0.895062 -0.894573 -0.893651 -0.893513 -0.892695 -0.892914 -0.892189   \n",
       "\n",
       "        77        78        79        80        81        82        83   \\\n",
       "0 -0.919093 -0.874049 -0.915646 -0.915291 -0.918842 -0.920998 -0.923962   \n",
       "1 -0.892650 -0.796739 -0.876548 -0.883601 -0.887398 -0.889228 -0.891595   \n",
       "\n",
       "        84        85        86        87        88        89        90   \\\n",
       "0 -0.926514 -0.930080 -0.933799 -0.937600 -0.941405 -0.944488 -0.947195   \n",
       "1 -0.892707 -0.894714 -0.896109 -0.898063 -0.899507 -0.901001 -0.901698   \n",
       "\n",
       "        91        92        93        94        95        96        97   \\\n",
       "0 -0.948339 -0.949467 -0.949753 -0.950542 -0.950378 -0.950625 -0.950354   \n",
       "1 -0.902557 -0.902267 -0.902527 -0.902363 -0.902631 -0.902473 -0.902632   \n",
       "\n",
       "        98        99        100       101       102       103       104  \\\n",
       "0 -0.950819 -0.950406 -0.950645 -0.950378 -0.950865 -0.950505 -0.950787   \n",
       "1 -0.902469 -0.902777 -0.902203 -0.902287 -0.902035 -0.902237 -0.902241   \n",
       "\n",
       "        105       106       107       108       109       110       111  \\\n",
       "0 -0.950418 -0.950811 -0.950364 -0.950595 -0.950326 -0.950811 -0.950460   \n",
       "1 -0.902534 -0.902452 -0.902818 -0.902261 -0.902375 -0.902142 -0.902383   \n",
       "\n",
       "        112       113       114       115       116       117       118  \\\n",
       "0 -0.950607 -0.950274 -0.950703 -0.950267 -0.950489 -0.950215 -0.950699   \n",
       "1 -0.902220 -0.902385 -0.902231 -0.902554 -0.901993 -0.902087 -0.901845   \n",
       "\n",
       "        119       120       121       122       123       124       125  \\\n",
       "0 -0.950345 -0.950629 -0.950268 -0.950668 -0.950227 -0.950460 -0.950194   \n",
       "1 -0.902059 -0.902075 -0.902380 -0.902301 -0.902679 -0.902127 -0.902249   \n",
       "\n",
       "        126       127  \n",
       "0 -0.950677 -0.950326  \n",
       "1 -0.902022 -0.902269  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 - t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1152, 128, 53), (1152, 128, 2), (1152, 128, 2))"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxx.shape, yyy.shape, yyy_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2f15bb7f0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2880x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data = yyy_pred.reshape((1152, 128, 2))\n",
    "data = yyy_pred\n",
    "data = data[:129,  :, 0]\n",
    "plt.rcParams[\"figure.figsize\"] = (40,10)\n",
    "plt.imshow(data.T, cmap='hot', interpolation='nearest', aspect=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x291daec40>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACNoAAAI/CAYAAACryCaGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAApTUlEQVR4nO3dX6zkZ33f8c+XnQEn0Ai7iS0Xk0IlKw1FDVQrREtUpSG0ToKwb2gdFWnVUFmVUhWqVKlJL6pcVEJqFZGLtJIFlJVCoSghtYWSFGuTKqmUOODQJlCTGhECLq43hKahtQTH8dOLHbdbc5b9en+/mfnN7OslWefMn/PMs+N5nvM7e977mxpjBAAAAAAAAAAA+Maet+8JAAAAAAAAAADAIRDaAAAAAAAAAABAg9AGAAAAAAAAAAAahDYAAAAAAAAAANAgtAEAAAAAAAAAgAahDQAAAAAAAAAANKx2+WBVNZQ9AAAAsBs1wxhjhjGAZZhjT0jsCwAAABy/p5MvjTG+7bTbJoU2VXVHkp9KcibJu8cY7/xG939ekhumPCAAAADQtp5hjJMZxgCWYY49IbEvAAAAcPyeTH7/Srdd8wlmqupMkp9O8v1JXpHkh6rqFdc6HgAAAAAAAAAALNmUd3J6TZLPjDE+O8b4WpIPJrlznmkBAAAAAAAAAMCyTAltXpLkC5ddfmxzHQAAAAAAAAAAHJ3VhK+tU64bX3enqnuS3HOlLwAAAAAAAAAAgEMwJbR5LMlLL7t8W5IvPvtOY4z7ktyXJGeqvi7EAQAAAAAAAACAQzDlraM+luT2qnp5VT0/yd1JHphnWgAAAAAAAAAAsCzXfEabMcZTVfX3k/z7JGeSvHeM8anZZgYAAAAAAAAAAAsy5a2jMsb4hSS/MNNcAAAAAAAAAABgsSaFNgAAAMD81jONczLTOMD+zbEv2BMAAABguuftewIAAAAAAAAAAHAIhDYAAAAAAAAAANAgtAEAAAAAAAAAgAahDQAAAAAAAAAANAhtAAAAAAAAAACgQWgDAAAAAAAAAAANQhsAAAAAAAAAAGgQ2gAAAAAAAAAAQIPQBgAAAAAAAAAAGoQ2AAAAAAAAAADQILQBAAAAAAAAAIAGoQ0AAAAAAAAAADQIbQAAAAAAAAAAoEFoAwAAAAAAAAAADUIbAAAAAAAAAABoENoAAAAAAAAAAECD0AYAAAAAAAAAABqENgAAAAAAAAAA0CC0AQAAAAAAAACABqENAAAAAAAAAAA0rPY9AQAAADgm6xnGOJlhDGA57AsAAABwPJzRBgAAAAAAAAAAGoQ2AAAAAAAAAADQILQBAAAAAAAAAIAGoQ0AAAAAAAAAADQIbQAAAAAAAAAAoEFoAwAAAAAAAAAADUIbAAAAAAAAAABoENoAAAAAAAAAAECD0AYAAAAAAAAAABqENgAAAAAAAAAA0CC0AQAAAAAAAACABqENAAAAAAAAAAA0CG0AAAAAAAAAAKBBaAMAAAAAAAAAAA1CGwAAAAAAAAAAaBDaAAAAAAAAAABAg9AGAAAAAAAAAAAahDYAAAAAAAAAANAgtAEAAAAAAAAAgIbVvicAAAAAS7GeYYyTGcYAlmGOPSGxLwAAAMAxcUYbAAAAAAAAAABoENoAAAAAAAAAAECD0AYAAAAAAAAAABqENgAAAAAAAAAA0CC0AQAAAAAAAACABqENAAAAAAAAAAA0CG0AAAAAAAAAAKBBaAMAAAAAAAAAAA1CGwAAAAAAAAAAaBDaAAAAAAAAAABAg9AGAAAAAAAAAAAahDYAAAAAAAAAANAgtAEAAAAAAAAAgAahDQAAAAAAAAAANAhtAAAAAAAAAACgQWgDAAAAAAAAAAANQhsAAAAAAAAAAGgQ2gAAAAAAAAAAQIPQBgAAAAAAAAAAGoQ2AAAAAAAAAADQsNr3BAAAAGCq9UzjnMw0DrB/c+wL9gQAAADg2ZzRBgAAAAAAAAAAGoQ2AAAAAAAAAADQILQBAAAAAAAAAIAGoQ0AAAAAAAAAADQIbQAAAAAAAAAAoEFoAwAAAAAAAAAADUIbAAAAAAAAAABoENoAAAAAAAAAAECD0AYAAAAAAAAAABqENgAAAAAAAAAA0CC0AQAAAAAAAACABqENAAAAAAAAAAA0CG0AAAAAAAAAAKBBaAMAAAAAAAAAAA1CGwAAAAAAAAAAaBDaAAAAAAAAAABAg9AGAAAAAAAAAAAahDYAAAAAAAAAANAgtAEAAAAAAAAAgIbVvicAAADA9W09wxgnM4wBLMMce0JiXwAAAAC2wxltAAAAAAAAAACgQWgDAAAAAAAAAAANQhsAAAAAAAAAAGgQ2gAAAAAAAAAAQIPQBgAAAAAAAAAAGoQ2AAAAAAAAAADQILQBAAAAAAAAAIAGoQ0AAAAAAAAAADQIbQAAAAAAAAAAoEFoAwAAAAAAAAAADUIbAAAAAAAAAABouGpoU1XvraqLVfXJy667qaoerKpHNx9v3O40AQAAAAAAAABgvzpntHlfkjuedd29SS6MMW5PcmFzGQAAAAAAAAAAjtZVQ5sxxq8m+fKzrr4zyfnN5+eT3DXvtAAAAAAAAAAAYFk6Z7Q5zS1jjMeTZPPx5vmmBAAAAAAAAAAAy7Pa9gNU1T1J7kmS2vaDAQAAAAAAAADAllzrGW2eqKpbk2Tz8eKV7jjGuG+McXaMcVZoAwAAAAAAAADAobrW0OaBJOc2n59Lcv880wEAAAAAAAAAgGW6amhTVR9I8utJvqOqHquqtyZ5Z5I3VNWjSd6wuQwAAAAAAAAAAEerxhg7e7AzVeOGnT0aAAAAh2A9wxgnM4wBLMMce0JiXwAAAACu3ZPJw2OMs6fdttr1ZAAAADgOfhkOPJtwDgAAADh2V33rKAAAAAAAAAAAQGgDAAAAAAAAAAAtQhsAAAAAAAAAAGgQ2gAAAAAAAAAAQIPQBgAAAAAAAAAAGoQ2AAAAAAAAAADQILQBAAAAAAAAAIAGoQ0AAAAAAAAAADQIbQAAAAAAAAAAoEFoAwAAAAAAAAAADUIbAAAAAAAAAABoENoAAAAAAAAAAECD0AYAAAAAAAAAABqENgAAAAAAAAAA0CC0AQAAAAAAAACABqENAAAAAAAAAAA0CG0AAAAAAAAAAKBBaAMAAAAAAAAAAA1CGwAAAAAAAAAAaFjtewIAAADs3nqGMU5mGANYhjn2hMS+AAAAABw/Z7QBAAAAAAAAAIAGoQ0AAAAAAAAAADQIbQAAAAAAAAAAoEFoAwAAAAAAAAAADUIbAAAAAAAAAABoENoAAAAAAAAAAECD0AYAAAAAAAAAABqENgAAAAAAAAAA0CC0AQAAAAAAAACABqENAAAAAAAAAAA0CG0AAAAAAAAAAKBBaAMAAAAAAAAAAA1CGwAAAAAAAAAAaBDaAAAAAAAAAABAg9AGAAAAAAAAAAAahDYAAAAAAAAAANAgtAEAAAAAAAAAgAahDQAAAAAAAAAANAhtAAAAAAAAAACgQWgDAAAAAAAAAAANq31PAAAAgL71TOOczDQOsH9z7Av2BAAAAIAeZ7QBAAAAAAAAAIAGoQ0AAAAAAAAAADQIbQAAAAAAAAAAoEFoAwAAAAAAAAAADUIbAAAAAAAAAABoENoAAAAAAAAAAECD0AYAAAAAAAAAABqENgAAAAAAAAAA0CC0AQAAAAAAAACABqENAAAAAAAAAAA0CG0AAAAAAAAAAKBBaAMAAAAAAAAAAA1CGwAAAAAAAAAAaBDaAAAAAAAAAABAg9AGAAAAAAAAAAAahDYAAAAAAAAAANAgtAEAAAAAAAAAgAahDQAAAAAAAAAANAhtAAAAAAAAAACgYbXvCQAAAFwv1jOMcTLDGMBy2BcAAAAADosz2gAAAAAAAAAAQIPQBgAAAAAAAAAAGoQ2AAAAAAAAAADQILQBAAAAAAAAAIAGoQ0AAAAAAAAAADQIbQAAAAAAAAAAoEFoAwAAAAAAAAAADUIbAAAAAAAAAABoENoAAAAAAAAAAECD0AYAAAAAAAAAABqENgAAAAAAAAAA0CC0AQAAAAAAAACABqENAAAAAAAAAAA0CG0AAAAAAAAAAKBBaAMAAAAAAAAAAA1CGwAAAAAAAAAAaBDaAAAAAAAAAABAg9AGAAAAAAAAAAAahDYAAAAAAAAAANAgtAEAAAAAAAAAgIbVvicAAABwCNYzjHEywxjAMsyxJyT2BQAAAIBD44w2AAAAAAAAAADQILQBAAAAAAAAAIAGoQ0AAAAAAAAAADQIbQAAAAAAAAAAoEFoAwAAAAAAAAAADUIbAAAAAAAAAABoENoAAAAAAAAAAECD0AYAAAAAAAAAABqENgAAAAAAAAAA0CC0AQAAAAAAAACAhquGNlX10qr6lap6pKo+VVVv21x/U1U9WFWPbj7euP3pAgAAAAAAAADAfnTOaPNUkh8dY3xnktcm+ZGqekWSe5NcGGPcnuTC5jIAAAAAAAAAABylq4Y2Y4zHxxi/tfn8K0keSfKSJHcmOb+52/kkd21pjgAAAAAAAAAAsHedM9r8X1X1siSvTvJQklvGGI8nl2KcJDfPPjsAAAAAAAAAAFiIVfeOVfWiJD+X5O1jjD+uqu7X3ZPkniTpfQUAAAAAAAAAACxP64w2VbXOpcjm/WOMD2+ufqKqbt3cfmuSi6d97RjjvjHG2THGWaENAAAAAAAAAACH6qqhTV06dc17kjwyxvjJy256IMm5zefnktw///QAAAAAAAAAAGAZaozxje9Q9d1Jfi3J7yR5enP1jyd5KMmHknx7ks8nefMY48vfaKwzVeOGqTMGAADYg/UMY5zMMAawDHPsCYl9AQAAAGCJnkweHmOcPe221dW+eIzxH5Nc6V2fXj9lYgAAAAAAAAAAcCiu+tZRAAAAAAAAAABA44w2AAAAh8zbuwDP5q3gAAAAALhWzmgDAAAAAAAAAAANQhsAAAAAAAAAAGgQ2gAAAAAAAAAAQIPQBgAAAAAAAAAAGoQ2AAAAAAAAAADQILQBAAAAAAAAAIAGoQ0AAAAAAAAAADQIbQAAAAAAAAAAoEFoAwAAAAAAAAAADUIbAAAAAAAAAABoENoAAAAAAAAAAECD0AYAAAAAAAAAABqENgAAAAAAAAAA0CC0AQAAAAAAAACABqENAAAAAAAAAAA0CG0AAAAAAAAAAKBBaAMAAAAAAAAAAA1CGwAAAAAAAAAAaBDaAAAAAAAAAABAw2rfEwAAALiS9QxjnMwwBrAMc+wJiX0BAAAAgGvnjDYAAAAAAAAAANAgtAEAAAAAAAAAgAahDQAAAAAAAAAANAhtAAAAAAAAAACgQWgDAAAAAAAAAAANQhsAAAAAAAAAAGgQ2gAAAAAAAAAAQIPQBgAAAAAAAAAAGoQ2AAAAAAAAAADQsNrlg1WS9cQxnj/HRDJ9HnONMdc4x/a8zPXczvG8LOn/s+dle2Mc2xqaa5xje16soe2NkXheTnNsa2iucY7tebGGtjdGMs/zcmauf16wlCfm2BbRXOMc2/OypEW0pP/Px/i83DDDGJ7b7Y1zbM+L5/Z0npfTLenP41hhe+Ms5bUy1zjHtobmGmc11w9Fcxy4LGWMucY5xrm8YIYxlvTnMZftjTPHayVZzvOypOf2GOdib9nOGHONc3xzeWHVLONciTPaAAAAAAAAAABAg9AGAAAAAAAAAAAahDYAAAAAAAAAANAgtAEAAAAAAAAAgAahDQAAAAAAAAAANAhtAAAAAAAAAACgQWgDAAAAAAAAAAANQhsAAAAAAAAAAGgQ2gAAAAAAAAAAQIPQBgAAAAAAAAAAGla7fLCR5GTiGFO/HvZlPcMYXv9cz6whmGaONZRYRxyop+cZZv3V6WOczDAGHCrHczCNNQTT+JmI69s8PxSt8+TkMU5mGAMOleM5mMYa4rmY6/j/SpzRBgAAAAAAAAAAGoQ2AAAAAAAAAADQILQBAAAAAAAAAIAGoQ0AAAAAAAAAADQIbQAAAAAAAAAAoEFoAwAAAAAAAAAADUIbAAAAAAAAAABoENoAAAAAAAAAAECD0AYAAAAAAAAAABpWu3ywSrKeOMbJHBOBPZjjtTt1/TzDOuIQWUMwzVyv2znWkTXEoVrK9yJriEO1lDWUWEccJmsIpvEzEUy3lO9F1hCHailrKLGOOEzWEM/Ftv8fOaMNAAAAAAAAAAA0CG0AAAAAAAAAAKBBaAMAAAAAAAAAAA1CGwAAAAAAAAAAaBDaAAAAAAAAAABAg9AGAAAAAAAAAAAahDYAAAAAAAAAANAgtAEAAAAAAAAAgAahDQAAAAAAAAAANKx2+WAjycnEMdZzTCTT5wH7MNfrdo51ZA1xiKwhmG6O167jOa5n1hBM43gOprGGYDrHczCNNQTTOJ6Daawh5uKMNgAAAAAAAAAA0CC0AQAAAAAAAACABqENAAAAAAAAAAA0CG0AAAAAAAAAAKBBaAMAAAAAAAAAAA1CGwAAAAAAAAAAaBDaAAAAAAAAAABAg9AGAAAAAAAAAAAahDYAAAAAAAAAANCw2vcEnquTmcZZzzDGXHOBXZvjtTvHGkqsIw6TNQTTOJ6DaawhmM7xHExjDcE0judgGmsIpnM8B9NYQzijDQAAAAAAAAAANAhtAAAAAAAAAACgQWgDAAAAAAAAAAANQhsAAAAAAAAAAGgQ2gAAAAAAAAAAQIPQBgAAAAAAAAAAGoQ2AAAAAAAAAADQILQBAAAAAAAAAIAGoQ0AAAAAAAAAADQIbQAAAAAAAAAAoGG17wnsy8kMY6xnGCOZZy6wa3O9budYR9YQh8gagumWcjxnDXGolrKGEuuIw+R4DqaxhmC6pRzPWUMcqqWsocQ64jA5noNprKHD5ow2AAAAAAAAAADQILQBAAAAAAAAAIAGoQ0AAAAAAAAAADQIbQAAAAAAAAAAoEFoAwAAAAAAAAAADUIbAAAAAAAAAABoENoAAAAAAAAAAECD0AYAAAAAAAAAABqENgAAAAAAAAAA0LDa9wQO2clM46xnGGOuucCuzfHatYa4ni1lDSXWEYfJGoJp/EwE0y3le5E1xKFayhpKrCMOkzUE0/iZCKZbyvcia4hDtZQ1lFxf6+iqZ7Spqhuq6jer6j9X1aeq6ic2199UVQ9W1aObjzduf7oAAAAAAAAAALAfnbeO+mqS7x1jfFeSVyW5o6pem+TeJBfGGLcnubC5DAAAAAAAAAAAR+mqoc245H9tLq43/40kdyY5v7n+fJK7tjFBAAAAAAAAAABYgs4ZbVJVZ6rqPyW5mOTBMcZDSW4ZYzyeJJuPN29tlgAAAAAAAAAAsGet0GaM8SdjjFcluS3Ja6rqld0HqKp7qurjVfXxcY2TBAAAAAAAAACAfWuFNs8YY/xRkv+Q5I4kT1TVrUmy+XjxCl9z3xjj7BjjbE2bKwAAAAAAAAAA7M1VQ5uq+raqevHm829K8n1JPp3kgSTnNnc7l+T+Lc0RAAAAAAAAAAD2btW4z61JzlfVmVwKcz40xvhIVf16kg9V1VuTfD7Jm7c4TwAAAAAAAAAA2KurhjZjjN9O8upTrv/DJK/fxqQAAAAAAAAAAGBpOme0YctOZhhjPcMYyTxzgV2zhmCauV63c6wja4hDZA3BdI7nYBprCKZxPAfTWEMwneM5mMYagmkczz13z9v3BAAAAAAAAAAA4BAIbQAAAAAAAAAAoEFoAwAAAAAAAAAADUIbAAAAAAAAAABoENoAAAAAAAAAAECD0AYAAAAAAAAAABqENgAAAAAAAAAA0CC0AQAAAAAAAACABqENAAAAAAAAAAA0rPY9AeZxMtM46xnGmGsusEvWEEw3x2t3jjWUWEccJmsIpnE8B9NYQzCd4zmYxhqCaRzPwTTWEEx3PR3POaMNAAAAAAAAAAA0CG0AAAAAAAAAAKBBaAMAAAAAAAAAAA1CGwAAAAAAAAAAaBDaAAAAAAAAAABAg9AGAAAAAAAAAAAahDYAAAAAAAAAANAgtAEAAAAAAAAAgAahDQAAAAAAAAAANAhtAAAAAAAAAACgYbXvCbAsJzOMsZ5hjDnmAfuwlDWUWEccprlet74Xcb2yhmC6pRzPWUMcqqWsocQ64jA5noNprCGYbinHc9YQh2opayixjjhMh/K6dUYbAAAAAAAAAABoENoAAAAAAAAAAECD0AYAAAAAAAAAABqENgAAAAAAAAAA0CC0AQAAAAAAAACABqENAAAAAAAAAAA0CG0AAAAAAAAAAKBBaAMAAAAAAAAAAA1CGwAAAAAAAAAAaFjtewIcn5MZxljPMEYyz1xg1+Z63c6xjqwhDtVSvhdZQxyqpayhxDriMFlDMI2fiWC6pXwvsoY4VEtZQ4l1xGGyhmAaPxPB8jmjDQAAAAAAAAAANAhtAAAAAAAAAACgQWgDAAAAAAAAAAANQhsAAAAAAAAAAGgQ2gAAAAAAAAAAQIPQBgAAAAAAAAAAGoQ2AAAAAAAAAADQILQBAAAAAAAAAIAGoQ0AAAAAAAAAADSs9j0BOM3JTOOsZxhjrrnArs3x2p1jDSXWEYfJGoJpHM/BNNYQTOd4DqaxhmAax3MwjTUE0zmeg+1xRhsAAAAAAAAAAGgQ2gAAAAAAAAAAQIPQBgAAAAAAAAAAGoQ2AAAAAAAAAADQILQBAAAAAAAAAIAGoQ0AAAAAAAAAADQIbQAAAAAAAAAAoEFoAwAAAAAAAAAADUIbAAAAAAAAAABoWO17ArBNJzOMsZ5hjGSeucCuzfW6nWMdWUMcImsIpnM8B9NYQzCN4zmYxhqC6RzPwTTWEEzjeA5O54w2AAAAAAAAAADQILQBAAAAAAAAAIAGoQ0AAAAAAAAAADQIbQAAAAAAAAAAoEFoAwAAAAAAAAAADUIbAAAAAAAAAABoENoAAAAAAAAAAECD0AYAAAAAAAAAABqENgAAAAAAAAAA0CC0AQAAAAAAAACAhtW+JwBLd7LvCcARsI5gGmsIprGGuJ6tZxjDGoLplrKO5tgTkuX8ebh+eM3BNNYQTGMNwXTWEcfGGW0AAAAAAAAAAKBBaAMAAAAAAAAAAA1CGwAAAAAAAAAAaBDaAAAAAAAAAABAg9AGAAAAAAAAAAAahDYAAAAAAAAAANAgtAEAAAAAAAAAgAahDQAAAAAAAAAANAhtAAAAAAAAAACgYbXvCQAAAAD/v/VM45zMNA6wf3PsC/YEAAAAmM4ZbQAAAAAAAAAAoEFoAwAAAAAAAAAADUIbAAAAAAAAAABoENoAAAAAAAAAAECD0AYAAAAAAAAAABqENgAAAAAAAAAA0CC0AQAAAAAAAACABqENAAAAAAAAAAA0CG0AAAAAAAAAAKBhte8JAAAAwDFZzzDGyQxjAMthXwAAAIDj4Yw2AAAAAAAAAADQILQBAAAAAAAAAIAGoQ0AAAAAAAAAADQIbQAAAAAAAAAAoEFoAwAAAAAAAAAADUIbAAAAAAAAAABoENoAAAAAAAAAAECD0AYAAAAAAAAAABqENgAAAAAAAAAA0CC0AQAAAAAAAACAhtW+JwAAAABLsZ5hjJMZxgCWYY49IbEvAAAAwDFpn9Gmqs5U1Seq6iObyzdV1YNV9ejm443bmyYAAAAAAAAAAOzXc3nrqLcleeSyy/cmuTDGuD3Jhc1lAAAAAAAAAAA4Sq3QpqpuS/KDSd592dV3Jjm/+fx8krtmnRkAAAAAAAAAACxI94w270ryY0mevuy6W8YYjyfJ5uPN804NAAAAAAAAAACW46qhTVW9McnFMcbD1/IAVXVPVX28qj4+rmUAAAAAAAAAAABYgFXjPq9L8qaq+oEkNyT5lqr6mSRPVNWtY4zHq+rWJBdP++Ixxn1J7kuSM1VaGwAAAAAAAAAADtJVz2gzxnjHGOO2McbLktyd5JfHGG9J8kCSc5u7nUty/9ZmCQAAAAAAAAAAe3bV0OYbeGeSN1TVo0nesLkMAAAAAAAAAABHqcbY3bs5nakaN+zs0QAAAOC5Wc8wxskMYwDLMMeekNgXAAAA4NA8mTw8xjh72m2rXU8GAAAA5uaX4cCzCecAAACAbZjy1lEAAAAAAAAAAHDdENoAAAAAAAAAAECD0AYAAAAAAAAAABqENgAAAAAAAAAA0CC0AQAAAAAAAACABqENAAAAAAAAAAA0CG0AAAAAAAAAAKBBaAMAAAAAAAAAAA1CGwAAAAAAAAAAaFjtewIAAABc39YzjHEywxjAMsyxJyT2BQAAAGA7nNEGAAAAAAAAAAAahDYAAAAAAAAAANAgtAEAAAAAAAAAgAahDQAAAAAAAAAANAhtAAAAAAAAAACgQWgDAAAAAAAAAAANQhsAAAAAAAAAAGgQ2gAAAAAAAAAAQIPQBgAAAAAAAAAAGlb7ngAAAACHaT3TOCczjQPs3xz7gj0BAAAAWDJntAEAAAAAAAAAgAahDQAAAAAAAAAANAhtAAAAAAAAAACgQWgDAAAAAAAAAAANQhsAAAAAAAAAAGgQ2gAAAAAAAAAAQIPQBgAAAAAAAAAAGoQ2AAAAAAAAAADQILQBAAAAAAAAAIAGoQ0AAAAAAAAAADSs9j0BAAAAdm89wxgnM4wBLMMce0JiXwAAAACOnzPaAAAAAAAAAABAg9AGAAAAAAAAAAAahDYAAAAAAAAAANAgtAEAAAAAAAAAgAahDQAAAAAAAAAANAhtAAAAAAAAAACgQWgDAAAAAAAAAAANQhsAAAAAAAAAAGgQ2gAAAAAAAAAAQMNq3xMAAACgbz3TOCczjQPs3xz7gj0BAAAAoMcZbQAAAAAAAAAAoEFoAwAAAAAAAAAADUIbAAAAAAAAAABoENoAAAAAAAAAAECD0AYAAAAAAAAAABqENgAAAAAAAAAA0CC0AQAAAAAAAACABqENAAAAAAAAAAA0CG0AAAAAAAAAAKBhte8JAAAAXC/WM4xxMsMYwHLYFwAAAAAOizPaAAAAAAAAAABAg9AGAAAAAAAAAAAahDYAAAAAAAAAANAgtAEAAAAAAAAAgAahDQAAAAAAAAAANAhtAAAAAAAAAACgQWgDAAAAAAAAAAANQhsAAAAAAAAAAGgQ2gAAAAAAAAAAQMNq3xMAAAA4BOsZxjiZYQxgGebYExL7AgAAAMChcUYbAAAAAAAAAABoENoAAAAAAAAAAECD0AYAAAAAAAAAABqENgAAAAAAAAAA0CC0AQAAAAAAAACABqENAAAAAAAAAAA0CG0AAAAAAAAAAKBBaAMAAAAAAAAAAA1CGwAAAAAAAAAAaBDaAAAAAAAAAABAw2rfEwAAANim9UzjnMw0DrB/c+wL9gQAAACA65Mz2gAAAAAAAAAAQIPQBgAAAAAAAAAAGoQ2AAAAAAAAAADQILQBAAAAAAAAAIAGoQ0AAAAAAAAAADQIbQAAAAAAAAAAoEFoAwAAAAAAAAAADUIbAAAAAAAAAABoENoAAAAAAAAAAEDDat8TAAAA2KaTfU8AWBz7AgAAAADXyhltAAAAAAAAAACgQWgDAAAAAAAAAAANQhsAAAAAAAAAAGgQ2gAAAAAAAAAAQIPQBgAAAAAAAAAAGoQ2AAAAAAAAAADQILQBAAAAAAAAAIAGoQ0AAAAAAAAAADQIbQAAAAAAAAAAoEFoAwAAAAAAAAAADavOnarqc0m+kuRPkjw1xjhbVTcl+bdJXpbkc0n+5hjjf2xnmgAAAAAAAAAAsF/P5Yw2f22M8aoxxtnN5XuTXBhj3J7kwuYyAAAAAAAAAAAcpSlvHXVnkvObz88nuWvybAAAAAAAAAAAYKG6oc1I8tGqeriq7tlcd8sY4/Ek2Xy8eRsTBAAAAAAAAACAJVg17/e6McYXq+rmJA9W1ae7D7AJc+5JkrqGCQIAAAAAAAAAwBK0zmgzxvji5uPFJD+f5DVJnqiqW5Nk8/HiFb72vjHG2THGWaENAAAAAAAAAACH6qqhTVW9sKr+1DOfJ/nrST6Z5IEk5zZ3O5fk/m1NEgAAAAAAAAAA9q3z1lG3JPn5qnrm/v9mjPFLVfWxJB+qqrcm+XySN29vmgAAAAAAAAAAsF81xtjZg52pGjfs7NEAAAAAAAAAAOC5eTJ5eIxx9rTbrvrWUQAAAAAAAAAAgNAGAAAAAAAAAABahDYAAAAAAAAAANAgtAEAAAAAAAAAgAahDQAAAAAAAAAANAhtAAAAAAAAAACgQWgDAAAAAAAAAAANQhsAAAAAAAAAAGgQ2gAAAAAAAAAAQIPQBgAAAAAAAAAAGoQ2AAAAAAAAAADQILQBAAAAAAAAAIAGoQ0AAAAAAAAAADQIbQAAAAAAAAAAoEFoAwAAAAAAAAAADUIbAAAAAAAAAABoENoAAAAAAAAAAECD0AYAAAAAAAAAABqENgAAAAAAAAAA0CC0AQAAAAAAAACABqENAAAAAAAAAAA0rHb5YE8nX3oy+f2r3O1bk3xpF/MBuM7ZbwF2x54LsDv2XIDdsecC7Ib9FmB37Lk8489e6YYaY+xyIldVVR8fY5zd9zwAjp39FmB37LkAu2PPBdgdey7AbthvAXbHnkuHt44CAAAAAAAAAIAGoQ0AAAAAAAAAADQsMbS5b98TALhO2G8BdseeC7A79lyA3bHnAuyG/RZgd+y5XFWNMfY9BwAAAAAAAAAAWLwlntEGAAAAAAAAAAAWZzGhTVXdUVW/W1Wfqap79z0fgGNSVS+tql+pqkeq6lNV9bbN9TdV1YNV9ejm4437nivAMaiqM1X1iar6yOay/RZgC6rqxVX1s1X16c2x7l+25wJsR1X9w83fKXyyqj5QVTfYcwHmUVXvraqLVfXJy6674h5bVe/Y/D7td6vqb+xn1gCH6Qp77j/f/N3Cb1fVz1fViy+7zZ7L11lEaFNVZ5L8dJLvT/KKJD9UVa/Y76wAjspTSX50jPGdSV6b5Ec2++y9SS6MMW5PcmFzGYDp3pbkkcsu228BtuOnkvzSGOPPJ/muXNp77bkAM6uqlyT5B0nOjjFemeRMkrtjzwWYy/uS3PGs607dYzd/r3t3kr+w+Zp/ufk9GwA978vX77kPJnnlGOMvJvmvSd6R2HO5skWENklek+QzY4zPjjG+luSDSe7c85wAjsYY4/Exxm9tPv9KLv0C4iW5tNee39ztfJK79jJBgCNSVbcl+cEk777savstwMyq6luS/NUk70mSMcbXxhh/FHsuwLasknxTVa2SfHOSL8aeCzCLMcavJvnys66+0h57Z5IPjjG+Osb4vSSfyaXfswHQcNqeO8b46Bjjqc3F30hy2+Zzey6nWkpo85IkX7js8mOb6wCYWVW9LMmrkzyU5JYxxuPJpRgnyc17nBrAsXhXkh9L8vRl19lvAeb355L8QZJ/vXm7vndX1QtjzwWY3RjjvyX5F0k+n+TxJP9zjPHR2HMBtulKe6zfqQFs1w8n+cXN5/ZcTrWU0KZOuW7sfBYAR66qXpTk55K8fYzxx/ueD8Cxqao3Jrk4xnh433MBuA6skvylJP9qjPHqJP873rIEYCuq6sZc+te8L0/yZ5K8sKrest9ZAVy3/E4NYEuq6p8keSrJ+5+56pS72XNZTGjzWJKXXnb5tlw69SgAM6mqdS5FNu8fY3x4c/UTVXXr5vZbk1zc1/wAjsTrkrypqj6XS2+H+r1V9TOx3wJsw2NJHhtjPLS5/LO5FN7YcwHm931Jfm+M8QdjjJMkH07yV2LPBdimK+2xfqcGsAVVdS7JG5P87THGMzGNPZdTLSW0+ViS26vq5VX1/CR3J3lgz3MCOBpVVUnek+SRMcZPXnbTA0nObT4/l+T+Xc8N4JiMMd4xxrhtjPGyXDqm/eUxxltivwWY3Rjjvyf5QlV9x+aq1yf5L7HnAmzD55O8tqq+efN3DK9P8kjsuQDbdKU99oEkd1fVC6rq5UluT/Kbe5gfwNGoqjuS/OMkbxpjPHnZTfZcTlX/L8bar6r6gSTvSnImyXvHGP9svzMCOB5V9d1Jfi3J7yR5enP1jyd5KMmHknx7Lv2l2ZvHGF/eyyQBjkxVfU+SfzTGeGNV/enYbwFmV1WvSvLuJM9P8tkkfyeX/lGRPRdgZlX1E0n+Vi6dSv8TSf5ukhfFngswWVV9IMn3JPnWJE8k+adJ/l2usMdu3trkh3NpT377GOMXdz9rgMN0hT33HUlekOQPN3f7jTHG39vc357L11lMaAMAAAAAAAAAAEu2lLeOAgAAAAAAAACARRPaAAAAAAAAAABAg9AGAAAAAAAAAAAahDYAAAAAAAAAANAgtAEAAAAAAAAAgAahDQAAAAAAAAAANAhtAAAAAAAAAACgQWgDAAAAAAAAAAAN/wcYK+xDGGm2TgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 2880x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data = yyy_pred.reshape((1152, 128, 2))\n",
    "data = input_notes[0, :, :]\n",
    "plt.rcParams[\"figure.figsize\"] = (40,10)\n",
    "plt.imshow(data.T, cmap='hot', interpolation='nearest', aspect=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_time_note_model(size=16):\n",
    "    reshaped_train_data = xxx\n",
    "    reshaped_train_data_labels = yyy\n",
    "    reshaped_train_data_labels_for_input = yyy_other\n",
    "    num_beats=128\n",
    "    # size=16\n",
    "\n",
    "    batch_sequence_size = 32\n",
    "\n",
    "    note_vicinity=24\n",
    "    outputs = []\n",
    "    all_probs = []\n",
    "    num_notes = 128\n",
    "    total_vicinity = 53\n",
    "    # offset = np.random.choice(range(len(reshaped_train_data)))\n",
    "    offset = 0\n",
    "    input_notes = reshaped_train_data[offset:offset+num_beats, :, :]\n",
    "    input_notes_b = reshaped_train_data_labels_for_input[offset:offset+num_beats, :, :]\n",
    "    input_notes_b = input_notes_b.reshape((num_beats*num_notes, 2))\n",
    "    input_notes_b = np.concatenate([np.zeros((1,2)), input_notes_b[1:, :]])\n",
    "    input_notes_b = input_notes_b.reshape((num_beats, num_notes, 2))\n",
    "\n",
    "    last_beats =[str(x) for x in input_notes[0, -1, -4:]]\n",
    "    last_beats_int = int(\"\".join(last_beats), 2)\n",
    "\n",
    "    # input_notes_b_reshape = input_notes_b.reshape((num_beats*(128//batch_sequence_size), batch_sequence_size, 2))\n",
    "    # input_notes_reshape = input_notes.reshape((num_beats*(128//batch_sequence_size), batch_sequence_size, 53))\n",
    "    input_notes_reshape = input_notes\n",
    "    input_notes_b_reshape = input_notes_b\n",
    "\n",
    "    input_notes_b_reshape = np.concatenate([input_notes_b_reshape[1:, :, :], np.zeros((1, num_beats, 2))], axis=0)\n",
    "\n",
    "    for l in range(size):\n",
    "        print(l)\n",
    "        \n",
    "\n",
    "        out_arr = np.zeros((num_notes,2))\n",
    "        probs_arr = np.zeros((num_notes,2))\n",
    "\n",
    "        # model.layers[5].reset_states()\n",
    "\n",
    "        reset = True\n",
    "        model.reset_states()\n",
    "    \n",
    "        for i in range(num_notes):\n",
    "            #TODO: Speed this up by not repredicting the time axis layers.\n",
    "\n",
    "            if i == 0:\n",
    "                probs = model.predict([input_notes_reshape, input_notes_b_reshape], batch_size=128)\n",
    "                \n",
    "                # Sub Model\n",
    "\n",
    "                intermediate_layer_model = tf.keras.Model(inputs=model.input,\n",
    "                                                outputs=model.layers[3].output)\n",
    "                intermediate_output = intermediate_layer_model.predict([input_notes_reshape, input_notes_b_reshape], batch_size=128)\n",
    "\n",
    "                input_shape = (128, 200)\n",
    "\n",
    "                sub_model = tf.keras.Sequential()\n",
    "                new_input = tf.keras.Input(shape=input_shape,  name=\"new_input\")\n",
    "                old_input_b = model.layers[4]\n",
    "\n",
    "\n",
    "                concat_lay = model.layers[5]([new_input, old_input_b.output])\n",
    "                \n",
    "\n",
    "                mod_notes = model.layers[6](concat_lay)\n",
    "                mod_notes = model.layers[7](mod_notes)\n",
    "\n",
    "                sub_model = tf.keras.Model(inputs=[new_input, old_input_b.input], outputs=[mod_notes])\n",
    "\n",
    " \n",
    "\n",
    "            else:\n",
    "                probs = sub_model.predict([intermediate_output, input_notes_b_reshape], batch_size=128)\n",
    "        \n",
    "            last_new_play_artic_probs = probs[-1, i, :]\n",
    "            last_new_play_artic = np.random.binomial(1, last_new_play_artic_probs, size=None).reshape((1, 2))\n",
    "\n",
    "\n",
    "            if i < (num_notes - 1):\n",
    "                input_notes_b_reshape[-1, i:i+1, :] = last_new_play_artic\n",
    "            else:\n",
    "                print(\"Is this getting hit\")\n",
    "                input_notes_b_reshape = np.concatenate([input_notes_b_reshape[1:, :, :], np.zeros((1, num_beats, 2))], axis=0)\n",
    "\n",
    "            probs_arr[i, 0] = last_new_play_artic_probs[0]\n",
    "            probs_arr[i, 1] = last_new_play_artic_probs[1]\n",
    "            out_arr[i, 0] = last_new_play_artic[0, 0]\n",
    "            out_arr[i, 1] = last_new_play_artic[0, 1]\n",
    "\n",
    "        out = out_arr.reshape((256,1))\n",
    "        probs = probs_arr.reshape((256,1))\n",
    "\n",
    "\n",
    "        # Need to window across out to get the input form again.\n",
    "        out = out.reshape((256,1))\n",
    "        probs = probs.reshape((256,))\n",
    "        outputs.append(out.reshape(256,))\n",
    "        all_probs.append(probs)\n",
    "\n",
    "        # note_vicinity = total_vicinity-4-12-12-1\n",
    "        next_pred, _, _ = MidiSupport().windowed_data_across_notes_time(out, mask_length_x=note_vicinity, return_labels=False)# Return (total_vicinity, 128)\n",
    "\n",
    "        # Get array of Midi values for each note value\n",
    "        n_notes = 128\n",
    "        midi_row = MidiSupport().add_midi_value(next_pred, n_notes)\n",
    "\n",
    "        # Get array of one hot encoded pitch values for each note value\n",
    "        pitchclass_rows = MidiSupport().calculate_pitchclass(midi_row, next_pred)\n",
    "\n",
    "        # Add total_pitch count repeated for each note window\n",
    "        previous_context = MidiSupport().build_context(next_pred, midi_row, pitchclass_rows)\n",
    "\n",
    "        midi_row = midi_row.reshape((1, -1))\n",
    "        next_pred = np.vstack((next_pred, midi_row, pitchclass_rows, previous_context))\n",
    "        \n",
    "        last_beats_int += 1\n",
    "        last_beats_int = last_beats_int%16\n",
    "        next_beats_ind = np.array([int(x) for x in bin(last_beats_int)[2:].zfill(4)])\n",
    "        next_beats_ind = next_beats_ind.reshape((4, 1))\n",
    "        next_beats_ind = np.repeat(next_beats_ind, num_notes, axis=1)\n",
    "\n",
    "        # TODO, check if beat is correctly increasing: might need to flip it before adding\n",
    "        last_new_note = np.concatenate([next_pred, next_beats_ind])\n",
    "        last_new_note = last_new_note[np.newaxis, :, :] # Shape now  (1, 28, 128)\n",
    "        last_new_note = np.swapaxes(last_new_note, 1, 2) # Shape now  (1, 128, 28)\n",
    "        # last_new_note = np.swapaxes(last_new_note, 0, 1) # Shape now  (128, 1, 28)\n",
    "        \n",
    "        together = np.concatenate([input_notes_reshape[1:, :, :], last_new_note], axis=0)\n",
    "        input_notes_reshape = together\n",
    "\n",
    "    outputs_joined = pd.DataFrame(outputs)\n",
    "    all_probs_joined = pd.DataFrame(all_probs)\n",
    "    return outputs_joined, all_probs_joined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 128, 53) dtype=float32 (created by layer 'input_a')>,\n",
       " <KerasTensor: shape=(None, 128, 2) dtype=float32 (created by layer 'input_b')>]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.layers[3].output, model.layers[4].input\n",
    "model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_21\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_22\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "1\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_23\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_24\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "2\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_25\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_26\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "3\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_27\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_28\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "4\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_29\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_30\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "5\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_31\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_32\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "6\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_33\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_34\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "7\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_35\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_36\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "8\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_37\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_38\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "9\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_39\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_40\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "10\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_41\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_42\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "11\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_43\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_44\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "12\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_45\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_46\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "13\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_47\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_48\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "14\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_49\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_50\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "15\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_51\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_52\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "16\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_53\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_54\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "17\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_55\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_56\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "18\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_57\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_58\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "19\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_59\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_60\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "20\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_61\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_62\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "21\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_63\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_64\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "22\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_65\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_66\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "23\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_67\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_68\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "24\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_69\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_70\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "25\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_71\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_72\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "26\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_73\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_74\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "27\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_75\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_76\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "28\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_77\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_78\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "29\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_79\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_80\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "30\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_81\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_82\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "31\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_83\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_84\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "32\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_85\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_86\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "33\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_87\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_88\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "34\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_89\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_90\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "35\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_91\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_92\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "36\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_93\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_94\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "37\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_95\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_96\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "38\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_97\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_98\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "39\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_99\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_100\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "40\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_101\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_102\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "41\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_103\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_104\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "42\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_105\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_106\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "43\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_107\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_108\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "44\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_109\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_110\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "45\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_111\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_112\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "46\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_113\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_114\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "47\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_115\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_116\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "48\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_117\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_118\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "49\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_119\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_120\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "50\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_121\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_122\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "51\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_123\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_124\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "52\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_125\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_126\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "53\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_127\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_128\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "54\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_129\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_130\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "55\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_131\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_132\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "56\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_133\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_134\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "57\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_135\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_136\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "58\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_137\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_138\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "59\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_139\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_140\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "60\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_141\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_142\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "61\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_143\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_144\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "62\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_145\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_146\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "63\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_147\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_148\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "64\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_149\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_150\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "65\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_151\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_152\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "66\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_153\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_154\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "67\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_155\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_156\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "68\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_157\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_158\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "69\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_159\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_160\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "70\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_161\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_162\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "71\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_163\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_164\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "72\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_165\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_166\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "73\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_167\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_168\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "74\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_169\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_170\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "75\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_171\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_172\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "76\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_173\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_174\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "77\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_175\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_176\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "78\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_177\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_178\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "79\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_179\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_180\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "80\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_181\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_182\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "81\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_183\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_184\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "82\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_185\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_186\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "83\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_187\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_188\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "84\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_189\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_190\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "85\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_191\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_192\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "86\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_193\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_194\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "87\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_195\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_196\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "88\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_197\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_198\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "89\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_199\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_200\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "90\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_201\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_202\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "91\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_203\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_204\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "92\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_205\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_206\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "93\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_207\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_208\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "94\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_209\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_210\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "95\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_211\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_212\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "96\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_213\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_214\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "97\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_215\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_216\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "98\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_217\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_218\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n",
      "99\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_219\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      " Swaplayer input tensor shape is (128, 128, 53)\n",
      " Swaplayer output tensor shape is (128, 128, 53)\n",
      " Swaplayer input tensor shape is (128, 128, 200)\n",
      " Swaplayer output tensor shape is (128, 128, 200)\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_220\" was not an Input tensor, it was generated by layer \"input_b\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 128, 2), dtype=tf.float32, name='input_b'), name='input_b', description=\"created by layer 'input_b'\")\n",
      "Is this getting hit\n"
     ]
    }
   ],
   "source": [
    "# %prun \n",
    "outputs_joined, all_probs_joined = predict_time_note_model(size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 2], [3,4], [5,6]]).reshape((6, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 256), (1, 256))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.T.shape, input_notes_reshape[-1, :, 12:14].reshape((1, 256)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    10   11   12   13   14   \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   15   16   17   18   19   20   21   22   23   24   25   26   27   28   29   \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   30   31   32   33   34   35   36   37   38   39   40   41   42   43   44   \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   45   46   47   48   49   50   51   52   53   54   55   56   57   58   59   \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   60   61   62   63   64   65   66   67   68   69   70   71   72   73   74   \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   75   76   77   78   79   80   81   82   83   84   85   86   87   88   89   \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   90   91   92   93   94   95   96   97   98   99   100  101  102  103  104  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
       "\n",
       "   105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  \\\n",
       "0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
       "\n",
       "   120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   135  136  137  138  139  140  141  142  143  144  145  146  147  148  149  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   150  151  152  153  154  155  156  157  158  159  160  161  162  163  164  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   165  166  167  168  169  170  171  172  173  174  175  176  177  178  179  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   180  181  182  183  184  185  186  187  188  189  190  191  192  193  194  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   195  196  197  198  199  200  201  202  203  204  205  206  207  208  209  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   210  211  212  213  214  215  216  217  218  219  220  221  222  223  224  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   240  241  242  243  244  245  246  247  248  249  250  251  252  253  254  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   255  \n",
       "0  0.0  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(out.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    10   11   12   13   14   \\\n",
       "0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   15   16   17   18   19   20   21   22   23   24   25   26   27   28   29   \\\n",
       "0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   30   31   32   33   34   35   36   37   38   39   40   41   42   43   44   \\\n",
       "0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   45   46   47   48   49   50   51   52   53   54   55   56   57   58   59   \\\n",
       "0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   60   61   62   63   64   65   66   67   68   69   70   71   72   73   74   \\\n",
       "0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   75   76   77   78   79   80   81   82   83   84   85   86   87   88   89   \\\n",
       "0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   90   91   92   93   94   95   96   97   98   99   100  101  102  103  104  \\\n",
       "0    0    0    0    0    0    0    0    1    0    0    0    0    0    0    0   \n",
       "\n",
       "   105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  \\\n",
       "0    1    0    0    0    0    0    1    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   135  136  137  138  139  140  141  142  143  144  145  146  147  148  149  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   150  151  152  153  154  155  156  157  158  159  160  161  162  163  164  \\\n",
       "0    0    0    0    0    0    1    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   165  166  167  168  169  170  171  172  173  174  175  176  177  178  179  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   180  181  182  183  184  185  186  187  188  189  190  191  192  193  194  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   195  196  197  198  199  200  201  202  203  204  205  206  207  208  209  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   210  211  212  213  214  215  216  217  218  219  220  221  222  223  224  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   240  241  242  243  244  245  246  247  248  249  250  251  252  253  254  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   255  \n",
       "0    0  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(input_notes_reshape[-1, :, 12:14].reshape((1, 256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((256, 32, 53), (256, 32, 2), (256, 1, 53), (256, 1, 2))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_notes_reshape.shape, input_notes_b_reshape.shape, input_notes_reshape[:, -1:, :].shape, input_notes_b_reshape[:, -1:, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<KerasTensor: shape=(32, 32, 53) dtype=float32 (created by layer 'input_a')>,\n",
       " <KerasTensor: shape=(32, 32, 200) dtype=float32 (created by layer 'swap_layer_10')>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].output, model.layers[2].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 32, 2)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probs = model.predict([input_notes_reshape[:, -1:, :], input_notes_b_reshape[:, -1:, :]], batch_size=1)\n",
    "probs = model.predict([input_notes_reshape, input_notes_b_reshape], batch_size=32)\n",
    "probs.shape\n",
    "# input_notes_reshape.shape, input_notes_b_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60, 32, 53), (60, 32, 2))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_notes_reshape.shape, input_notes_b_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4608, 32, 53), (4608, 32, 2))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxx_2.shape, yyy_other_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sequence_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 256)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_joined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d5533760>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2880x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from src.utils.visualization import note_and_artic_to_one\n",
    "\n",
    "# data = yyy_pred.reshape((1152, 128, 2))\n",
    "# data = all_probs_joined[:,  :, 0]\n",
    "\n",
    "# outputs_joined, all_probs_joined \n",
    "\n",
    "data = note_and_artic_to_one(all_probs_joined, what=\"note_hold\")\n",
    "plt.rcParams[\"figure.figsize\"] = (40,10)\n",
    "plt.imshow(data, cmap='hot', interpolation='nearest', aspect=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>0.001460</td>\n",
       "      <td>0.001371</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.001370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.001587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.001732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.001783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.001842</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.001845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.001742</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.001740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.001671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.001656</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.001658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.001742</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.001743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000695</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.001745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.001723</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.001725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>0.001637</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.001638</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.001638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.001638</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.001638</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.001638</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.001638</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.001638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001377</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.534315</td>\n",
       "      <td>0.013638</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.001519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.001373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.001326</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.001325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.001371</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.001371</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.001371</td>\n",
       "      <td>0.001178</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001178</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.001178</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.001178</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.001178</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.001178</td>\n",
       "      <td>0.001370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.001113</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.001341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.001321</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.001323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.001421</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.001423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0   0.001376  0.001503  0.001460  0.001371  0.001459  0.001370  0.001458   \n",
       "1   0.001081  0.001585  0.001084  0.001588  0.001084  0.001588  0.001084   \n",
       "2   0.000845  0.001726  0.000843  0.001732  0.000843  0.001732  0.000843   \n",
       "3   0.000699  0.001779  0.000698  0.001783  0.000698  0.001783  0.000698   \n",
       "4   0.000632  0.001842  0.000632  0.001845  0.000632  0.001845  0.000632   \n",
       "5   0.000634  0.001741  0.000633  0.001742  0.000633  0.001741  0.000633   \n",
       "6   0.000648  0.001672  0.000646  0.001672  0.000645  0.001671  0.000645   \n",
       "7   0.000648  0.001656  0.000645  0.001657  0.000644  0.001657  0.000644   \n",
       "8   0.000658  0.001733  0.000655  0.001742  0.000655  0.001743  0.000655   \n",
       "9   0.000695  0.001729  0.000697  0.001741  0.000698  0.001744  0.000698   \n",
       "10  0.000758  0.001716  0.000758  0.001723  0.000759  0.001724  0.000759   \n",
       "11  0.000799  0.001619  0.000808  0.001634  0.000809  0.001637  0.000810   \n",
       "12  0.001000  0.001519  0.001002  0.001520  0.001002  0.001520  0.001001   \n",
       "13  0.001157  0.001374  0.001155  0.001374  0.001155  0.001373  0.001155   \n",
       "14  0.001134  0.001325  0.001134  0.001326  0.001134  0.001325  0.001134   \n",
       "15  0.001183  0.001372  0.001179  0.001371  0.001179  0.001371  0.001179   \n",
       "16  0.001096  0.001366  0.001095  0.001367  0.001095  0.001367  0.001095   \n",
       "17  0.001113  0.001341  0.001114  0.001342  0.001114  0.001342  0.001114   \n",
       "18  0.001081  0.001321  0.001083  0.001323  0.001083  0.001323  0.001083   \n",
       "19  0.001080  0.001421  0.001081  0.001423  0.001081  0.001423  0.001081   \n",
       "\n",
       "         7         8         9    ...       246       247       248       249  \\\n",
       "0   0.001370  0.001458  0.001370  ...  0.001458  0.001370  0.001458  0.001370   \n",
       "1   0.001588  0.001085  0.001587  ...  0.001085  0.001587  0.001085  0.001587   \n",
       "2   0.001732  0.000843  0.001732  ...  0.000843  0.001732  0.000843  0.001732   \n",
       "3   0.001783  0.000698  0.001783  ...  0.000698  0.001783  0.000698  0.001783   \n",
       "4   0.001845  0.000632  0.001845  ...  0.000632  0.001845  0.000632  0.001845   \n",
       "5   0.001741  0.000633  0.001741  ...  0.000633  0.001740  0.000633  0.001740   \n",
       "6   0.001671  0.000645  0.001671  ...  0.000645  0.001671  0.000645  0.001671   \n",
       "7   0.001657  0.000644  0.001658  ...  0.000644  0.001658  0.000644  0.001658   \n",
       "8   0.001743  0.000655  0.001743  ...  0.000655  0.001743  0.000655  0.001743   \n",
       "9   0.001745  0.000698  0.001745  ...  0.000698  0.001745  0.000698  0.001745   \n",
       "10  0.001725  0.000759  0.001725  ...  0.000759  0.001725  0.000759  0.001725   \n",
       "11  0.001638  0.000810  0.001638  ...  0.000810  0.001638  0.000810  0.001638   \n",
       "12  0.001519  0.001001  0.001519  ...  0.001377  0.001525  0.001211  0.001702   \n",
       "13  0.001373  0.001155  0.001373  ...  0.001155  0.001373  0.001155  0.001373   \n",
       "14  0.001325  0.001134  0.001325  ...  0.001134  0.001325  0.001134  0.001325   \n",
       "15  0.001371  0.001178  0.001370  ...  0.001178  0.001370  0.001178  0.001370   \n",
       "16  0.001367  0.001095  0.001367  ...  0.001095  0.001367  0.001095  0.001367   \n",
       "17  0.001342  0.001114  0.001342  ...  0.001114  0.001341  0.001114  0.001341   \n",
       "18  0.001323  0.001083  0.001323  ...  0.001083  0.001323  0.001083  0.001323   \n",
       "19  0.001423  0.001080  0.001423  ...  0.001080  0.001423  0.001080  0.001423   \n",
       "\n",
       "         250       251       252       253       254       255  \n",
       "0   0.001458  0.001370  0.001458  0.001370  0.001458  0.001370  \n",
       "1   0.001085  0.001587  0.001085  0.001587  0.001085  0.001587  \n",
       "2   0.000843  0.001732  0.000843  0.001732  0.000843  0.001732  \n",
       "3   0.000698  0.001783  0.000698  0.001783  0.000698  0.001783  \n",
       "4   0.000632  0.001845  0.000632  0.001845  0.000632  0.001845  \n",
       "5   0.000633  0.001740  0.000633  0.001740  0.000633  0.001740  \n",
       "6   0.000645  0.001671  0.000645  0.001671  0.000645  0.001671  \n",
       "7   0.000644  0.001658  0.000644  0.001658  0.000644  0.001658  \n",
       "8   0.000655  0.001743  0.000655  0.001743  0.000655  0.001743  \n",
       "9   0.000698  0.001745  0.000698  0.001745  0.000698  0.001745  \n",
       "10  0.000759  0.001725  0.000759  0.001725  0.000759  0.001725  \n",
       "11  0.000810  0.001638  0.000810  0.001638  0.000810  0.001638  \n",
       "12  0.001004  0.001523  0.534315  0.013638  0.001001  0.001519  \n",
       "13  0.001155  0.001373  0.001155  0.001373  0.001155  0.001373  \n",
       "14  0.001134  0.001325  0.001134  0.001325  0.001134  0.001325  \n",
       "15  0.001178  0.001370  0.001178  0.001370  0.001178  0.001370  \n",
       "16  0.001095  0.001367  0.001095  0.001367  0.001095  0.001367  \n",
       "17  0.001114  0.001341  0.001114  0.001341  0.001114  0.001341  \n",
       "18  0.001083  0.001323  0.001083  0.001323  0.001083  0.001323  \n",
       "19  0.001080  0.001423  0.001080  0.001423  0.001080  0.001423  \n",
       "\n",
       "[20 rows x 256 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_probs_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_139 (LSTM)             (None, 10, 100)           42400     \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 10, 2)             202       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42,602\n",
      "Trainable params: 42,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_139 (LSTM)             (None, 10, 100)           42400     \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 10, 2)             202       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42,602\n",
      "Trainable params: 42,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_shape = (10, 5)\n",
    "\n",
    "model_tst = tf.keras.Sequential()\n",
    "\n",
    "model_tst.add(tf.keras.Input(shape=input_shape))\n",
    "model_tst.add(tf.keras.layers.LSTM(100,  return_sequences=True))\n",
    "model_tst.add(tf.keras.layers.Dense(2, activation=\"sigmoid\"))\n",
    "\n",
    "model_tst.summary()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "model_tst.compile(\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        # metrics=[tf.keras.metrics.BinaryCrossentropy()\n",
    "        metrics=[\"mse\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "model_tst.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "      <th>311</th>\n",
       "      <th>312</th>\n",
       "      <th>313</th>\n",
       "      <th>314</th>\n",
       "      <th>315</th>\n",
       "      <th>316</th>\n",
       "      <th>317</th>\n",
       "      <th>318</th>\n",
       "      <th>319</th>\n",
       "      <th>320</th>\n",
       "      <th>321</th>\n",
       "      <th>322</th>\n",
       "      <th>323</th>\n",
       "      <th>324</th>\n",
       "      <th>325</th>\n",
       "      <th>326</th>\n",
       "      <th>327</th>\n",
       "      <th>328</th>\n",
       "      <th>329</th>\n",
       "      <th>330</th>\n",
       "      <th>331</th>\n",
       "      <th>332</th>\n",
       "      <th>333</th>\n",
       "      <th>334</th>\n",
       "      <th>335</th>\n",
       "      <th>336</th>\n",
       "      <th>337</th>\n",
       "      <th>338</th>\n",
       "      <th>339</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>345</th>\n",
       "      <th>346</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "      <th>350</th>\n",
       "      <th>351</th>\n",
       "      <th>352</th>\n",
       "      <th>353</th>\n",
       "      <th>354</th>\n",
       "      <th>355</th>\n",
       "      <th>356</th>\n",
       "      <th>357</th>\n",
       "      <th>358</th>\n",
       "      <th>359</th>\n",
       "      <th>360</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>363</th>\n",
       "      <th>364</th>\n",
       "      <th>365</th>\n",
       "      <th>366</th>\n",
       "      <th>367</th>\n",
       "      <th>368</th>\n",
       "      <th>369</th>\n",
       "      <th>370</th>\n",
       "      <th>371</th>\n",
       "      <th>372</th>\n",
       "      <th>373</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>384</th>\n",
       "      <th>385</th>\n",
       "      <th>386</th>\n",
       "      <th>387</th>\n",
       "      <th>388</th>\n",
       "      <th>389</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>400</th>\n",
       "      <th>401</th>\n",
       "      <th>402</th>\n",
       "      <th>403</th>\n",
       "      <th>404</th>\n",
       "      <th>405</th>\n",
       "      <th>406</th>\n",
       "      <th>407</th>\n",
       "      <th>408</th>\n",
       "      <th>409</th>\n",
       "      <th>410</th>\n",
       "      <th>411</th>\n",
       "      <th>412</th>\n",
       "      <th>413</th>\n",
       "      <th>414</th>\n",
       "      <th>415</th>\n",
       "      <th>416</th>\n",
       "      <th>417</th>\n",
       "      <th>418</th>\n",
       "      <th>419</th>\n",
       "      <th>420</th>\n",
       "      <th>421</th>\n",
       "      <th>422</th>\n",
       "      <th>423</th>\n",
       "      <th>424</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "      <th>436</th>\n",
       "      <th>437</th>\n",
       "      <th>438</th>\n",
       "      <th>439</th>\n",
       "      <th>440</th>\n",
       "      <th>441</th>\n",
       "      <th>442</th>\n",
       "      <th>443</th>\n",
       "      <th>444</th>\n",
       "      <th>445</th>\n",
       "      <th>446</th>\n",
       "      <th>447</th>\n",
       "      <th>448</th>\n",
       "      <th>449</th>\n",
       "      <th>450</th>\n",
       "      <th>451</th>\n",
       "      <th>452</th>\n",
       "      <th>453</th>\n",
       "      <th>454</th>\n",
       "      <th>455</th>\n",
       "      <th>456</th>\n",
       "      <th>457</th>\n",
       "      <th>458</th>\n",
       "      <th>459</th>\n",
       "      <th>460</th>\n",
       "      <th>461</th>\n",
       "      <th>462</th>\n",
       "      <th>463</th>\n",
       "      <th>464</th>\n",
       "      <th>465</th>\n",
       "      <th>466</th>\n",
       "      <th>467</th>\n",
       "      <th>468</th>\n",
       "      <th>469</th>\n",
       "      <th>470</th>\n",
       "      <th>471</th>\n",
       "      <th>472</th>\n",
       "      <th>473</th>\n",
       "      <th>474</th>\n",
       "      <th>475</th>\n",
       "      <th>476</th>\n",
       "      <th>477</th>\n",
       "      <th>478</th>\n",
       "      <th>479</th>\n",
       "      <th>480</th>\n",
       "      <th>481</th>\n",
       "      <th>482</th>\n",
       "      <th>483</th>\n",
       "      <th>484</th>\n",
       "      <th>485</th>\n",
       "      <th>486</th>\n",
       "      <th>487</th>\n",
       "      <th>488</th>\n",
       "      <th>489</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>500</th>\n",
       "      <th>501</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "      <th>512</th>\n",
       "      <th>513</th>\n",
       "      <th>514</th>\n",
       "      <th>515</th>\n",
       "      <th>516</th>\n",
       "      <th>517</th>\n",
       "      <th>518</th>\n",
       "      <th>519</th>\n",
       "      <th>520</th>\n",
       "      <th>521</th>\n",
       "      <th>522</th>\n",
       "      <th>523</th>\n",
       "      <th>524</th>\n",
       "      <th>525</th>\n",
       "      <th>526</th>\n",
       "      <th>527</th>\n",
       "      <th>528</th>\n",
       "      <th>529</th>\n",
       "      <th>530</th>\n",
       "      <th>531</th>\n",
       "      <th>532</th>\n",
       "      <th>533</th>\n",
       "      <th>534</th>\n",
       "      <th>535</th>\n",
       "      <th>536</th>\n",
       "      <th>537</th>\n",
       "      <th>538</th>\n",
       "      <th>539</th>\n",
       "      <th>540</th>\n",
       "      <th>541</th>\n",
       "      <th>542</th>\n",
       "      <th>543</th>\n",
       "      <th>544</th>\n",
       "      <th>545</th>\n",
       "      <th>546</th>\n",
       "      <th>547</th>\n",
       "      <th>548</th>\n",
       "      <th>549</th>\n",
       "      <th>550</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "      <th>561</th>\n",
       "      <th>562</th>\n",
       "      <th>563</th>\n",
       "      <th>564</th>\n",
       "      <th>565</th>\n",
       "      <th>566</th>\n",
       "      <th>567</th>\n",
       "      <th>568</th>\n",
       "      <th>569</th>\n",
       "      <th>570</th>\n",
       "      <th>571</th>\n",
       "      <th>572</th>\n",
       "      <th>573</th>\n",
       "      <th>574</th>\n",
       "      <th>575</th>\n",
       "      <th>576</th>\n",
       "      <th>577</th>\n",
       "      <th>578</th>\n",
       "      <th>579</th>\n",
       "      <th>580</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>590</th>\n",
       "      <th>591</th>\n",
       "      <th>592</th>\n",
       "      <th>593</th>\n",
       "      <th>594</th>\n",
       "      <th>595</th>\n",
       "      <th>596</th>\n",
       "      <th>597</th>\n",
       "      <th>598</th>\n",
       "      <th>599</th>\n",
       "      <th>600</th>\n",
       "      <th>601</th>\n",
       "      <th>602</th>\n",
       "      <th>603</th>\n",
       "      <th>604</th>\n",
       "      <th>605</th>\n",
       "      <th>606</th>\n",
       "      <th>607</th>\n",
       "      <th>608</th>\n",
       "      <th>609</th>\n",
       "      <th>610</th>\n",
       "      <th>611</th>\n",
       "      <th>612</th>\n",
       "      <th>613</th>\n",
       "      <th>614</th>\n",
       "      <th>615</th>\n",
       "      <th>616</th>\n",
       "      <th>617</th>\n",
       "      <th>618</th>\n",
       "      <th>619</th>\n",
       "      <th>620</th>\n",
       "      <th>621</th>\n",
       "      <th>622</th>\n",
       "      <th>623</th>\n",
       "      <th>624</th>\n",
       "      <th>625</th>\n",
       "      <th>626</th>\n",
       "      <th>627</th>\n",
       "      <th>628</th>\n",
       "      <th>629</th>\n",
       "      <th>630</th>\n",
       "      <th>631</th>\n",
       "      <th>632</th>\n",
       "      <th>633</th>\n",
       "      <th>634</th>\n",
       "      <th>635</th>\n",
       "      <th>636</th>\n",
       "      <th>637</th>\n",
       "      <th>638</th>\n",
       "      <th>639</th>\n",
       "      <th>640</th>\n",
       "      <th>641</th>\n",
       "      <th>642</th>\n",
       "      <th>643</th>\n",
       "      <th>644</th>\n",
       "      <th>645</th>\n",
       "      <th>646</th>\n",
       "      <th>647</th>\n",
       "      <th>648</th>\n",
       "      <th>649</th>\n",
       "      <th>650</th>\n",
       "      <th>651</th>\n",
       "      <th>652</th>\n",
       "      <th>653</th>\n",
       "      <th>654</th>\n",
       "      <th>655</th>\n",
       "      <th>656</th>\n",
       "      <th>657</th>\n",
       "      <th>658</th>\n",
       "      <th>659</th>\n",
       "      <th>660</th>\n",
       "      <th>661</th>\n",
       "      <th>662</th>\n",
       "      <th>663</th>\n",
       "      <th>664</th>\n",
       "      <th>665</th>\n",
       "      <th>666</th>\n",
       "      <th>667</th>\n",
       "      <th>668</th>\n",
       "      <th>669</th>\n",
       "      <th>670</th>\n",
       "      <th>671</th>\n",
       "      <th>672</th>\n",
       "      <th>673</th>\n",
       "      <th>674</th>\n",
       "      <th>675</th>\n",
       "      <th>676</th>\n",
       "      <th>677</th>\n",
       "      <th>678</th>\n",
       "      <th>679</th>\n",
       "      <th>680</th>\n",
       "      <th>681</th>\n",
       "      <th>682</th>\n",
       "      <th>683</th>\n",
       "      <th>684</th>\n",
       "      <th>685</th>\n",
       "      <th>686</th>\n",
       "      <th>687</th>\n",
       "      <th>688</th>\n",
       "      <th>689</th>\n",
       "      <th>690</th>\n",
       "      <th>691</th>\n",
       "      <th>692</th>\n",
       "      <th>693</th>\n",
       "      <th>694</th>\n",
       "      <th>695</th>\n",
       "      <th>696</th>\n",
       "      <th>697</th>\n",
       "      <th>698</th>\n",
       "      <th>699</th>\n",
       "      <th>700</th>\n",
       "      <th>701</th>\n",
       "      <th>702</th>\n",
       "      <th>703</th>\n",
       "      <th>704</th>\n",
       "      <th>705</th>\n",
       "      <th>706</th>\n",
       "      <th>707</th>\n",
       "      <th>708</th>\n",
       "      <th>709</th>\n",
       "      <th>710</th>\n",
       "      <th>711</th>\n",
       "      <th>712</th>\n",
       "      <th>713</th>\n",
       "      <th>714</th>\n",
       "      <th>715</th>\n",
       "      <th>716</th>\n",
       "      <th>717</th>\n",
       "      <th>718</th>\n",
       "      <th>719</th>\n",
       "      <th>720</th>\n",
       "      <th>721</th>\n",
       "      <th>722</th>\n",
       "      <th>723</th>\n",
       "      <th>724</th>\n",
       "      <th>725</th>\n",
       "      <th>726</th>\n",
       "      <th>727</th>\n",
       "      <th>728</th>\n",
       "      <th>729</th>\n",
       "      <th>730</th>\n",
       "      <th>731</th>\n",
       "      <th>732</th>\n",
       "      <th>733</th>\n",
       "      <th>734</th>\n",
       "      <th>735</th>\n",
       "      <th>736</th>\n",
       "      <th>737</th>\n",
       "      <th>738</th>\n",
       "      <th>739</th>\n",
       "      <th>740</th>\n",
       "      <th>741</th>\n",
       "      <th>742</th>\n",
       "      <th>743</th>\n",
       "      <th>744</th>\n",
       "      <th>745</th>\n",
       "      <th>746</th>\n",
       "      <th>747</th>\n",
       "      <th>748</th>\n",
       "      <th>749</th>\n",
       "      <th>750</th>\n",
       "      <th>751</th>\n",
       "      <th>752</th>\n",
       "      <th>753</th>\n",
       "      <th>754</th>\n",
       "      <th>755</th>\n",
       "      <th>756</th>\n",
       "      <th>757</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>768</th>\n",
       "      <th>769</th>\n",
       "      <th>770</th>\n",
       "      <th>771</th>\n",
       "      <th>772</th>\n",
       "      <th>773</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "      <th>785</th>\n",
       "      <th>786</th>\n",
       "      <th>787</th>\n",
       "      <th>788</th>\n",
       "      <th>789</th>\n",
       "      <th>790</th>\n",
       "      <th>791</th>\n",
       "      <th>792</th>\n",
       "      <th>793</th>\n",
       "      <th>794</th>\n",
       "      <th>795</th>\n",
       "      <th>796</th>\n",
       "      <th>797</th>\n",
       "      <th>798</th>\n",
       "      <th>799</th>\n",
       "      <th>800</th>\n",
       "      <th>801</th>\n",
       "      <th>802</th>\n",
       "      <th>803</th>\n",
       "      <th>804</th>\n",
       "      <th>805</th>\n",
       "      <th>806</th>\n",
       "      <th>807</th>\n",
       "      <th>808</th>\n",
       "      <th>809</th>\n",
       "      <th>810</th>\n",
       "      <th>811</th>\n",
       "      <th>812</th>\n",
       "      <th>813</th>\n",
       "      <th>814</th>\n",
       "      <th>815</th>\n",
       "      <th>816</th>\n",
       "      <th>817</th>\n",
       "      <th>818</th>\n",
       "      <th>819</th>\n",
       "      <th>820</th>\n",
       "      <th>821</th>\n",
       "      <th>822</th>\n",
       "      <th>823</th>\n",
       "      <th>824</th>\n",
       "      <th>825</th>\n",
       "      <th>826</th>\n",
       "      <th>827</th>\n",
       "      <th>828</th>\n",
       "      <th>829</th>\n",
       "      <th>830</th>\n",
       "      <th>831</th>\n",
       "      <th>832</th>\n",
       "      <th>833</th>\n",
       "      <th>834</th>\n",
       "      <th>835</th>\n",
       "      <th>836</th>\n",
       "      <th>837</th>\n",
       "      <th>838</th>\n",
       "      <th>839</th>\n",
       "      <th>840</th>\n",
       "      <th>841</th>\n",
       "      <th>842</th>\n",
       "      <th>843</th>\n",
       "      <th>844</th>\n",
       "      <th>845</th>\n",
       "      <th>846</th>\n",
       "      <th>847</th>\n",
       "      <th>848</th>\n",
       "      <th>849</th>\n",
       "      <th>850</th>\n",
       "      <th>851</th>\n",
       "      <th>852</th>\n",
       "      <th>853</th>\n",
       "      <th>854</th>\n",
       "      <th>855</th>\n",
       "      <th>856</th>\n",
       "      <th>857</th>\n",
       "      <th>858</th>\n",
       "      <th>859</th>\n",
       "      <th>860</th>\n",
       "      <th>861</th>\n",
       "      <th>862</th>\n",
       "      <th>863</th>\n",
       "      <th>864</th>\n",
       "      <th>865</th>\n",
       "      <th>866</th>\n",
       "      <th>867</th>\n",
       "      <th>868</th>\n",
       "      <th>869</th>\n",
       "      <th>870</th>\n",
       "      <th>871</th>\n",
       "      <th>872</th>\n",
       "      <th>873</th>\n",
       "      <th>874</th>\n",
       "      <th>875</th>\n",
       "      <th>876</th>\n",
       "      <th>877</th>\n",
       "      <th>878</th>\n",
       "      <th>879</th>\n",
       "      <th>880</th>\n",
       "      <th>881</th>\n",
       "      <th>882</th>\n",
       "      <th>883</th>\n",
       "      <th>884</th>\n",
       "      <th>885</th>\n",
       "      <th>886</th>\n",
       "      <th>887</th>\n",
       "      <th>888</th>\n",
       "      <th>889</th>\n",
       "      <th>890</th>\n",
       "      <th>891</th>\n",
       "      <th>892</th>\n",
       "      <th>893</th>\n",
       "      <th>894</th>\n",
       "      <th>895</th>\n",
       "      <th>896</th>\n",
       "      <th>897</th>\n",
       "      <th>898</th>\n",
       "      <th>899</th>\n",
       "      <th>900</th>\n",
       "      <th>901</th>\n",
       "      <th>902</th>\n",
       "      <th>903</th>\n",
       "      <th>904</th>\n",
       "      <th>905</th>\n",
       "      <th>906</th>\n",
       "      <th>907</th>\n",
       "      <th>908</th>\n",
       "      <th>909</th>\n",
       "      <th>910</th>\n",
       "      <th>911</th>\n",
       "      <th>912</th>\n",
       "      <th>913</th>\n",
       "      <th>914</th>\n",
       "      <th>915</th>\n",
       "      <th>916</th>\n",
       "      <th>917</th>\n",
       "      <th>918</th>\n",
       "      <th>919</th>\n",
       "      <th>920</th>\n",
       "      <th>921</th>\n",
       "      <th>922</th>\n",
       "      <th>923</th>\n",
       "      <th>924</th>\n",
       "      <th>925</th>\n",
       "      <th>926</th>\n",
       "      <th>927</th>\n",
       "      <th>928</th>\n",
       "      <th>929</th>\n",
       "      <th>930</th>\n",
       "      <th>931</th>\n",
       "      <th>932</th>\n",
       "      <th>933</th>\n",
       "      <th>934</th>\n",
       "      <th>935</th>\n",
       "      <th>936</th>\n",
       "      <th>937</th>\n",
       "      <th>938</th>\n",
       "      <th>939</th>\n",
       "      <th>940</th>\n",
       "      <th>941</th>\n",
       "      <th>942</th>\n",
       "      <th>943</th>\n",
       "      <th>944</th>\n",
       "      <th>945</th>\n",
       "      <th>946</th>\n",
       "      <th>947</th>\n",
       "      <th>948</th>\n",
       "      <th>949</th>\n",
       "      <th>950</th>\n",
       "      <th>951</th>\n",
       "      <th>952</th>\n",
       "      <th>953</th>\n",
       "      <th>954</th>\n",
       "      <th>955</th>\n",
       "      <th>956</th>\n",
       "      <th>957</th>\n",
       "      <th>958</th>\n",
       "      <th>959</th>\n",
       "      <th>960</th>\n",
       "      <th>961</th>\n",
       "      <th>962</th>\n",
       "      <th>963</th>\n",
       "      <th>964</th>\n",
       "      <th>965</th>\n",
       "      <th>966</th>\n",
       "      <th>967</th>\n",
       "      <th>968</th>\n",
       "      <th>969</th>\n",
       "      <th>970</th>\n",
       "      <th>971</th>\n",
       "      <th>972</th>\n",
       "      <th>973</th>\n",
       "      <th>974</th>\n",
       "      <th>975</th>\n",
       "      <th>976</th>\n",
       "      <th>977</th>\n",
       "      <th>978</th>\n",
       "      <th>979</th>\n",
       "      <th>980</th>\n",
       "      <th>981</th>\n",
       "      <th>982</th>\n",
       "      <th>983</th>\n",
       "      <th>984</th>\n",
       "      <th>985</th>\n",
       "      <th>986</th>\n",
       "      <th>987</th>\n",
       "      <th>988</th>\n",
       "      <th>989</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004554</td>\n",
       "      <td>-0.030177</td>\n",
       "      <td>-0.072558</td>\n",
       "      <td>-0.182296</td>\n",
       "      <td>-0.198521</td>\n",
       "      <td>-0.201616</td>\n",
       "      <td>-0.203007</td>\n",
       "      <td>-0.204214</td>\n",
       "      <td>-0.205417</td>\n",
       "      <td>-0.206631</td>\n",
       "      <td>-0.207850</td>\n",
       "      <td>-0.209070</td>\n",
       "      <td>-0.210285</td>\n",
       "      <td>-0.211491</td>\n",
       "      <td>-0.212684</td>\n",
       "      <td>-0.213861</td>\n",
       "      <td>-0.215019</td>\n",
       "      <td>-0.216158</td>\n",
       "      <td>-0.217275</td>\n",
       "      <td>-0.218369</td>\n",
       "      <td>-0.219441</td>\n",
       "      <td>-0.220489</td>\n",
       "      <td>-0.221512</td>\n",
       "      <td>-0.222512</td>\n",
       "      <td>-0.223487</td>\n",
       "      <td>-0.224439</td>\n",
       "      <td>-0.225366</td>\n",
       "      <td>-0.226270</td>\n",
       "      <td>-0.227150</td>\n",
       "      <td>-0.228007</td>\n",
       "      <td>-0.228841</td>\n",
       "      <td>-0.229652</td>\n",
       "      <td>-0.230442</td>\n",
       "      <td>-0.231210</td>\n",
       "      <td>-0.231957</td>\n",
       "      <td>-0.232683</td>\n",
       "      <td>-0.233388</td>\n",
       "      <td>-0.234074</td>\n",
       "      <td>-0.234740</td>\n",
       "      <td>-0.235388</td>\n",
       "      <td>-0.236016</td>\n",
       "      <td>-0.236627</td>\n",
       "      <td>-0.237220</td>\n",
       "      <td>-0.237796</td>\n",
       "      <td>-0.238355</td>\n",
       "      <td>-0.238898</td>\n",
       "      <td>-0.239425</td>\n",
       "      <td>-0.239936</td>\n",
       "      <td>-0.240432</td>\n",
       "      <td>-0.240914</td>\n",
       "      <td>-0.241381</td>\n",
       "      <td>-0.241834</td>\n",
       "      <td>-0.242274</td>\n",
       "      <td>-0.242701</td>\n",
       "      <td>-0.243114</td>\n",
       "      <td>-0.243516</td>\n",
       "      <td>-0.243905</td>\n",
       "      <td>-0.244282</td>\n",
       "      <td>-0.244648</td>\n",
       "      <td>-0.245003</td>\n",
       "      <td>-0.245347</td>\n",
       "      <td>-0.245681</td>\n",
       "      <td>-0.246005</td>\n",
       "      <td>-0.246318</td>\n",
       "      <td>-0.246622</td>\n",
       "      <td>-0.246917</td>\n",
       "      <td>-0.247202</td>\n",
       "      <td>-0.247479</td>\n",
       "      <td>-0.247747</td>\n",
       "      <td>-0.248007</td>\n",
       "      <td>-0.248258</td>\n",
       "      <td>-0.248502</td>\n",
       "      <td>-0.248739</td>\n",
       "      <td>-0.248968</td>\n",
       "      <td>-0.249190</td>\n",
       "      <td>-0.249404</td>\n",
       "      <td>-0.249613</td>\n",
       "      <td>-0.249814</td>\n",
       "      <td>-0.250009</td>\n",
       "      <td>-0.250199</td>\n",
       "      <td>-0.250382</td>\n",
       "      <td>-0.250559</td>\n",
       "      <td>-0.250731</td>\n",
       "      <td>-0.250897</td>\n",
       "      <td>-0.251058</td>\n",
       "      <td>-0.251213</td>\n",
       "      <td>-0.251364</td>\n",
       "      <td>-0.251510</td>\n",
       "      <td>-0.251651</td>\n",
       "      <td>-0.251788</td>\n",
       "      <td>-0.251920</td>\n",
       "      <td>-0.252048</td>\n",
       "      <td>-0.252172</td>\n",
       "      <td>-0.252292</td>\n",
       "      <td>-0.252408</td>\n",
       "      <td>-0.252520</td>\n",
       "      <td>-0.252628</td>\n",
       "      <td>-0.252733</td>\n",
       "      <td>-0.252834</td>\n",
       "      <td>-0.252932</td>\n",
       "      <td>-0.253026</td>\n",
       "      <td>-0.253118</td>\n",
       "      <td>-0.253206</td>\n",
       "      <td>-0.253292</td>\n",
       "      <td>-0.253374</td>\n",
       "      <td>-0.253454</td>\n",
       "      <td>-0.253531</td>\n",
       "      <td>-0.253605</td>\n",
       "      <td>-0.253677</td>\n",
       "      <td>-0.253746</td>\n",
       "      <td>-0.253812</td>\n",
       "      <td>-0.253877</td>\n",
       "      <td>-0.253939</td>\n",
       "      <td>-0.253999</td>\n",
       "      <td>-0.254057</td>\n",
       "      <td>-0.254112</td>\n",
       "      <td>-0.254166</td>\n",
       "      <td>-0.254218</td>\n",
       "      <td>-0.254267</td>\n",
       "      <td>-0.254315</td>\n",
       "      <td>-0.254361</td>\n",
       "      <td>-0.254405</td>\n",
       "      <td>-0.254448</td>\n",
       "      <td>-0.254489</td>\n",
       "      <td>-0.254529</td>\n",
       "      <td>-0.254566</td>\n",
       "      <td>-0.254603</td>\n",
       "      <td>-0.254637</td>\n",
       "      <td>-0.254671</td>\n",
       "      <td>-0.254703</td>\n",
       "      <td>-0.254734</td>\n",
       "      <td>-0.254763</td>\n",
       "      <td>-0.254791</td>\n",
       "      <td>-0.254818</td>\n",
       "      <td>-0.254844</td>\n",
       "      <td>-0.254868</td>\n",
       "      <td>-0.254892</td>\n",
       "      <td>-0.254914</td>\n",
       "      <td>-0.254935</td>\n",
       "      <td>-0.254955</td>\n",
       "      <td>-0.254975</td>\n",
       "      <td>-0.254993</td>\n",
       "      <td>-0.255010</td>\n",
       "      <td>-0.255027</td>\n",
       "      <td>-0.255042</td>\n",
       "      <td>-0.255057</td>\n",
       "      <td>-0.255071</td>\n",
       "      <td>-0.255083</td>\n",
       "      <td>-0.255096</td>\n",
       "      <td>-0.255107</td>\n",
       "      <td>-0.255118</td>\n",
       "      <td>-0.255128</td>\n",
       "      <td>-0.255137</td>\n",
       "      <td>-0.255145</td>\n",
       "      <td>-0.255153</td>\n",
       "      <td>-0.255160</td>\n",
       "      <td>-0.255167</td>\n",
       "      <td>-0.255173</td>\n",
       "      <td>-0.255178</td>\n",
       "      <td>-0.255183</td>\n",
       "      <td>-0.255187</td>\n",
       "      <td>-0.255191</td>\n",
       "      <td>-0.255194</td>\n",
       "      <td>-0.255197</td>\n",
       "      <td>-0.255199</td>\n",
       "      <td>-0.255201</td>\n",
       "      <td>-0.255202</td>\n",
       "      <td>-0.255202</td>\n",
       "      <td>-0.255203</td>\n",
       "      <td>-0.255203</td>\n",
       "      <td>-0.255202</td>\n",
       "      <td>-0.255201</td>\n",
       "      <td>-0.255200</td>\n",
       "      <td>-0.255198</td>\n",
       "      <td>-0.255196</td>\n",
       "      <td>-0.255193</td>\n",
       "      <td>-0.255190</td>\n",
       "      <td>-0.255187</td>\n",
       "      <td>-0.255184</td>\n",
       "      <td>-0.255180</td>\n",
       "      <td>-0.255175</td>\n",
       "      <td>-0.255171</td>\n",
       "      <td>-0.255166</td>\n",
       "      <td>-0.255161</td>\n",
       "      <td>-0.255156</td>\n",
       "      <td>-0.25515</td>\n",
       "      <td>-0.255144</td>\n",
       "      <td>-0.255138</td>\n",
       "      <td>-0.255132</td>\n",
       "      <td>-0.255125</td>\n",
       "      <td>-0.255118</td>\n",
       "      <td>-0.255111</td>\n",
       "      <td>-0.255104</td>\n",
       "      <td>-0.255096</td>\n",
       "      <td>-0.255088</td>\n",
       "      <td>-0.255080</td>\n",
       "      <td>-0.255072</td>\n",
       "      <td>-0.255064</td>\n",
       "      <td>-0.255055</td>\n",
       "      <td>-0.255047</td>\n",
       "      <td>-0.255038</td>\n",
       "      <td>-0.255029</td>\n",
       "      <td>-0.255019</td>\n",
       "      <td>-0.255010</td>\n",
       "      <td>-0.255000</td>\n",
       "      <td>-0.254991</td>\n",
       "      <td>-0.254981</td>\n",
       "      <td>-0.254971</td>\n",
       "      <td>-0.254961</td>\n",
       "      <td>-0.254950</td>\n",
       "      <td>-0.25494</td>\n",
       "      <td>-0.254930</td>\n",
       "      <td>-0.254919</td>\n",
       "      <td>-0.254908</td>\n",
       "      <td>-0.254897</td>\n",
       "      <td>-0.254886</td>\n",
       "      <td>-0.254875</td>\n",
       "      <td>-0.254864</td>\n",
       "      <td>-0.254853</td>\n",
       "      <td>-0.254841</td>\n",
       "      <td>-0.254830</td>\n",
       "      <td>-0.254818</td>\n",
       "      <td>-0.254807</td>\n",
       "      <td>-0.254795</td>\n",
       "      <td>-0.254783</td>\n",
       "      <td>-0.254771</td>\n",
       "      <td>-0.254759</td>\n",
       "      <td>-0.254747</td>\n",
       "      <td>-0.254735</td>\n",
       "      <td>-0.254723</td>\n",
       "      <td>-0.254710</td>\n",
       "      <td>-0.254698</td>\n",
       "      <td>-0.254685</td>\n",
       "      <td>-0.254673</td>\n",
       "      <td>-0.254660</td>\n",
       "      <td>-0.254648</td>\n",
       "      <td>-0.254635</td>\n",
       "      <td>-0.254623</td>\n",
       "      <td>-0.254610</td>\n",
       "      <td>-0.254597</td>\n",
       "      <td>-0.254584</td>\n",
       "      <td>-0.254571</td>\n",
       "      <td>-0.254558</td>\n",
       "      <td>-0.254545</td>\n",
       "      <td>-0.254532</td>\n",
       "      <td>-0.254519</td>\n",
       "      <td>-0.254506</td>\n",
       "      <td>-0.254493</td>\n",
       "      <td>-0.254480</td>\n",
       "      <td>-0.254467</td>\n",
       "      <td>-0.254453</td>\n",
       "      <td>-0.254440</td>\n",
       "      <td>-0.254427</td>\n",
       "      <td>-0.254413</td>\n",
       "      <td>-0.254400</td>\n",
       "      <td>-0.254387</td>\n",
       "      <td>-0.254373</td>\n",
       "      <td>-0.254360</td>\n",
       "      <td>-0.254346</td>\n",
       "      <td>-0.254333</td>\n",
       "      <td>-0.254319</td>\n",
       "      <td>-0.254306</td>\n",
       "      <td>-0.254292</td>\n",
       "      <td>-0.254279</td>\n",
       "      <td>-0.254265</td>\n",
       "      <td>-0.254252</td>\n",
       "      <td>-0.254238</td>\n",
       "      <td>-0.254225</td>\n",
       "      <td>-0.254211</td>\n",
       "      <td>-0.254197</td>\n",
       "      <td>-0.254184</td>\n",
       "      <td>-0.254170</td>\n",
       "      <td>-0.254156</td>\n",
       "      <td>-0.254143</td>\n",
       "      <td>-0.254129</td>\n",
       "      <td>-0.254115</td>\n",
       "      <td>-0.254102</td>\n",
       "      <td>-0.254088</td>\n",
       "      <td>-0.254074</td>\n",
       "      <td>-0.254061</td>\n",
       "      <td>-0.254047</td>\n",
       "      <td>-0.254033</td>\n",
       "      <td>-0.254019</td>\n",
       "      <td>-0.254006</td>\n",
       "      <td>-0.253992</td>\n",
       "      <td>-0.253978</td>\n",
       "      <td>-0.253964</td>\n",
       "      <td>-0.253951</td>\n",
       "      <td>-0.253937</td>\n",
       "      <td>-0.253923</td>\n",
       "      <td>-0.253910</td>\n",
       "      <td>-0.253896</td>\n",
       "      <td>-0.253882</td>\n",
       "      <td>-0.253868</td>\n",
       "      <td>-0.253855</td>\n",
       "      <td>-0.253841</td>\n",
       "      <td>-0.253827</td>\n",
       "      <td>-0.253814</td>\n",
       "      <td>-0.253800</td>\n",
       "      <td>-0.253786</td>\n",
       "      <td>-0.253773</td>\n",
       "      <td>-0.253759</td>\n",
       "      <td>-0.253745</td>\n",
       "      <td>-0.253732</td>\n",
       "      <td>-0.253718</td>\n",
       "      <td>-0.253705</td>\n",
       "      <td>-0.253691</td>\n",
       "      <td>-0.253677</td>\n",
       "      <td>-0.253664</td>\n",
       "      <td>-0.253650</td>\n",
       "      <td>-0.253637</td>\n",
       "      <td>-0.253623</td>\n",
       "      <td>-0.253609</td>\n",
       "      <td>-0.253596</td>\n",
       "      <td>-0.253582</td>\n",
       "      <td>-0.253569</td>\n",
       "      <td>-0.253555</td>\n",
       "      <td>-0.253542</td>\n",
       "      <td>-0.253528</td>\n",
       "      <td>-0.253515</td>\n",
       "      <td>-0.253501</td>\n",
       "      <td>-0.253488</td>\n",
       "      <td>-0.253474</td>\n",
       "      <td>-0.253461</td>\n",
       "      <td>-0.253448</td>\n",
       "      <td>-0.253434</td>\n",
       "      <td>-0.253421</td>\n",
       "      <td>-0.253407</td>\n",
       "      <td>-0.253394</td>\n",
       "      <td>-0.253381</td>\n",
       "      <td>-0.253367</td>\n",
       "      <td>-0.253354</td>\n",
       "      <td>-0.253341</td>\n",
       "      <td>-0.253327</td>\n",
       "      <td>-0.253314</td>\n",
       "      <td>-0.253301</td>\n",
       "      <td>-0.253288</td>\n",
       "      <td>-0.253275</td>\n",
       "      <td>-0.253261</td>\n",
       "      <td>-0.253248</td>\n",
       "      <td>-0.253235</td>\n",
       "      <td>-0.253222</td>\n",
       "      <td>-0.253209</td>\n",
       "      <td>-0.253195</td>\n",
       "      <td>-0.253182</td>\n",
       "      <td>-0.253169</td>\n",
       "      <td>-0.253156</td>\n",
       "      <td>-0.253143</td>\n",
       "      <td>-0.25313</td>\n",
       "      <td>-0.253117</td>\n",
       "      <td>-0.253104</td>\n",
       "      <td>-0.253091</td>\n",
       "      <td>-0.253078</td>\n",
       "      <td>-0.253065</td>\n",
       "      <td>-0.253052</td>\n",
       "      <td>-0.253039</td>\n",
       "      <td>-0.253026</td>\n",
       "      <td>-0.253013</td>\n",
       "      <td>-0.253000</td>\n",
       "      <td>-0.252988</td>\n",
       "      <td>-0.252975</td>\n",
       "      <td>-0.252962</td>\n",
       "      <td>-0.252949</td>\n",
       "      <td>-0.252936</td>\n",
       "      <td>-0.252924</td>\n",
       "      <td>-0.252911</td>\n",
       "      <td>-0.252898</td>\n",
       "      <td>-0.252885</td>\n",
       "      <td>-0.252873</td>\n",
       "      <td>-0.252860</td>\n",
       "      <td>-0.252847</td>\n",
       "      <td>-0.252835</td>\n",
       "      <td>-0.252822</td>\n",
       "      <td>-0.252809</td>\n",
       "      <td>-0.252797</td>\n",
       "      <td>-0.252784</td>\n",
       "      <td>-0.252772</td>\n",
       "      <td>-0.252759</td>\n",
       "      <td>-0.252747</td>\n",
       "      <td>-0.252734</td>\n",
       "      <td>-0.252722</td>\n",
       "      <td>-0.252709</td>\n",
       "      <td>-0.252697</td>\n",
       "      <td>-0.252685</td>\n",
       "      <td>-0.252672</td>\n",
       "      <td>-0.252660</td>\n",
       "      <td>-0.252647</td>\n",
       "      <td>-0.252635</td>\n",
       "      <td>-0.252623</td>\n",
       "      <td>-0.252611</td>\n",
       "      <td>-0.252598</td>\n",
       "      <td>-0.252586</td>\n",
       "      <td>-0.252574</td>\n",
       "      <td>-0.252562</td>\n",
       "      <td>-0.252549</td>\n",
       "      <td>-0.252537</td>\n",
       "      <td>-0.252525</td>\n",
       "      <td>-0.252513</td>\n",
       "      <td>-0.252501</td>\n",
       "      <td>-0.252489</td>\n",
       "      <td>-0.252477</td>\n",
       "      <td>-0.252464</td>\n",
       "      <td>-0.252452</td>\n",
       "      <td>-0.252440</td>\n",
       "      <td>-0.252428</td>\n",
       "      <td>-0.252416</td>\n",
       "      <td>-0.252404</td>\n",
       "      <td>-0.252393</td>\n",
       "      <td>-0.252381</td>\n",
       "      <td>-0.252369</td>\n",
       "      <td>-0.252357</td>\n",
       "      <td>-0.252345</td>\n",
       "      <td>-0.252333</td>\n",
       "      <td>-0.252321</td>\n",
       "      <td>-0.25231</td>\n",
       "      <td>-0.252298</td>\n",
       "      <td>-0.252286</td>\n",
       "      <td>-0.252274</td>\n",
       "      <td>-0.252263</td>\n",
       "      <td>-0.252251</td>\n",
       "      <td>-0.252239</td>\n",
       "      <td>-0.252228</td>\n",
       "      <td>-0.252216</td>\n",
       "      <td>-0.252204</td>\n",
       "      <td>-0.252193</td>\n",
       "      <td>-0.252181</td>\n",
       "      <td>-0.252170</td>\n",
       "      <td>-0.252158</td>\n",
       "      <td>-0.252147</td>\n",
       "      <td>-0.252135</td>\n",
       "      <td>-0.252124</td>\n",
       "      <td>-0.252112</td>\n",
       "      <td>-0.252101</td>\n",
       "      <td>-0.252089</td>\n",
       "      <td>-0.252078</td>\n",
       "      <td>-0.252067</td>\n",
       "      <td>-0.252055</td>\n",
       "      <td>-0.252044</td>\n",
       "      <td>-0.252033</td>\n",
       "      <td>-0.252021</td>\n",
       "      <td>-0.252010</td>\n",
       "      <td>-0.251999</td>\n",
       "      <td>-0.251988</td>\n",
       "      <td>-0.251976</td>\n",
       "      <td>-0.251965</td>\n",
       "      <td>-0.251954</td>\n",
       "      <td>-0.251943</td>\n",
       "      <td>-0.251932</td>\n",
       "      <td>-0.251921</td>\n",
       "      <td>-0.251909</td>\n",
       "      <td>-0.251898</td>\n",
       "      <td>-0.251887</td>\n",
       "      <td>-0.251876</td>\n",
       "      <td>-0.251865</td>\n",
       "      <td>-0.251854</td>\n",
       "      <td>-0.251843</td>\n",
       "      <td>-0.251832</td>\n",
       "      <td>-0.251821</td>\n",
       "      <td>-0.251811</td>\n",
       "      <td>-0.251800</td>\n",
       "      <td>-0.251789</td>\n",
       "      <td>-0.251778</td>\n",
       "      <td>-0.251767</td>\n",
       "      <td>-0.251756</td>\n",
       "      <td>-0.251746</td>\n",
       "      <td>-0.251735</td>\n",
       "      <td>-0.251724</td>\n",
       "      <td>-0.251713</td>\n",
       "      <td>-0.251703</td>\n",
       "      <td>-0.251692</td>\n",
       "      <td>-0.251681</td>\n",
       "      <td>-0.251671</td>\n",
       "      <td>-0.25166</td>\n",
       "      <td>-0.251649</td>\n",
       "      <td>-0.251639</td>\n",
       "      <td>-0.251628</td>\n",
       "      <td>-0.251618</td>\n",
       "      <td>-0.251607</td>\n",
       "      <td>-0.251597</td>\n",
       "      <td>-0.251586</td>\n",
       "      <td>-0.251576</td>\n",
       "      <td>-0.251565</td>\n",
       "      <td>-0.251555</td>\n",
       "      <td>-0.251544</td>\n",
       "      <td>-0.251534</td>\n",
       "      <td>-0.251524</td>\n",
       "      <td>-0.251513</td>\n",
       "      <td>-0.251503</td>\n",
       "      <td>-0.251493</td>\n",
       "      <td>-0.251483</td>\n",
       "      <td>-0.251472</td>\n",
       "      <td>-0.251462</td>\n",
       "      <td>-0.251452</td>\n",
       "      <td>-0.251442</td>\n",
       "      <td>-0.251431</td>\n",
       "      <td>-0.251421</td>\n",
       "      <td>-0.251411</td>\n",
       "      <td>-0.251401</td>\n",
       "      <td>-0.251391</td>\n",
       "      <td>-0.251381</td>\n",
       "      <td>-0.251371</td>\n",
       "      <td>-0.251361</td>\n",
       "      <td>-0.251351</td>\n",
       "      <td>-0.251341</td>\n",
       "      <td>-0.251331</td>\n",
       "      <td>-0.251321</td>\n",
       "      <td>-0.251311</td>\n",
       "      <td>-0.251301</td>\n",
       "      <td>-0.251291</td>\n",
       "      <td>-0.251281</td>\n",
       "      <td>-0.251271</td>\n",
       "      <td>-0.251261</td>\n",
       "      <td>-0.251252</td>\n",
       "      <td>-0.251242</td>\n",
       "      <td>-0.251232</td>\n",
       "      <td>-0.251222</td>\n",
       "      <td>-0.251213</td>\n",
       "      <td>-0.251203</td>\n",
       "      <td>-0.251193</td>\n",
       "      <td>-0.251183</td>\n",
       "      <td>-0.251174</td>\n",
       "      <td>-0.251164</td>\n",
       "      <td>-0.251155</td>\n",
       "      <td>-0.251145</td>\n",
       "      <td>-0.251135</td>\n",
       "      <td>-0.251126</td>\n",
       "      <td>-0.251116</td>\n",
       "      <td>-0.251107</td>\n",
       "      <td>-0.251097</td>\n",
       "      <td>-0.251088</td>\n",
       "      <td>-0.251078</td>\n",
       "      <td>-0.251069</td>\n",
       "      <td>-0.251059</td>\n",
       "      <td>-0.251050</td>\n",
       "      <td>-0.251041</td>\n",
       "      <td>-0.251031</td>\n",
       "      <td>-0.251022</td>\n",
       "      <td>-0.251013</td>\n",
       "      <td>-0.251003</td>\n",
       "      <td>-0.250994</td>\n",
       "      <td>-0.250985</td>\n",
       "      <td>-0.250975</td>\n",
       "      <td>-0.250966</td>\n",
       "      <td>-0.250957</td>\n",
       "      <td>-0.250948</td>\n",
       "      <td>-0.250939</td>\n",
       "      <td>-0.250929</td>\n",
       "      <td>-0.250920</td>\n",
       "      <td>-0.250911</td>\n",
       "      <td>-0.250902</td>\n",
       "      <td>-0.250893</td>\n",
       "      <td>-0.250884</td>\n",
       "      <td>-0.250875</td>\n",
       "      <td>-0.250866</td>\n",
       "      <td>-0.250857</td>\n",
       "      <td>-0.250848</td>\n",
       "      <td>-0.250839</td>\n",
       "      <td>-0.25083</td>\n",
       "      <td>-0.250821</td>\n",
       "      <td>-0.250812</td>\n",
       "      <td>-0.250803</td>\n",
       "      <td>-0.250794</td>\n",
       "      <td>-0.250786</td>\n",
       "      <td>-0.250777</td>\n",
       "      <td>-0.250768</td>\n",
       "      <td>-0.250759</td>\n",
       "      <td>-0.250750</td>\n",
       "      <td>-0.250742</td>\n",
       "      <td>-0.250733</td>\n",
       "      <td>-0.250724</td>\n",
       "      <td>-0.250716</td>\n",
       "      <td>-0.250707</td>\n",
       "      <td>-0.250698</td>\n",
       "      <td>-0.250690</td>\n",
       "      <td>-0.250681</td>\n",
       "      <td>-0.250672</td>\n",
       "      <td>-0.250664</td>\n",
       "      <td>-0.250655</td>\n",
       "      <td>-0.250647</td>\n",
       "      <td>-0.250638</td>\n",
       "      <td>-0.25063</td>\n",
       "      <td>-0.250621</td>\n",
       "      <td>-0.250613</td>\n",
       "      <td>-0.250604</td>\n",
       "      <td>-0.250596</td>\n",
       "      <td>-0.250587</td>\n",
       "      <td>-0.250579</td>\n",
       "      <td>-0.250571</td>\n",
       "      <td>-0.250562</td>\n",
       "      <td>-0.250554</td>\n",
       "      <td>-0.250546</td>\n",
       "      <td>-0.250537</td>\n",
       "      <td>-0.250529</td>\n",
       "      <td>-0.250521</td>\n",
       "      <td>-0.250513</td>\n",
       "      <td>-0.250504</td>\n",
       "      <td>-0.250496</td>\n",
       "      <td>-0.250488</td>\n",
       "      <td>-0.250480</td>\n",
       "      <td>-0.250472</td>\n",
       "      <td>-0.250464</td>\n",
       "      <td>-0.250455</td>\n",
       "      <td>-0.250447</td>\n",
       "      <td>-0.250439</td>\n",
       "      <td>-0.250431</td>\n",
       "      <td>-0.250423</td>\n",
       "      <td>-0.250415</td>\n",
       "      <td>-0.250407</td>\n",
       "      <td>-0.250399</td>\n",
       "      <td>-0.250391</td>\n",
       "      <td>-0.250383</td>\n",
       "      <td>-0.250375</td>\n",
       "      <td>-0.250367</td>\n",
       "      <td>-0.250360</td>\n",
       "      <td>-0.250352</td>\n",
       "      <td>-0.250344</td>\n",
       "      <td>-0.250336</td>\n",
       "      <td>-0.250328</td>\n",
       "      <td>-0.250320</td>\n",
       "      <td>-0.250313</td>\n",
       "      <td>-0.250305</td>\n",
       "      <td>-0.250297</td>\n",
       "      <td>-0.250289</td>\n",
       "      <td>-0.250282</td>\n",
       "      <td>-0.250274</td>\n",
       "      <td>-0.250266</td>\n",
       "      <td>-0.250259</td>\n",
       "      <td>-0.250251</td>\n",
       "      <td>-0.250243</td>\n",
       "      <td>-0.250236</td>\n",
       "      <td>-0.250228</td>\n",
       "      <td>-0.250221</td>\n",
       "      <td>-0.250213</td>\n",
       "      <td>-0.250206</td>\n",
       "      <td>-0.250198</td>\n",
       "      <td>-0.250191</td>\n",
       "      <td>-0.250183</td>\n",
       "      <td>-0.250176</td>\n",
       "      <td>-0.250168</td>\n",
       "      <td>-0.250161</td>\n",
       "      <td>-0.250154</td>\n",
       "      <td>-0.250146</td>\n",
       "      <td>-0.250139</td>\n",
       "      <td>-0.250131</td>\n",
       "      <td>-0.250124</td>\n",
       "      <td>-0.250117</td>\n",
       "      <td>-0.250110</td>\n",
       "      <td>-0.250102</td>\n",
       "      <td>-0.250095</td>\n",
       "      <td>-0.250088</td>\n",
       "      <td>-0.250081</td>\n",
       "      <td>-0.250073</td>\n",
       "      <td>-0.250066</td>\n",
       "      <td>-0.250059</td>\n",
       "      <td>-0.250052</td>\n",
       "      <td>-0.250045</td>\n",
       "      <td>-0.250038</td>\n",
       "      <td>-0.250031</td>\n",
       "      <td>-0.250024</td>\n",
       "      <td>-0.250016</td>\n",
       "      <td>-0.250009</td>\n",
       "      <td>-0.250002</td>\n",
       "      <td>-0.249995</td>\n",
       "      <td>-0.249988</td>\n",
       "      <td>-0.249981</td>\n",
       "      <td>-0.249974</td>\n",
       "      <td>-0.249968</td>\n",
       "      <td>-0.249961</td>\n",
       "      <td>-0.249954</td>\n",
       "      <td>-0.249947</td>\n",
       "      <td>-0.24994</td>\n",
       "      <td>-0.249933</td>\n",
       "      <td>-0.249926</td>\n",
       "      <td>-0.249920</td>\n",
       "      <td>-0.249913</td>\n",
       "      <td>-0.249906</td>\n",
       "      <td>-0.249899</td>\n",
       "      <td>-0.249892</td>\n",
       "      <td>-0.249886</td>\n",
       "      <td>-0.249879</td>\n",
       "      <td>-0.249872</td>\n",
       "      <td>-0.249866</td>\n",
       "      <td>-0.249859</td>\n",
       "      <td>-0.249852</td>\n",
       "      <td>-0.249846</td>\n",
       "      <td>-0.249839</td>\n",
       "      <td>-0.249833</td>\n",
       "      <td>-0.249826</td>\n",
       "      <td>-0.249819</td>\n",
       "      <td>-0.249813</td>\n",
       "      <td>-0.249806</td>\n",
       "      <td>-0.24980</td>\n",
       "      <td>-0.249793</td>\n",
       "      <td>-0.249787</td>\n",
       "      <td>-0.249780</td>\n",
       "      <td>-0.249774</td>\n",
       "      <td>-0.249768</td>\n",
       "      <td>-0.249761</td>\n",
       "      <td>-0.249755</td>\n",
       "      <td>-0.249749</td>\n",
       "      <td>-0.249742</td>\n",
       "      <td>-0.249736</td>\n",
       "      <td>-0.249730</td>\n",
       "      <td>-0.249723</td>\n",
       "      <td>-0.249717</td>\n",
       "      <td>-0.249711</td>\n",
       "      <td>-0.249704</td>\n",
       "      <td>-0.249698</td>\n",
       "      <td>-0.249692</td>\n",
       "      <td>-0.249686</td>\n",
       "      <td>-0.24968</td>\n",
       "      <td>-0.249673</td>\n",
       "      <td>-0.249667</td>\n",
       "      <td>-0.249661</td>\n",
       "      <td>-0.249655</td>\n",
       "      <td>-0.249649</td>\n",
       "      <td>-0.249643</td>\n",
       "      <td>-0.249637</td>\n",
       "      <td>-0.249631</td>\n",
       "      <td>-0.249625</td>\n",
       "      <td>-0.249619</td>\n",
       "      <td>-0.249613</td>\n",
       "      <td>-0.249607</td>\n",
       "      <td>-0.249601</td>\n",
       "      <td>-0.249595</td>\n",
       "      <td>-0.249589</td>\n",
       "      <td>-0.249583</td>\n",
       "      <td>-0.249577</td>\n",
       "      <td>-0.249571</td>\n",
       "      <td>-0.249565</td>\n",
       "      <td>-0.249560</td>\n",
       "      <td>-0.249554</td>\n",
       "      <td>-0.249548</td>\n",
       "      <td>-0.249542</td>\n",
       "      <td>-0.249536</td>\n",
       "      <td>-0.249531</td>\n",
       "      <td>-0.249525</td>\n",
       "      <td>-0.249519</td>\n",
       "      <td>-0.249513</td>\n",
       "      <td>-0.249508</td>\n",
       "      <td>-0.249502</td>\n",
       "      <td>-0.249496</td>\n",
       "      <td>-0.249491</td>\n",
       "      <td>-0.249485</td>\n",
       "      <td>-0.249479</td>\n",
       "      <td>-0.249474</td>\n",
       "      <td>-0.249468</td>\n",
       "      <td>-0.249463</td>\n",
       "      <td>-0.249457</td>\n",
       "      <td>-0.249452</td>\n",
       "      <td>-0.249446</td>\n",
       "      <td>-0.249441</td>\n",
       "      <td>-0.249435</td>\n",
       "      <td>-0.249430</td>\n",
       "      <td>-0.249424</td>\n",
       "      <td>-0.249419</td>\n",
       "      <td>-0.249413</td>\n",
       "      <td>-0.249408</td>\n",
       "      <td>-0.249403</td>\n",
       "      <td>-0.249397</td>\n",
       "      <td>-0.249392</td>\n",
       "      <td>-0.249386</td>\n",
       "      <td>-0.249381</td>\n",
       "      <td>-0.249376</td>\n",
       "      <td>-0.249371</td>\n",
       "      <td>-0.249365</td>\n",
       "      <td>-0.249360</td>\n",
       "      <td>-0.249355</td>\n",
       "      <td>-0.249349</td>\n",
       "      <td>-0.249344</td>\n",
       "      <td>-0.249339</td>\n",
       "      <td>-0.249334</td>\n",
       "      <td>-0.249329</td>\n",
       "      <td>-0.249324</td>\n",
       "      <td>-0.249318</td>\n",
       "      <td>-0.249313</td>\n",
       "      <td>-0.249308</td>\n",
       "      <td>-0.249303</td>\n",
       "      <td>-0.249298</td>\n",
       "      <td>-0.249293</td>\n",
       "      <td>-0.249288</td>\n",
       "      <td>-0.249283</td>\n",
       "      <td>-0.249278</td>\n",
       "      <td>-0.249273</td>\n",
       "      <td>-0.249268</td>\n",
       "      <td>-0.249263</td>\n",
       "      <td>-0.249258</td>\n",
       "      <td>-0.249253</td>\n",
       "      <td>-0.249248</td>\n",
       "      <td>-0.249243</td>\n",
       "      <td>-0.249238</td>\n",
       "      <td>-0.249233</td>\n",
       "      <td>-0.249229</td>\n",
       "      <td>-0.249224</td>\n",
       "      <td>-0.249219</td>\n",
       "      <td>-0.249214</td>\n",
       "      <td>-0.249209</td>\n",
       "      <td>-0.249205</td>\n",
       "      <td>-0.249200</td>\n",
       "      <td>-0.249195</td>\n",
       "      <td>-0.249190</td>\n",
       "      <td>-0.249186</td>\n",
       "      <td>-0.249181</td>\n",
       "      <td>-0.249176</td>\n",
       "      <td>-0.249171</td>\n",
       "      <td>-0.249167</td>\n",
       "      <td>-0.249162</td>\n",
       "      <td>-0.249158</td>\n",
       "      <td>-0.249153</td>\n",
       "      <td>-0.249148</td>\n",
       "      <td>-0.249144</td>\n",
       "      <td>-0.249139</td>\n",
       "      <td>-0.249135</td>\n",
       "      <td>-0.249130</td>\n",
       "      <td>-0.249126</td>\n",
       "      <td>-0.249121</td>\n",
       "      <td>-0.249116</td>\n",
       "      <td>-0.249112</td>\n",
       "      <td>-0.249108</td>\n",
       "      <td>-0.249103</td>\n",
       "      <td>-0.249099</td>\n",
       "      <td>-0.249094</td>\n",
       "      <td>-0.249090</td>\n",
       "      <td>-0.249085</td>\n",
       "      <td>-0.249081</td>\n",
       "      <td>-0.249077</td>\n",
       "      <td>-0.249072</td>\n",
       "      <td>-0.249068</td>\n",
       "      <td>-0.249064</td>\n",
       "      <td>-0.249059</td>\n",
       "      <td>-0.249055</td>\n",
       "      <td>-0.249051</td>\n",
       "      <td>-0.249047</td>\n",
       "      <td>-0.249042</td>\n",
       "      <td>-0.249038</td>\n",
       "      <td>-0.249034</td>\n",
       "      <td>-0.249030</td>\n",
       "      <td>-0.249025</td>\n",
       "      <td>-0.249021</td>\n",
       "      <td>-0.249017</td>\n",
       "      <td>-0.249013</td>\n",
       "      <td>-0.249009</td>\n",
       "      <td>-0.249005</td>\n",
       "      <td>-0.249001</td>\n",
       "      <td>-0.248997</td>\n",
       "      <td>-0.248993</td>\n",
       "      <td>-0.248988</td>\n",
       "      <td>-0.248984</td>\n",
       "      <td>-0.248980</td>\n",
       "      <td>-0.248976</td>\n",
       "      <td>-0.248972</td>\n",
       "      <td>-0.248968</td>\n",
       "      <td>-0.248964</td>\n",
       "      <td>-0.24896</td>\n",
       "      <td>-0.248957</td>\n",
       "      <td>-0.248953</td>\n",
       "      <td>-0.248949</td>\n",
       "      <td>-0.248945</td>\n",
       "      <td>-0.248941</td>\n",
       "      <td>-0.248937</td>\n",
       "      <td>-0.248933</td>\n",
       "      <td>-0.248929</td>\n",
       "      <td>-0.248926</td>\n",
       "      <td>-0.248922</td>\n",
       "      <td>-0.248918</td>\n",
       "      <td>-0.248914</td>\n",
       "      <td>-0.24891</td>\n",
       "      <td>-0.248907</td>\n",
       "      <td>-0.248903</td>\n",
       "      <td>-0.248899</td>\n",
       "      <td>-0.248895</td>\n",
       "      <td>-0.248892</td>\n",
       "      <td>-0.248888</td>\n",
       "      <td>-0.248884</td>\n",
       "      <td>-0.248881</td>\n",
       "      <td>-0.248877</td>\n",
       "      <td>-0.248873</td>\n",
       "      <td>-0.248870</td>\n",
       "      <td>-0.248866</td>\n",
       "      <td>-0.248863</td>\n",
       "      <td>-0.248859</td>\n",
       "      <td>-0.248855</td>\n",
       "      <td>-0.248852</td>\n",
       "      <td>-0.248848</td>\n",
       "      <td>-0.248845</td>\n",
       "      <td>-0.248841</td>\n",
       "      <td>-0.248838</td>\n",
       "      <td>-0.248834</td>\n",
       "      <td>-0.248831</td>\n",
       "      <td>-0.248827</td>\n",
       "      <td>-0.248824</td>\n",
       "      <td>-0.248820</td>\n",
       "      <td>-0.248817</td>\n",
       "      <td>-0.248814</td>\n",
       "      <td>-0.248810</td>\n",
       "      <td>-0.248807</td>\n",
       "      <td>-0.248804</td>\n",
       "      <td>-0.248800</td>\n",
       "      <td>-0.248797</td>\n",
       "      <td>-0.248793</td>\n",
       "      <td>-0.248790</td>\n",
       "      <td>-0.248787</td>\n",
       "      <td>-0.248784</td>\n",
       "      <td>-0.24878</td>\n",
       "      <td>-0.248777</td>\n",
       "      <td>-0.248774</td>\n",
       "      <td>-0.248771</td>\n",
       "      <td>-0.248767</td>\n",
       "      <td>-0.248764</td>\n",
       "      <td>-0.248761</td>\n",
       "      <td>-0.248758</td>\n",
       "      <td>-0.248755</td>\n",
       "      <td>-0.248751</td>\n",
       "      <td>-0.248748</td>\n",
       "      <td>-0.248745</td>\n",
       "      <td>-0.248742</td>\n",
       "      <td>-0.248739</td>\n",
       "      <td>-0.248736</td>\n",
       "      <td>-0.248733</td>\n",
       "      <td>-0.248730</td>\n",
       "      <td>-0.248727</td>\n",
       "      <td>-0.248724</td>\n",
       "      <td>-0.248721</td>\n",
       "      <td>-0.248718</td>\n",
       "      <td>-0.248715</td>\n",
       "      <td>-0.248712</td>\n",
       "      <td>-0.248709</td>\n",
       "      <td>-0.248706</td>\n",
       "      <td>-0.248703</td>\n",
       "      <td>-0.248700</td>\n",
       "      <td>-0.248697</td>\n",
       "      <td>-0.248694</td>\n",
       "      <td>-0.248691</td>\n",
       "      <td>-0.248688</td>\n",
       "      <td>-0.248686</td>\n",
       "      <td>-0.248683</td>\n",
       "      <td>-0.248680</td>\n",
       "      <td>-0.248677</td>\n",
       "      <td>-0.248674</td>\n",
       "      <td>-0.248671</td>\n",
       "      <td>-0.248669</td>\n",
       "      <td>-0.248666</td>\n",
       "      <td>-0.248663</td>\n",
       "      <td>-0.248660</td>\n",
       "      <td>-0.248658</td>\n",
       "      <td>-0.248655</td>\n",
       "      <td>-0.248652</td>\n",
       "      <td>-0.248650</td>\n",
       "      <td>-0.248647</td>\n",
       "      <td>-0.248644</td>\n",
       "      <td>-0.248642</td>\n",
       "      <td>-0.248639</td>\n",
       "      <td>-0.248636</td>\n",
       "      <td>-0.248634</td>\n",
       "      <td>-0.248631</td>\n",
       "      <td>-0.248628</td>\n",
       "      <td>-0.248626</td>\n",
       "      <td>-0.248623</td>\n",
       "      <td>-0.248621</td>\n",
       "      <td>-0.248618</td>\n",
       "      <td>-0.248616</td>\n",
       "      <td>-0.248613</td>\n",
       "      <td>-0.248611</td>\n",
       "      <td>-0.248608</td>\n",
       "      <td>-0.248606</td>\n",
       "      <td>-0.248603</td>\n",
       "      <td>-0.248601</td>\n",
       "      <td>-0.248598</td>\n",
       "      <td>-0.248596</td>\n",
       "      <td>-0.248593</td>\n",
       "      <td>-0.248591</td>\n",
       "      <td>-0.248589</td>\n",
       "      <td>-0.248586</td>\n",
       "      <td>-0.248584</td>\n",
       "      <td>-0.248581</td>\n",
       "      <td>-0.248579</td>\n",
       "      <td>-0.248577</td>\n",
       "      <td>-0.248574</td>\n",
       "      <td>-0.248572</td>\n",
       "      <td>-0.248570</td>\n",
       "      <td>-0.248567</td>\n",
       "      <td>-0.248565</td>\n",
       "      <td>-0.248563</td>\n",
       "      <td>-0.248561</td>\n",
       "      <td>-0.248558</td>\n",
       "      <td>-0.248556</td>\n",
       "      <td>-0.248554</td>\n",
       "      <td>-0.248552</td>\n",
       "      <td>-0.248550</td>\n",
       "      <td>-0.248547</td>\n",
       "      <td>-0.248545</td>\n",
       "      <td>-0.248543</td>\n",
       "      <td>-0.265621</td>\n",
       "      <td>-0.278570</td>\n",
       "      <td>-0.301747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.055126</td>\n",
       "      <td>-0.037456</td>\n",
       "      <td>-0.030337</td>\n",
       "      <td>-0.103080</td>\n",
       "      <td>-0.103322</td>\n",
       "      <td>-0.102151</td>\n",
       "      <td>-0.101233</td>\n",
       "      <td>-0.100570</td>\n",
       "      <td>-0.100030</td>\n",
       "      <td>-0.099548</td>\n",
       "      <td>-0.099096</td>\n",
       "      <td>-0.098664</td>\n",
       "      <td>-0.098248</td>\n",
       "      <td>-0.097849</td>\n",
       "      <td>-0.097468</td>\n",
       "      <td>-0.097108</td>\n",
       "      <td>-0.096769</td>\n",
       "      <td>-0.096454</td>\n",
       "      <td>-0.096163</td>\n",
       "      <td>-0.095897</td>\n",
       "      <td>-0.095655</td>\n",
       "      <td>-0.095439</td>\n",
       "      <td>-0.095246</td>\n",
       "      <td>-0.095078</td>\n",
       "      <td>-0.094932</td>\n",
       "      <td>-0.094808</td>\n",
       "      <td>-0.094706</td>\n",
       "      <td>-0.094623</td>\n",
       "      <td>-0.094559</td>\n",
       "      <td>-0.094513</td>\n",
       "      <td>-0.094483</td>\n",
       "      <td>-0.094468</td>\n",
       "      <td>-0.094468</td>\n",
       "      <td>-0.094481</td>\n",
       "      <td>-0.094505</td>\n",
       "      <td>-0.094541</td>\n",
       "      <td>-0.094586</td>\n",
       "      <td>-0.094640</td>\n",
       "      <td>-0.094703</td>\n",
       "      <td>-0.094772</td>\n",
       "      <td>-0.094848</td>\n",
       "      <td>-0.094929</td>\n",
       "      <td>-0.095016</td>\n",
       "      <td>-0.095106</td>\n",
       "      <td>-0.095200</td>\n",
       "      <td>-0.095297</td>\n",
       "      <td>-0.095396</td>\n",
       "      <td>-0.095497</td>\n",
       "      <td>-0.095600</td>\n",
       "      <td>-0.095704</td>\n",
       "      <td>-0.095808</td>\n",
       "      <td>-0.095913</td>\n",
       "      <td>-0.096017</td>\n",
       "      <td>-0.096122</td>\n",
       "      <td>-0.096225</td>\n",
       "      <td>-0.096328</td>\n",
       "      <td>-0.096430</td>\n",
       "      <td>-0.096530</td>\n",
       "      <td>-0.096630</td>\n",
       "      <td>-0.096727</td>\n",
       "      <td>-0.096823</td>\n",
       "      <td>-0.096918</td>\n",
       "      <td>-0.097010</td>\n",
       "      <td>-0.097100</td>\n",
       "      <td>-0.097189</td>\n",
       "      <td>-0.097275</td>\n",
       "      <td>-0.097359</td>\n",
       "      <td>-0.097441</td>\n",
       "      <td>-0.097521</td>\n",
       "      <td>-0.097598</td>\n",
       "      <td>-0.097674</td>\n",
       "      <td>-0.097747</td>\n",
       "      <td>-0.097818</td>\n",
       "      <td>-0.097886</td>\n",
       "      <td>-0.097953</td>\n",
       "      <td>-0.098017</td>\n",
       "      <td>-0.098080</td>\n",
       "      <td>-0.098140</td>\n",
       "      <td>-0.098198</td>\n",
       "      <td>-0.098254</td>\n",
       "      <td>-0.098308</td>\n",
       "      <td>-0.098360</td>\n",
       "      <td>-0.098410</td>\n",
       "      <td>-0.098458</td>\n",
       "      <td>-0.098504</td>\n",
       "      <td>-0.098549</td>\n",
       "      <td>-0.098591</td>\n",
       "      <td>-0.098632</td>\n",
       "      <td>-0.098672</td>\n",
       "      <td>-0.098709</td>\n",
       "      <td>-0.098745</td>\n",
       "      <td>-0.098780</td>\n",
       "      <td>-0.098813</td>\n",
       "      <td>-0.098845</td>\n",
       "      <td>-0.098875</td>\n",
       "      <td>-0.098903</td>\n",
       "      <td>-0.098931</td>\n",
       "      <td>-0.098957</td>\n",
       "      <td>-0.098982</td>\n",
       "      <td>-0.099006</td>\n",
       "      <td>-0.099028</td>\n",
       "      <td>-0.099050</td>\n",
       "      <td>-0.099070</td>\n",
       "      <td>-0.099089</td>\n",
       "      <td>-0.099107</td>\n",
       "      <td>-0.099125</td>\n",
       "      <td>-0.099141</td>\n",
       "      <td>-0.099156</td>\n",
       "      <td>-0.099171</td>\n",
       "      <td>-0.099185</td>\n",
       "      <td>-0.099197</td>\n",
       "      <td>-0.099209</td>\n",
       "      <td>-0.099221</td>\n",
       "      <td>-0.099231</td>\n",
       "      <td>-0.099241</td>\n",
       "      <td>-0.099250</td>\n",
       "      <td>-0.099259</td>\n",
       "      <td>-0.099267</td>\n",
       "      <td>-0.099274</td>\n",
       "      <td>-0.099281</td>\n",
       "      <td>-0.099287</td>\n",
       "      <td>-0.099293</td>\n",
       "      <td>-0.099298</td>\n",
       "      <td>-0.099303</td>\n",
       "      <td>-0.099307</td>\n",
       "      <td>-0.099311</td>\n",
       "      <td>-0.099314</td>\n",
       "      <td>-0.099317</td>\n",
       "      <td>-0.099319</td>\n",
       "      <td>-0.099322</td>\n",
       "      <td>-0.099323</td>\n",
       "      <td>-0.099325</td>\n",
       "      <td>-0.099326</td>\n",
       "      <td>-0.099327</td>\n",
       "      <td>-0.099327</td>\n",
       "      <td>-0.099327</td>\n",
       "      <td>-0.099327</td>\n",
       "      <td>-0.099327</td>\n",
       "      <td>-0.099326</td>\n",
       "      <td>-0.099326</td>\n",
       "      <td>-0.099324</td>\n",
       "      <td>-0.099323</td>\n",
       "      <td>-0.099322</td>\n",
       "      <td>-0.099320</td>\n",
       "      <td>-0.099318</td>\n",
       "      <td>-0.099316</td>\n",
       "      <td>-0.099314</td>\n",
       "      <td>-0.099311</td>\n",
       "      <td>-0.099309</td>\n",
       "      <td>-0.099306</td>\n",
       "      <td>-0.099303</td>\n",
       "      <td>-0.099300</td>\n",
       "      <td>-0.099297</td>\n",
       "      <td>-0.099294</td>\n",
       "      <td>-0.099290</td>\n",
       "      <td>-0.099287</td>\n",
       "      <td>-0.099283</td>\n",
       "      <td>-0.099279</td>\n",
       "      <td>-0.099276</td>\n",
       "      <td>-0.099272</td>\n",
       "      <td>-0.099268</td>\n",
       "      <td>-0.099264</td>\n",
       "      <td>-0.099259</td>\n",
       "      <td>-0.099255</td>\n",
       "      <td>-0.099251</td>\n",
       "      <td>-0.099247</td>\n",
       "      <td>-0.099242</td>\n",
       "      <td>-0.099238</td>\n",
       "      <td>-0.099233</td>\n",
       "      <td>-0.099229</td>\n",
       "      <td>-0.099224</td>\n",
       "      <td>-0.099219</td>\n",
       "      <td>-0.099215</td>\n",
       "      <td>-0.099210</td>\n",
       "      <td>-0.099205</td>\n",
       "      <td>-0.099200</td>\n",
       "      <td>-0.099195</td>\n",
       "      <td>-0.099190</td>\n",
       "      <td>-0.099185</td>\n",
       "      <td>-0.099181</td>\n",
       "      <td>-0.099176</td>\n",
       "      <td>-0.099171</td>\n",
       "      <td>-0.099165</td>\n",
       "      <td>-0.099160</td>\n",
       "      <td>-0.099155</td>\n",
       "      <td>-0.09915</td>\n",
       "      <td>-0.099145</td>\n",
       "      <td>-0.099140</td>\n",
       "      <td>-0.099135</td>\n",
       "      <td>-0.099130</td>\n",
       "      <td>-0.099125</td>\n",
       "      <td>-0.099120</td>\n",
       "      <td>-0.099114</td>\n",
       "      <td>-0.099109</td>\n",
       "      <td>-0.099104</td>\n",
       "      <td>-0.099099</td>\n",
       "      <td>-0.099093</td>\n",
       "      <td>-0.099088</td>\n",
       "      <td>-0.099083</td>\n",
       "      <td>-0.099078</td>\n",
       "      <td>-0.099073</td>\n",
       "      <td>-0.099067</td>\n",
       "      <td>-0.099062</td>\n",
       "      <td>-0.099057</td>\n",
       "      <td>-0.099052</td>\n",
       "      <td>-0.099046</td>\n",
       "      <td>-0.099041</td>\n",
       "      <td>-0.099036</td>\n",
       "      <td>-0.099031</td>\n",
       "      <td>-0.099025</td>\n",
       "      <td>-0.09902</td>\n",
       "      <td>-0.099015</td>\n",
       "      <td>-0.099010</td>\n",
       "      <td>-0.099005</td>\n",
       "      <td>-0.098999</td>\n",
       "      <td>-0.098994</td>\n",
       "      <td>-0.098989</td>\n",
       "      <td>-0.098984</td>\n",
       "      <td>-0.098979</td>\n",
       "      <td>-0.098973</td>\n",
       "      <td>-0.098968</td>\n",
       "      <td>-0.098963</td>\n",
       "      <td>-0.098958</td>\n",
       "      <td>-0.098952</td>\n",
       "      <td>-0.098947</td>\n",
       "      <td>-0.098942</td>\n",
       "      <td>-0.098937</td>\n",
       "      <td>-0.098932</td>\n",
       "      <td>-0.098927</td>\n",
       "      <td>-0.098921</td>\n",
       "      <td>-0.098916</td>\n",
       "      <td>-0.098911</td>\n",
       "      <td>-0.098906</td>\n",
       "      <td>-0.098901</td>\n",
       "      <td>-0.098896</td>\n",
       "      <td>-0.098890</td>\n",
       "      <td>-0.098885</td>\n",
       "      <td>-0.098880</td>\n",
       "      <td>-0.098875</td>\n",
       "      <td>-0.098870</td>\n",
       "      <td>-0.098865</td>\n",
       "      <td>-0.098860</td>\n",
       "      <td>-0.098855</td>\n",
       "      <td>-0.098849</td>\n",
       "      <td>-0.098844</td>\n",
       "      <td>-0.098839</td>\n",
       "      <td>-0.098834</td>\n",
       "      <td>-0.098829</td>\n",
       "      <td>-0.098824</td>\n",
       "      <td>-0.098819</td>\n",
       "      <td>-0.098814</td>\n",
       "      <td>-0.098809</td>\n",
       "      <td>-0.098804</td>\n",
       "      <td>-0.098799</td>\n",
       "      <td>-0.098794</td>\n",
       "      <td>-0.098789</td>\n",
       "      <td>-0.098784</td>\n",
       "      <td>-0.098778</td>\n",
       "      <td>-0.098773</td>\n",
       "      <td>-0.098768</td>\n",
       "      <td>-0.098763</td>\n",
       "      <td>-0.098758</td>\n",
       "      <td>-0.098753</td>\n",
       "      <td>-0.098748</td>\n",
       "      <td>-0.098743</td>\n",
       "      <td>-0.098738</td>\n",
       "      <td>-0.098733</td>\n",
       "      <td>-0.098728</td>\n",
       "      <td>-0.098723</td>\n",
       "      <td>-0.098718</td>\n",
       "      <td>-0.098713</td>\n",
       "      <td>-0.098708</td>\n",
       "      <td>-0.098703</td>\n",
       "      <td>-0.098698</td>\n",
       "      <td>-0.098693</td>\n",
       "      <td>-0.098688</td>\n",
       "      <td>-0.098683</td>\n",
       "      <td>-0.098678</td>\n",
       "      <td>-0.098673</td>\n",
       "      <td>-0.098668</td>\n",
       "      <td>-0.098663</td>\n",
       "      <td>-0.098659</td>\n",
       "      <td>-0.098654</td>\n",
       "      <td>-0.098649</td>\n",
       "      <td>-0.098644</td>\n",
       "      <td>-0.098639</td>\n",
       "      <td>-0.098634</td>\n",
       "      <td>-0.098629</td>\n",
       "      <td>-0.098624</td>\n",
       "      <td>-0.098619</td>\n",
       "      <td>-0.098614</td>\n",
       "      <td>-0.098609</td>\n",
       "      <td>-0.098604</td>\n",
       "      <td>-0.098599</td>\n",
       "      <td>-0.098594</td>\n",
       "      <td>-0.098589</td>\n",
       "      <td>-0.098584</td>\n",
       "      <td>-0.098579</td>\n",
       "      <td>-0.098574</td>\n",
       "      <td>-0.098570</td>\n",
       "      <td>-0.098565</td>\n",
       "      <td>-0.098560</td>\n",
       "      <td>-0.098555</td>\n",
       "      <td>-0.098550</td>\n",
       "      <td>-0.098545</td>\n",
       "      <td>-0.098540</td>\n",
       "      <td>-0.098535</td>\n",
       "      <td>-0.098530</td>\n",
       "      <td>-0.098525</td>\n",
       "      <td>-0.098521</td>\n",
       "      <td>-0.098516</td>\n",
       "      <td>-0.098511</td>\n",
       "      <td>-0.098506</td>\n",
       "      <td>-0.098501</td>\n",
       "      <td>-0.098496</td>\n",
       "      <td>-0.098491</td>\n",
       "      <td>-0.098486</td>\n",
       "      <td>-0.098481</td>\n",
       "      <td>-0.098476</td>\n",
       "      <td>-0.098472</td>\n",
       "      <td>-0.098467</td>\n",
       "      <td>-0.098462</td>\n",
       "      <td>-0.098457</td>\n",
       "      <td>-0.098452</td>\n",
       "      <td>-0.098447</td>\n",
       "      <td>-0.098442</td>\n",
       "      <td>-0.098437</td>\n",
       "      <td>-0.098432</td>\n",
       "      <td>-0.098428</td>\n",
       "      <td>-0.098423</td>\n",
       "      <td>-0.098418</td>\n",
       "      <td>-0.098413</td>\n",
       "      <td>-0.098408</td>\n",
       "      <td>-0.098403</td>\n",
       "      <td>-0.098398</td>\n",
       "      <td>-0.098393</td>\n",
       "      <td>-0.098389</td>\n",
       "      <td>-0.098384</td>\n",
       "      <td>-0.098379</td>\n",
       "      <td>-0.098374</td>\n",
       "      <td>-0.098369</td>\n",
       "      <td>-0.098364</td>\n",
       "      <td>-0.098359</td>\n",
       "      <td>-0.098355</td>\n",
       "      <td>-0.098350</td>\n",
       "      <td>-0.098345</td>\n",
       "      <td>-0.098340</td>\n",
       "      <td>-0.098335</td>\n",
       "      <td>-0.09833</td>\n",
       "      <td>-0.098325</td>\n",
       "      <td>-0.098321</td>\n",
       "      <td>-0.098316</td>\n",
       "      <td>-0.098311</td>\n",
       "      <td>-0.098306</td>\n",
       "      <td>-0.098301</td>\n",
       "      <td>-0.098296</td>\n",
       "      <td>-0.098291</td>\n",
       "      <td>-0.098287</td>\n",
       "      <td>-0.098282</td>\n",
       "      <td>-0.098277</td>\n",
       "      <td>-0.098272</td>\n",
       "      <td>-0.098267</td>\n",
       "      <td>-0.098262</td>\n",
       "      <td>-0.098257</td>\n",
       "      <td>-0.098253</td>\n",
       "      <td>-0.098248</td>\n",
       "      <td>-0.098243</td>\n",
       "      <td>-0.098238</td>\n",
       "      <td>-0.098233</td>\n",
       "      <td>-0.098228</td>\n",
       "      <td>-0.098223</td>\n",
       "      <td>-0.098219</td>\n",
       "      <td>-0.098214</td>\n",
       "      <td>-0.098209</td>\n",
       "      <td>-0.098204</td>\n",
       "      <td>-0.098199</td>\n",
       "      <td>-0.098194</td>\n",
       "      <td>-0.098190</td>\n",
       "      <td>-0.098185</td>\n",
       "      <td>-0.098180</td>\n",
       "      <td>-0.098175</td>\n",
       "      <td>-0.098170</td>\n",
       "      <td>-0.098165</td>\n",
       "      <td>-0.098160</td>\n",
       "      <td>-0.098156</td>\n",
       "      <td>-0.098151</td>\n",
       "      <td>-0.098146</td>\n",
       "      <td>-0.098141</td>\n",
       "      <td>-0.098136</td>\n",
       "      <td>-0.098131</td>\n",
       "      <td>-0.098127</td>\n",
       "      <td>-0.098122</td>\n",
       "      <td>-0.098117</td>\n",
       "      <td>-0.098112</td>\n",
       "      <td>-0.098107</td>\n",
       "      <td>-0.098102</td>\n",
       "      <td>-0.098097</td>\n",
       "      <td>-0.098093</td>\n",
       "      <td>-0.098088</td>\n",
       "      <td>-0.098083</td>\n",
       "      <td>-0.098078</td>\n",
       "      <td>-0.098073</td>\n",
       "      <td>-0.098068</td>\n",
       "      <td>-0.098064</td>\n",
       "      <td>-0.098059</td>\n",
       "      <td>-0.098054</td>\n",
       "      <td>-0.098049</td>\n",
       "      <td>-0.098044</td>\n",
       "      <td>-0.098039</td>\n",
       "      <td>-0.098035</td>\n",
       "      <td>-0.098030</td>\n",
       "      <td>-0.098025</td>\n",
       "      <td>-0.098020</td>\n",
       "      <td>-0.098015</td>\n",
       "      <td>-0.09801</td>\n",
       "      <td>-0.098005</td>\n",
       "      <td>-0.098001</td>\n",
       "      <td>-0.097996</td>\n",
       "      <td>-0.097991</td>\n",
       "      <td>-0.097986</td>\n",
       "      <td>-0.097981</td>\n",
       "      <td>-0.097976</td>\n",
       "      <td>-0.097972</td>\n",
       "      <td>-0.097967</td>\n",
       "      <td>-0.097962</td>\n",
       "      <td>-0.097957</td>\n",
       "      <td>-0.097952</td>\n",
       "      <td>-0.097947</td>\n",
       "      <td>-0.097943</td>\n",
       "      <td>-0.097938</td>\n",
       "      <td>-0.097933</td>\n",
       "      <td>-0.097928</td>\n",
       "      <td>-0.097923</td>\n",
       "      <td>-0.097918</td>\n",
       "      <td>-0.097914</td>\n",
       "      <td>-0.097909</td>\n",
       "      <td>-0.097904</td>\n",
       "      <td>-0.097899</td>\n",
       "      <td>-0.097894</td>\n",
       "      <td>-0.097889</td>\n",
       "      <td>-0.097885</td>\n",
       "      <td>-0.097880</td>\n",
       "      <td>-0.097875</td>\n",
       "      <td>-0.097870</td>\n",
       "      <td>-0.097865</td>\n",
       "      <td>-0.097860</td>\n",
       "      <td>-0.097856</td>\n",
       "      <td>-0.097851</td>\n",
       "      <td>-0.097846</td>\n",
       "      <td>-0.097841</td>\n",
       "      <td>-0.097836</td>\n",
       "      <td>-0.097831</td>\n",
       "      <td>-0.097827</td>\n",
       "      <td>-0.097822</td>\n",
       "      <td>-0.097817</td>\n",
       "      <td>-0.097812</td>\n",
       "      <td>-0.097807</td>\n",
       "      <td>-0.097802</td>\n",
       "      <td>-0.097798</td>\n",
       "      <td>-0.097793</td>\n",
       "      <td>-0.097788</td>\n",
       "      <td>-0.097783</td>\n",
       "      <td>-0.097778</td>\n",
       "      <td>-0.097773</td>\n",
       "      <td>-0.097769</td>\n",
       "      <td>-0.097764</td>\n",
       "      <td>-0.097759</td>\n",
       "      <td>-0.097754</td>\n",
       "      <td>-0.097749</td>\n",
       "      <td>-0.097744</td>\n",
       "      <td>-0.097740</td>\n",
       "      <td>-0.097735</td>\n",
       "      <td>-0.09773</td>\n",
       "      <td>-0.097725</td>\n",
       "      <td>-0.097720</td>\n",
       "      <td>-0.097715</td>\n",
       "      <td>-0.097711</td>\n",
       "      <td>-0.097706</td>\n",
       "      <td>-0.097701</td>\n",
       "      <td>-0.097696</td>\n",
       "      <td>-0.097691</td>\n",
       "      <td>-0.097686</td>\n",
       "      <td>-0.097682</td>\n",
       "      <td>-0.097677</td>\n",
       "      <td>-0.097672</td>\n",
       "      <td>-0.097667</td>\n",
       "      <td>-0.097662</td>\n",
       "      <td>-0.097658</td>\n",
       "      <td>-0.097653</td>\n",
       "      <td>-0.097648</td>\n",
       "      <td>-0.097643</td>\n",
       "      <td>-0.097638</td>\n",
       "      <td>-0.097633</td>\n",
       "      <td>-0.097629</td>\n",
       "      <td>-0.097624</td>\n",
       "      <td>-0.097619</td>\n",
       "      <td>-0.097614</td>\n",
       "      <td>-0.097609</td>\n",
       "      <td>-0.097605</td>\n",
       "      <td>-0.097600</td>\n",
       "      <td>-0.097595</td>\n",
       "      <td>-0.097590</td>\n",
       "      <td>-0.097585</td>\n",
       "      <td>-0.097580</td>\n",
       "      <td>-0.097576</td>\n",
       "      <td>-0.097571</td>\n",
       "      <td>-0.097566</td>\n",
       "      <td>-0.097561</td>\n",
       "      <td>-0.097556</td>\n",
       "      <td>-0.097551</td>\n",
       "      <td>-0.097547</td>\n",
       "      <td>-0.097542</td>\n",
       "      <td>-0.097537</td>\n",
       "      <td>-0.097532</td>\n",
       "      <td>-0.097527</td>\n",
       "      <td>-0.097522</td>\n",
       "      <td>-0.097518</td>\n",
       "      <td>-0.097513</td>\n",
       "      <td>-0.097508</td>\n",
       "      <td>-0.097503</td>\n",
       "      <td>-0.097498</td>\n",
       "      <td>-0.097494</td>\n",
       "      <td>-0.097489</td>\n",
       "      <td>-0.097484</td>\n",
       "      <td>-0.097479</td>\n",
       "      <td>-0.097474</td>\n",
       "      <td>-0.097470</td>\n",
       "      <td>-0.097465</td>\n",
       "      <td>-0.097460</td>\n",
       "      <td>-0.097455</td>\n",
       "      <td>-0.097450</td>\n",
       "      <td>-0.097445</td>\n",
       "      <td>-0.097441</td>\n",
       "      <td>-0.097436</td>\n",
       "      <td>-0.097431</td>\n",
       "      <td>-0.097426</td>\n",
       "      <td>-0.097421</td>\n",
       "      <td>-0.097417</td>\n",
       "      <td>-0.097412</td>\n",
       "      <td>-0.097407</td>\n",
       "      <td>-0.097402</td>\n",
       "      <td>-0.097397</td>\n",
       "      <td>-0.097392</td>\n",
       "      <td>-0.097388</td>\n",
       "      <td>-0.097383</td>\n",
       "      <td>-0.097378</td>\n",
       "      <td>-0.097373</td>\n",
       "      <td>-0.097368</td>\n",
       "      <td>-0.097364</td>\n",
       "      <td>-0.097359</td>\n",
       "      <td>-0.097354</td>\n",
       "      <td>-0.097349</td>\n",
       "      <td>-0.097344</td>\n",
       "      <td>-0.097340</td>\n",
       "      <td>-0.097335</td>\n",
       "      <td>-0.097330</td>\n",
       "      <td>-0.097325</td>\n",
       "      <td>-0.09732</td>\n",
       "      <td>-0.097315</td>\n",
       "      <td>-0.097311</td>\n",
       "      <td>-0.097306</td>\n",
       "      <td>-0.097301</td>\n",
       "      <td>-0.097296</td>\n",
       "      <td>-0.097291</td>\n",
       "      <td>-0.097287</td>\n",
       "      <td>-0.097282</td>\n",
       "      <td>-0.097277</td>\n",
       "      <td>-0.097272</td>\n",
       "      <td>-0.097267</td>\n",
       "      <td>-0.097263</td>\n",
       "      <td>-0.097258</td>\n",
       "      <td>-0.097253</td>\n",
       "      <td>-0.097248</td>\n",
       "      <td>-0.097243</td>\n",
       "      <td>-0.097239</td>\n",
       "      <td>-0.097234</td>\n",
       "      <td>-0.097229</td>\n",
       "      <td>-0.097224</td>\n",
       "      <td>-0.097219</td>\n",
       "      <td>-0.097215</td>\n",
       "      <td>-0.09721</td>\n",
       "      <td>-0.097205</td>\n",
       "      <td>-0.097200</td>\n",
       "      <td>-0.097195</td>\n",
       "      <td>-0.097191</td>\n",
       "      <td>-0.097186</td>\n",
       "      <td>-0.097181</td>\n",
       "      <td>-0.097176</td>\n",
       "      <td>-0.097171</td>\n",
       "      <td>-0.097167</td>\n",
       "      <td>-0.097162</td>\n",
       "      <td>-0.097157</td>\n",
       "      <td>-0.097152</td>\n",
       "      <td>-0.097147</td>\n",
       "      <td>-0.097143</td>\n",
       "      <td>-0.097138</td>\n",
       "      <td>-0.097133</td>\n",
       "      <td>-0.097128</td>\n",
       "      <td>-0.097124</td>\n",
       "      <td>-0.097119</td>\n",
       "      <td>-0.097114</td>\n",
       "      <td>-0.097109</td>\n",
       "      <td>-0.097104</td>\n",
       "      <td>-0.097099</td>\n",
       "      <td>-0.097095</td>\n",
       "      <td>-0.097090</td>\n",
       "      <td>-0.097085</td>\n",
       "      <td>-0.097080</td>\n",
       "      <td>-0.097076</td>\n",
       "      <td>-0.097071</td>\n",
       "      <td>-0.097066</td>\n",
       "      <td>-0.097061</td>\n",
       "      <td>-0.097056</td>\n",
       "      <td>-0.097052</td>\n",
       "      <td>-0.097047</td>\n",
       "      <td>-0.097042</td>\n",
       "      <td>-0.097037</td>\n",
       "      <td>-0.097032</td>\n",
       "      <td>-0.097028</td>\n",
       "      <td>-0.097023</td>\n",
       "      <td>-0.097018</td>\n",
       "      <td>-0.097013</td>\n",
       "      <td>-0.097009</td>\n",
       "      <td>-0.097004</td>\n",
       "      <td>-0.096999</td>\n",
       "      <td>-0.096994</td>\n",
       "      <td>-0.096990</td>\n",
       "      <td>-0.096985</td>\n",
       "      <td>-0.096980</td>\n",
       "      <td>-0.096975</td>\n",
       "      <td>-0.096970</td>\n",
       "      <td>-0.096966</td>\n",
       "      <td>-0.096961</td>\n",
       "      <td>-0.096956</td>\n",
       "      <td>-0.096951</td>\n",
       "      <td>-0.096946</td>\n",
       "      <td>-0.096942</td>\n",
       "      <td>-0.096937</td>\n",
       "      <td>-0.096932</td>\n",
       "      <td>-0.096927</td>\n",
       "      <td>-0.096923</td>\n",
       "      <td>-0.096918</td>\n",
       "      <td>-0.096913</td>\n",
       "      <td>-0.096908</td>\n",
       "      <td>-0.096904</td>\n",
       "      <td>-0.096899</td>\n",
       "      <td>-0.096894</td>\n",
       "      <td>-0.096889</td>\n",
       "      <td>-0.096884</td>\n",
       "      <td>-0.096880</td>\n",
       "      <td>-0.096875</td>\n",
       "      <td>-0.096870</td>\n",
       "      <td>-0.096865</td>\n",
       "      <td>-0.096861</td>\n",
       "      <td>-0.096856</td>\n",
       "      <td>-0.096851</td>\n",
       "      <td>-0.096846</td>\n",
       "      <td>-0.096842</td>\n",
       "      <td>-0.096837</td>\n",
       "      <td>-0.096832</td>\n",
       "      <td>-0.096827</td>\n",
       "      <td>-0.096823</td>\n",
       "      <td>-0.096818</td>\n",
       "      <td>-0.096813</td>\n",
       "      <td>-0.096808</td>\n",
       "      <td>-0.096803</td>\n",
       "      <td>-0.096799</td>\n",
       "      <td>-0.096794</td>\n",
       "      <td>-0.096789</td>\n",
       "      <td>-0.096784</td>\n",
       "      <td>-0.09678</td>\n",
       "      <td>-0.096775</td>\n",
       "      <td>-0.096770</td>\n",
       "      <td>-0.096765</td>\n",
       "      <td>-0.096761</td>\n",
       "      <td>-0.096756</td>\n",
       "      <td>-0.096751</td>\n",
       "      <td>-0.096746</td>\n",
       "      <td>-0.096742</td>\n",
       "      <td>-0.096737</td>\n",
       "      <td>-0.096732</td>\n",
       "      <td>-0.096727</td>\n",
       "      <td>-0.096723</td>\n",
       "      <td>-0.096718</td>\n",
       "      <td>-0.096713</td>\n",
       "      <td>-0.096708</td>\n",
       "      <td>-0.096704</td>\n",
       "      <td>-0.096699</td>\n",
       "      <td>-0.096694</td>\n",
       "      <td>-0.096689</td>\n",
       "      <td>-0.096685</td>\n",
       "      <td>-0.09668</td>\n",
       "      <td>-0.096675</td>\n",
       "      <td>-0.096670</td>\n",
       "      <td>-0.096666</td>\n",
       "      <td>-0.096661</td>\n",
       "      <td>-0.096656</td>\n",
       "      <td>-0.096651</td>\n",
       "      <td>-0.096647</td>\n",
       "      <td>-0.096642</td>\n",
       "      <td>-0.096637</td>\n",
       "      <td>-0.096632</td>\n",
       "      <td>-0.096628</td>\n",
       "      <td>-0.096623</td>\n",
       "      <td>-0.096618</td>\n",
       "      <td>-0.096613</td>\n",
       "      <td>-0.096609</td>\n",
       "      <td>-0.096604</td>\n",
       "      <td>-0.096599</td>\n",
       "      <td>-0.096595</td>\n",
       "      <td>-0.09659</td>\n",
       "      <td>-0.096585</td>\n",
       "      <td>-0.096580</td>\n",
       "      <td>-0.096576</td>\n",
       "      <td>-0.096571</td>\n",
       "      <td>-0.096566</td>\n",
       "      <td>-0.096561</td>\n",
       "      <td>-0.096557</td>\n",
       "      <td>-0.096552</td>\n",
       "      <td>-0.096547</td>\n",
       "      <td>-0.096543</td>\n",
       "      <td>-0.096538</td>\n",
       "      <td>-0.096533</td>\n",
       "      <td>-0.096528</td>\n",
       "      <td>-0.096524</td>\n",
       "      <td>-0.096519</td>\n",
       "      <td>-0.096514</td>\n",
       "      <td>-0.096509</td>\n",
       "      <td>-0.096505</td>\n",
       "      <td>-0.096500</td>\n",
       "      <td>-0.096495</td>\n",
       "      <td>-0.096491</td>\n",
       "      <td>-0.096486</td>\n",
       "      <td>-0.096481</td>\n",
       "      <td>-0.096476</td>\n",
       "      <td>-0.096472</td>\n",
       "      <td>-0.096467</td>\n",
       "      <td>-0.096462</td>\n",
       "      <td>-0.096458</td>\n",
       "      <td>-0.096453</td>\n",
       "      <td>-0.096448</td>\n",
       "      <td>-0.096443</td>\n",
       "      <td>-0.096439</td>\n",
       "      <td>-0.096434</td>\n",
       "      <td>-0.096429</td>\n",
       "      <td>-0.096425</td>\n",
       "      <td>-0.096420</td>\n",
       "      <td>-0.096415</td>\n",
       "      <td>-0.096410</td>\n",
       "      <td>-0.096406</td>\n",
       "      <td>-0.096401</td>\n",
       "      <td>-0.096396</td>\n",
       "      <td>-0.096392</td>\n",
       "      <td>-0.096387</td>\n",
       "      <td>-0.096382</td>\n",
       "      <td>-0.096377</td>\n",
       "      <td>-0.096373</td>\n",
       "      <td>-0.096368</td>\n",
       "      <td>-0.096363</td>\n",
       "      <td>-0.096359</td>\n",
       "      <td>-0.096354</td>\n",
       "      <td>-0.096349</td>\n",
       "      <td>-0.096345</td>\n",
       "      <td>-0.096340</td>\n",
       "      <td>-0.096335</td>\n",
       "      <td>-0.096330</td>\n",
       "      <td>-0.096326</td>\n",
       "      <td>-0.096321</td>\n",
       "      <td>-0.096316</td>\n",
       "      <td>-0.096312</td>\n",
       "      <td>-0.096307</td>\n",
       "      <td>-0.096302</td>\n",
       "      <td>-0.096298</td>\n",
       "      <td>-0.096293</td>\n",
       "      <td>-0.096288</td>\n",
       "      <td>-0.096284</td>\n",
       "      <td>-0.096279</td>\n",
       "      <td>-0.096274</td>\n",
       "      <td>-0.096269</td>\n",
       "      <td>-0.096265</td>\n",
       "      <td>-0.096260</td>\n",
       "      <td>-0.096255</td>\n",
       "      <td>-0.096251</td>\n",
       "      <td>-0.096246</td>\n",
       "      <td>-0.096241</td>\n",
       "      <td>-0.096237</td>\n",
       "      <td>-0.096232</td>\n",
       "      <td>-0.096227</td>\n",
       "      <td>-0.096223</td>\n",
       "      <td>-0.096218</td>\n",
       "      <td>-0.096213</td>\n",
       "      <td>-0.096209</td>\n",
       "      <td>-0.096204</td>\n",
       "      <td>-0.096199</td>\n",
       "      <td>-0.096195</td>\n",
       "      <td>-0.096190</td>\n",
       "      <td>-0.096185</td>\n",
       "      <td>-0.096181</td>\n",
       "      <td>-0.096176</td>\n",
       "      <td>-0.096171</td>\n",
       "      <td>-0.096167</td>\n",
       "      <td>-0.096162</td>\n",
       "      <td>-0.096157</td>\n",
       "      <td>-0.096153</td>\n",
       "      <td>-0.096148</td>\n",
       "      <td>-0.096143</td>\n",
       "      <td>-0.096139</td>\n",
       "      <td>-0.096134</td>\n",
       "      <td>-0.096129</td>\n",
       "      <td>-0.096125</td>\n",
       "      <td>-0.096120</td>\n",
       "      <td>-0.096115</td>\n",
       "      <td>-0.096111</td>\n",
       "      <td>-0.096106</td>\n",
       "      <td>-0.096101</td>\n",
       "      <td>-0.096097</td>\n",
       "      <td>-0.096092</td>\n",
       "      <td>-0.096087</td>\n",
       "      <td>-0.096083</td>\n",
       "      <td>-0.096078</td>\n",
       "      <td>-0.096073</td>\n",
       "      <td>-0.096069</td>\n",
       "      <td>-0.096064</td>\n",
       "      <td>-0.096059</td>\n",
       "      <td>-0.096055</td>\n",
       "      <td>-0.096050</td>\n",
       "      <td>-0.096045</td>\n",
       "      <td>-0.096041</td>\n",
       "      <td>-0.096036</td>\n",
       "      <td>-0.096031</td>\n",
       "      <td>-0.096027</td>\n",
       "      <td>-0.096022</td>\n",
       "      <td>-0.096017</td>\n",
       "      <td>-0.096013</td>\n",
       "      <td>-0.096008</td>\n",
       "      <td>-0.096004</td>\n",
       "      <td>-0.095999</td>\n",
       "      <td>-0.095994</td>\n",
       "      <td>-0.095990</td>\n",
       "      <td>-0.095985</td>\n",
       "      <td>-0.095980</td>\n",
       "      <td>-0.095976</td>\n",
       "      <td>-0.095971</td>\n",
       "      <td>-0.095966</td>\n",
       "      <td>-0.095962</td>\n",
       "      <td>-0.095957</td>\n",
       "      <td>-0.095953</td>\n",
       "      <td>-0.095948</td>\n",
       "      <td>-0.095943</td>\n",
       "      <td>-0.095939</td>\n",
       "      <td>-0.095934</td>\n",
       "      <td>-0.095929</td>\n",
       "      <td>-0.095925</td>\n",
       "      <td>-0.09592</td>\n",
       "      <td>-0.095915</td>\n",
       "      <td>-0.095911</td>\n",
       "      <td>-0.095906</td>\n",
       "      <td>-0.095902</td>\n",
       "      <td>-0.095897</td>\n",
       "      <td>-0.095892</td>\n",
       "      <td>-0.095888</td>\n",
       "      <td>-0.095883</td>\n",
       "      <td>-0.095878</td>\n",
       "      <td>-0.095874</td>\n",
       "      <td>-0.095869</td>\n",
       "      <td>-0.095865</td>\n",
       "      <td>-0.09586</td>\n",
       "      <td>-0.095855</td>\n",
       "      <td>-0.095851</td>\n",
       "      <td>-0.095846</td>\n",
       "      <td>-0.095842</td>\n",
       "      <td>-0.095837</td>\n",
       "      <td>-0.095832</td>\n",
       "      <td>-0.095828</td>\n",
       "      <td>-0.095823</td>\n",
       "      <td>-0.095819</td>\n",
       "      <td>-0.095814</td>\n",
       "      <td>-0.095809</td>\n",
       "      <td>-0.095805</td>\n",
       "      <td>-0.095800</td>\n",
       "      <td>-0.095795</td>\n",
       "      <td>-0.095791</td>\n",
       "      <td>-0.095786</td>\n",
       "      <td>-0.095782</td>\n",
       "      <td>-0.095777</td>\n",
       "      <td>-0.095772</td>\n",
       "      <td>-0.095768</td>\n",
       "      <td>-0.095763</td>\n",
       "      <td>-0.095759</td>\n",
       "      <td>-0.095754</td>\n",
       "      <td>-0.095749</td>\n",
       "      <td>-0.095745</td>\n",
       "      <td>-0.095740</td>\n",
       "      <td>-0.095736</td>\n",
       "      <td>-0.095731</td>\n",
       "      <td>-0.095727</td>\n",
       "      <td>-0.095722</td>\n",
       "      <td>-0.095717</td>\n",
       "      <td>-0.095713</td>\n",
       "      <td>-0.095708</td>\n",
       "      <td>-0.095704</td>\n",
       "      <td>-0.095699</td>\n",
       "      <td>-0.095694</td>\n",
       "      <td>-0.09569</td>\n",
       "      <td>-0.095685</td>\n",
       "      <td>-0.095681</td>\n",
       "      <td>-0.095676</td>\n",
       "      <td>-0.095671</td>\n",
       "      <td>-0.095667</td>\n",
       "      <td>-0.095662</td>\n",
       "      <td>-0.095658</td>\n",
       "      <td>-0.095653</td>\n",
       "      <td>-0.095649</td>\n",
       "      <td>-0.095644</td>\n",
       "      <td>-0.095640</td>\n",
       "      <td>-0.095635</td>\n",
       "      <td>-0.095630</td>\n",
       "      <td>-0.095626</td>\n",
       "      <td>-0.095621</td>\n",
       "      <td>-0.095617</td>\n",
       "      <td>-0.095612</td>\n",
       "      <td>-0.095608</td>\n",
       "      <td>-0.095603</td>\n",
       "      <td>-0.095598</td>\n",
       "      <td>-0.095594</td>\n",
       "      <td>-0.095589</td>\n",
       "      <td>-0.095585</td>\n",
       "      <td>-0.095580</td>\n",
       "      <td>-0.095576</td>\n",
       "      <td>-0.095571</td>\n",
       "      <td>-0.095566</td>\n",
       "      <td>-0.095562</td>\n",
       "      <td>-0.095557</td>\n",
       "      <td>-0.095553</td>\n",
       "      <td>-0.095548</td>\n",
       "      <td>-0.095544</td>\n",
       "      <td>-0.095539</td>\n",
       "      <td>-0.095535</td>\n",
       "      <td>-0.095530</td>\n",
       "      <td>-0.095526</td>\n",
       "      <td>-0.095521</td>\n",
       "      <td>-0.095516</td>\n",
       "      <td>-0.095512</td>\n",
       "      <td>-0.095507</td>\n",
       "      <td>-0.095503</td>\n",
       "      <td>-0.095498</td>\n",
       "      <td>-0.095494</td>\n",
       "      <td>-0.095489</td>\n",
       "      <td>-0.095485</td>\n",
       "      <td>-0.095480</td>\n",
       "      <td>-0.095475</td>\n",
       "      <td>-0.095471</td>\n",
       "      <td>-0.095466</td>\n",
       "      <td>-0.095462</td>\n",
       "      <td>-0.095457</td>\n",
       "      <td>-0.095453</td>\n",
       "      <td>-0.095448</td>\n",
       "      <td>-0.095444</td>\n",
       "      <td>-0.095439</td>\n",
       "      <td>-0.095435</td>\n",
       "      <td>-0.095430</td>\n",
       "      <td>-0.095426</td>\n",
       "      <td>-0.095421</td>\n",
       "      <td>-0.095417</td>\n",
       "      <td>-0.095412</td>\n",
       "      <td>-0.095408</td>\n",
       "      <td>-0.095403</td>\n",
       "      <td>-0.095399</td>\n",
       "      <td>-0.095394</td>\n",
       "      <td>-0.095390</td>\n",
       "      <td>-0.095385</td>\n",
       "      <td>-0.095380</td>\n",
       "      <td>-0.095376</td>\n",
       "      <td>-0.095371</td>\n",
       "      <td>-0.095367</td>\n",
       "      <td>-0.095362</td>\n",
       "      <td>-0.095358</td>\n",
       "      <td>-0.095353</td>\n",
       "      <td>-0.095349</td>\n",
       "      <td>-0.095344</td>\n",
       "      <td>-0.095340</td>\n",
       "      <td>-0.095335</td>\n",
       "      <td>-0.095331</td>\n",
       "      <td>-0.095326</td>\n",
       "      <td>-0.095322</td>\n",
       "      <td>-0.095317</td>\n",
       "      <td>-0.095313</td>\n",
       "      <td>-0.095308</td>\n",
       "      <td>-0.095304</td>\n",
       "      <td>-0.095299</td>\n",
       "      <td>-0.095295</td>\n",
       "      <td>-0.095290</td>\n",
       "      <td>0.056804</td>\n",
       "      <td>0.051225</td>\n",
       "      <td>-0.025032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4         5         6         7         8         9    \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.004554 -0.030177 -0.072558 -0.182296 -0.198521   \n",
       "1  0.0  0.0  0.0  0.0  0.0 -0.055126 -0.037456 -0.030337 -0.103080 -0.103322   \n",
       "\n",
       "        10        11        12        13        14        15        16   \\\n",
       "0 -0.201616 -0.203007 -0.204214 -0.205417 -0.206631 -0.207850 -0.209070   \n",
       "1 -0.102151 -0.101233 -0.100570 -0.100030 -0.099548 -0.099096 -0.098664   \n",
       "\n",
       "        17        18        19        20        21        22        23   \\\n",
       "0 -0.210285 -0.211491 -0.212684 -0.213861 -0.215019 -0.216158 -0.217275   \n",
       "1 -0.098248 -0.097849 -0.097468 -0.097108 -0.096769 -0.096454 -0.096163   \n",
       "\n",
       "        24        25        26        27        28        29        30   \\\n",
       "0 -0.218369 -0.219441 -0.220489 -0.221512 -0.222512 -0.223487 -0.224439   \n",
       "1 -0.095897 -0.095655 -0.095439 -0.095246 -0.095078 -0.094932 -0.094808   \n",
       "\n",
       "        31        32        33        34        35        36        37   \\\n",
       "0 -0.225366 -0.226270 -0.227150 -0.228007 -0.228841 -0.229652 -0.230442   \n",
       "1 -0.094706 -0.094623 -0.094559 -0.094513 -0.094483 -0.094468 -0.094468   \n",
       "\n",
       "        38        39        40        41        42        43        44   \\\n",
       "0 -0.231210 -0.231957 -0.232683 -0.233388 -0.234074 -0.234740 -0.235388   \n",
       "1 -0.094481 -0.094505 -0.094541 -0.094586 -0.094640 -0.094703 -0.094772   \n",
       "\n",
       "        45        46        47        48        49        50        51   \\\n",
       "0 -0.236016 -0.236627 -0.237220 -0.237796 -0.238355 -0.238898 -0.239425   \n",
       "1 -0.094848 -0.094929 -0.095016 -0.095106 -0.095200 -0.095297 -0.095396   \n",
       "\n",
       "        52        53        54        55        56        57        58   \\\n",
       "0 -0.239936 -0.240432 -0.240914 -0.241381 -0.241834 -0.242274 -0.242701   \n",
       "1 -0.095497 -0.095600 -0.095704 -0.095808 -0.095913 -0.096017 -0.096122   \n",
       "\n",
       "        59        60        61        62        63        64        65   \\\n",
       "0 -0.243114 -0.243516 -0.243905 -0.244282 -0.244648 -0.245003 -0.245347   \n",
       "1 -0.096225 -0.096328 -0.096430 -0.096530 -0.096630 -0.096727 -0.096823   \n",
       "\n",
       "        66        67        68        69        70        71        72   \\\n",
       "0 -0.245681 -0.246005 -0.246318 -0.246622 -0.246917 -0.247202 -0.247479   \n",
       "1 -0.096918 -0.097010 -0.097100 -0.097189 -0.097275 -0.097359 -0.097441   \n",
       "\n",
       "        73        74        75        76        77        78        79   \\\n",
       "0 -0.247747 -0.248007 -0.248258 -0.248502 -0.248739 -0.248968 -0.249190   \n",
       "1 -0.097521 -0.097598 -0.097674 -0.097747 -0.097818 -0.097886 -0.097953   \n",
       "\n",
       "        80        81        82        83        84        85        86   \\\n",
       "0 -0.249404 -0.249613 -0.249814 -0.250009 -0.250199 -0.250382 -0.250559   \n",
       "1 -0.098017 -0.098080 -0.098140 -0.098198 -0.098254 -0.098308 -0.098360   \n",
       "\n",
       "        87        88        89        90        91        92        93   \\\n",
       "0 -0.250731 -0.250897 -0.251058 -0.251213 -0.251364 -0.251510 -0.251651   \n",
       "1 -0.098410 -0.098458 -0.098504 -0.098549 -0.098591 -0.098632 -0.098672   \n",
       "\n",
       "        94        95        96        97        98        99        100  \\\n",
       "0 -0.251788 -0.251920 -0.252048 -0.252172 -0.252292 -0.252408 -0.252520   \n",
       "1 -0.098709 -0.098745 -0.098780 -0.098813 -0.098845 -0.098875 -0.098903   \n",
       "\n",
       "        101       102       103       104       105       106       107  \\\n",
       "0 -0.252628 -0.252733 -0.252834 -0.252932 -0.253026 -0.253118 -0.253206   \n",
       "1 -0.098931 -0.098957 -0.098982 -0.099006 -0.099028 -0.099050 -0.099070   \n",
       "\n",
       "        108       109       110       111       112       113       114  \\\n",
       "0 -0.253292 -0.253374 -0.253454 -0.253531 -0.253605 -0.253677 -0.253746   \n",
       "1 -0.099089 -0.099107 -0.099125 -0.099141 -0.099156 -0.099171 -0.099185   \n",
       "\n",
       "        115       116       117       118       119       120       121  \\\n",
       "0 -0.253812 -0.253877 -0.253939 -0.253999 -0.254057 -0.254112 -0.254166   \n",
       "1 -0.099197 -0.099209 -0.099221 -0.099231 -0.099241 -0.099250 -0.099259   \n",
       "\n",
       "        122       123       124       125       126       127       128  \\\n",
       "0 -0.254218 -0.254267 -0.254315 -0.254361 -0.254405 -0.254448 -0.254489   \n",
       "1 -0.099267 -0.099274 -0.099281 -0.099287 -0.099293 -0.099298 -0.099303   \n",
       "\n",
       "        129       130       131       132       133       134       135  \\\n",
       "0 -0.254529 -0.254566 -0.254603 -0.254637 -0.254671 -0.254703 -0.254734   \n",
       "1 -0.099307 -0.099311 -0.099314 -0.099317 -0.099319 -0.099322 -0.099323   \n",
       "\n",
       "        136       137       138       139       140       141       142  \\\n",
       "0 -0.254763 -0.254791 -0.254818 -0.254844 -0.254868 -0.254892 -0.254914   \n",
       "1 -0.099325 -0.099326 -0.099327 -0.099327 -0.099327 -0.099327 -0.099327   \n",
       "\n",
       "        143       144       145       146       147       148       149  \\\n",
       "0 -0.254935 -0.254955 -0.254975 -0.254993 -0.255010 -0.255027 -0.255042   \n",
       "1 -0.099326 -0.099326 -0.099324 -0.099323 -0.099322 -0.099320 -0.099318   \n",
       "\n",
       "        150       151       152       153       154       155       156  \\\n",
       "0 -0.255057 -0.255071 -0.255083 -0.255096 -0.255107 -0.255118 -0.255128   \n",
       "1 -0.099316 -0.099314 -0.099311 -0.099309 -0.099306 -0.099303 -0.099300   \n",
       "\n",
       "        157       158       159       160       161       162       163  \\\n",
       "0 -0.255137 -0.255145 -0.255153 -0.255160 -0.255167 -0.255173 -0.255178   \n",
       "1 -0.099297 -0.099294 -0.099290 -0.099287 -0.099283 -0.099279 -0.099276   \n",
       "\n",
       "        164       165       166       167       168       169       170  \\\n",
       "0 -0.255183 -0.255187 -0.255191 -0.255194 -0.255197 -0.255199 -0.255201   \n",
       "1 -0.099272 -0.099268 -0.099264 -0.099259 -0.099255 -0.099251 -0.099247   \n",
       "\n",
       "        171       172       173       174       175       176       177  \\\n",
       "0 -0.255202 -0.255202 -0.255203 -0.255203 -0.255202 -0.255201 -0.255200   \n",
       "1 -0.099242 -0.099238 -0.099233 -0.099229 -0.099224 -0.099219 -0.099215   \n",
       "\n",
       "        178       179       180       181       182       183       184  \\\n",
       "0 -0.255198 -0.255196 -0.255193 -0.255190 -0.255187 -0.255184 -0.255180   \n",
       "1 -0.099210 -0.099205 -0.099200 -0.099195 -0.099190 -0.099185 -0.099181   \n",
       "\n",
       "        185       186       187       188       189      190       191  \\\n",
       "0 -0.255175 -0.255171 -0.255166 -0.255161 -0.255156 -0.25515 -0.255144   \n",
       "1 -0.099176 -0.099171 -0.099165 -0.099160 -0.099155 -0.09915 -0.099145   \n",
       "\n",
       "        192       193       194       195       196       197       198  \\\n",
       "0 -0.255138 -0.255132 -0.255125 -0.255118 -0.255111 -0.255104 -0.255096   \n",
       "1 -0.099140 -0.099135 -0.099130 -0.099125 -0.099120 -0.099114 -0.099109   \n",
       "\n",
       "        199       200       201       202       203       204       205  \\\n",
       "0 -0.255088 -0.255080 -0.255072 -0.255064 -0.255055 -0.255047 -0.255038   \n",
       "1 -0.099104 -0.099099 -0.099093 -0.099088 -0.099083 -0.099078 -0.099073   \n",
       "\n",
       "        206       207       208       209       210       211       212  \\\n",
       "0 -0.255029 -0.255019 -0.255010 -0.255000 -0.254991 -0.254981 -0.254971   \n",
       "1 -0.099067 -0.099062 -0.099057 -0.099052 -0.099046 -0.099041 -0.099036   \n",
       "\n",
       "        213       214      215       216       217       218       219  \\\n",
       "0 -0.254961 -0.254950 -0.25494 -0.254930 -0.254919 -0.254908 -0.254897   \n",
       "1 -0.099031 -0.099025 -0.09902 -0.099015 -0.099010 -0.099005 -0.098999   \n",
       "\n",
       "        220       221       222       223       224       225       226  \\\n",
       "0 -0.254886 -0.254875 -0.254864 -0.254853 -0.254841 -0.254830 -0.254818   \n",
       "1 -0.098994 -0.098989 -0.098984 -0.098979 -0.098973 -0.098968 -0.098963   \n",
       "\n",
       "        227       228       229       230       231       232       233  \\\n",
       "0 -0.254807 -0.254795 -0.254783 -0.254771 -0.254759 -0.254747 -0.254735   \n",
       "1 -0.098958 -0.098952 -0.098947 -0.098942 -0.098937 -0.098932 -0.098927   \n",
       "\n",
       "        234       235       236       237       238       239       240  \\\n",
       "0 -0.254723 -0.254710 -0.254698 -0.254685 -0.254673 -0.254660 -0.254648   \n",
       "1 -0.098921 -0.098916 -0.098911 -0.098906 -0.098901 -0.098896 -0.098890   \n",
       "\n",
       "        241       242       243       244       245       246       247  \\\n",
       "0 -0.254635 -0.254623 -0.254610 -0.254597 -0.254584 -0.254571 -0.254558   \n",
       "1 -0.098885 -0.098880 -0.098875 -0.098870 -0.098865 -0.098860 -0.098855   \n",
       "\n",
       "        248       249       250       251       252       253       254  \\\n",
       "0 -0.254545 -0.254532 -0.254519 -0.254506 -0.254493 -0.254480 -0.254467   \n",
       "1 -0.098849 -0.098844 -0.098839 -0.098834 -0.098829 -0.098824 -0.098819   \n",
       "\n",
       "        255       256       257       258       259       260       261  \\\n",
       "0 -0.254453 -0.254440 -0.254427 -0.254413 -0.254400 -0.254387 -0.254373   \n",
       "1 -0.098814 -0.098809 -0.098804 -0.098799 -0.098794 -0.098789 -0.098784   \n",
       "\n",
       "        262       263       264       265       266       267       268  \\\n",
       "0 -0.254360 -0.254346 -0.254333 -0.254319 -0.254306 -0.254292 -0.254279   \n",
       "1 -0.098778 -0.098773 -0.098768 -0.098763 -0.098758 -0.098753 -0.098748   \n",
       "\n",
       "        269       270       271       272       273       274       275  \\\n",
       "0 -0.254265 -0.254252 -0.254238 -0.254225 -0.254211 -0.254197 -0.254184   \n",
       "1 -0.098743 -0.098738 -0.098733 -0.098728 -0.098723 -0.098718 -0.098713   \n",
       "\n",
       "        276       277       278       279       280       281       282  \\\n",
       "0 -0.254170 -0.254156 -0.254143 -0.254129 -0.254115 -0.254102 -0.254088   \n",
       "1 -0.098708 -0.098703 -0.098698 -0.098693 -0.098688 -0.098683 -0.098678   \n",
       "\n",
       "        283       284       285       286       287       288       289  \\\n",
       "0 -0.254074 -0.254061 -0.254047 -0.254033 -0.254019 -0.254006 -0.253992   \n",
       "1 -0.098673 -0.098668 -0.098663 -0.098659 -0.098654 -0.098649 -0.098644   \n",
       "\n",
       "        290       291       292       293       294       295       296  \\\n",
       "0 -0.253978 -0.253964 -0.253951 -0.253937 -0.253923 -0.253910 -0.253896   \n",
       "1 -0.098639 -0.098634 -0.098629 -0.098624 -0.098619 -0.098614 -0.098609   \n",
       "\n",
       "        297       298       299       300       301       302       303  \\\n",
       "0 -0.253882 -0.253868 -0.253855 -0.253841 -0.253827 -0.253814 -0.253800   \n",
       "1 -0.098604 -0.098599 -0.098594 -0.098589 -0.098584 -0.098579 -0.098574   \n",
       "\n",
       "        304       305       306       307       308       309       310  \\\n",
       "0 -0.253786 -0.253773 -0.253759 -0.253745 -0.253732 -0.253718 -0.253705   \n",
       "1 -0.098570 -0.098565 -0.098560 -0.098555 -0.098550 -0.098545 -0.098540   \n",
       "\n",
       "        311       312       313       314       315       316       317  \\\n",
       "0 -0.253691 -0.253677 -0.253664 -0.253650 -0.253637 -0.253623 -0.253609   \n",
       "1 -0.098535 -0.098530 -0.098525 -0.098521 -0.098516 -0.098511 -0.098506   \n",
       "\n",
       "        318       319       320       321       322       323       324  \\\n",
       "0 -0.253596 -0.253582 -0.253569 -0.253555 -0.253542 -0.253528 -0.253515   \n",
       "1 -0.098501 -0.098496 -0.098491 -0.098486 -0.098481 -0.098476 -0.098472   \n",
       "\n",
       "        325       326       327       328       329       330       331  \\\n",
       "0 -0.253501 -0.253488 -0.253474 -0.253461 -0.253448 -0.253434 -0.253421   \n",
       "1 -0.098467 -0.098462 -0.098457 -0.098452 -0.098447 -0.098442 -0.098437   \n",
       "\n",
       "        332       333       334       335       336       337       338  \\\n",
       "0 -0.253407 -0.253394 -0.253381 -0.253367 -0.253354 -0.253341 -0.253327   \n",
       "1 -0.098432 -0.098428 -0.098423 -0.098418 -0.098413 -0.098408 -0.098403   \n",
       "\n",
       "        339       340       341       342       343       344       345  \\\n",
       "0 -0.253314 -0.253301 -0.253288 -0.253275 -0.253261 -0.253248 -0.253235   \n",
       "1 -0.098398 -0.098393 -0.098389 -0.098384 -0.098379 -0.098374 -0.098369   \n",
       "\n",
       "        346       347       348       349       350       351       352  \\\n",
       "0 -0.253222 -0.253209 -0.253195 -0.253182 -0.253169 -0.253156 -0.253143   \n",
       "1 -0.098364 -0.098359 -0.098355 -0.098350 -0.098345 -0.098340 -0.098335   \n",
       "\n",
       "       353       354       355       356       357       358       359  \\\n",
       "0 -0.25313 -0.253117 -0.253104 -0.253091 -0.253078 -0.253065 -0.253052   \n",
       "1 -0.09833 -0.098325 -0.098321 -0.098316 -0.098311 -0.098306 -0.098301   \n",
       "\n",
       "        360       361       362       363       364       365       366  \\\n",
       "0 -0.253039 -0.253026 -0.253013 -0.253000 -0.252988 -0.252975 -0.252962   \n",
       "1 -0.098296 -0.098291 -0.098287 -0.098282 -0.098277 -0.098272 -0.098267   \n",
       "\n",
       "        367       368       369       370       371       372       373  \\\n",
       "0 -0.252949 -0.252936 -0.252924 -0.252911 -0.252898 -0.252885 -0.252873   \n",
       "1 -0.098262 -0.098257 -0.098253 -0.098248 -0.098243 -0.098238 -0.098233   \n",
       "\n",
       "        374       375       376       377       378       379       380  \\\n",
       "0 -0.252860 -0.252847 -0.252835 -0.252822 -0.252809 -0.252797 -0.252784   \n",
       "1 -0.098228 -0.098223 -0.098219 -0.098214 -0.098209 -0.098204 -0.098199   \n",
       "\n",
       "        381       382       383       384       385       386       387  \\\n",
       "0 -0.252772 -0.252759 -0.252747 -0.252734 -0.252722 -0.252709 -0.252697   \n",
       "1 -0.098194 -0.098190 -0.098185 -0.098180 -0.098175 -0.098170 -0.098165   \n",
       "\n",
       "        388       389       390       391       392       393       394  \\\n",
       "0 -0.252685 -0.252672 -0.252660 -0.252647 -0.252635 -0.252623 -0.252611   \n",
       "1 -0.098160 -0.098156 -0.098151 -0.098146 -0.098141 -0.098136 -0.098131   \n",
       "\n",
       "        395       396       397       398       399       400       401  \\\n",
       "0 -0.252598 -0.252586 -0.252574 -0.252562 -0.252549 -0.252537 -0.252525   \n",
       "1 -0.098127 -0.098122 -0.098117 -0.098112 -0.098107 -0.098102 -0.098097   \n",
       "\n",
       "        402       403       404       405       406       407       408  \\\n",
       "0 -0.252513 -0.252501 -0.252489 -0.252477 -0.252464 -0.252452 -0.252440   \n",
       "1 -0.098093 -0.098088 -0.098083 -0.098078 -0.098073 -0.098068 -0.098064   \n",
       "\n",
       "        409       410       411       412       413       414       415  \\\n",
       "0 -0.252428 -0.252416 -0.252404 -0.252393 -0.252381 -0.252369 -0.252357   \n",
       "1 -0.098059 -0.098054 -0.098049 -0.098044 -0.098039 -0.098035 -0.098030   \n",
       "\n",
       "        416       417       418      419       420       421       422  \\\n",
       "0 -0.252345 -0.252333 -0.252321 -0.25231 -0.252298 -0.252286 -0.252274   \n",
       "1 -0.098025 -0.098020 -0.098015 -0.09801 -0.098005 -0.098001 -0.097996   \n",
       "\n",
       "        423       424       425       426       427       428       429  \\\n",
       "0 -0.252263 -0.252251 -0.252239 -0.252228 -0.252216 -0.252204 -0.252193   \n",
       "1 -0.097991 -0.097986 -0.097981 -0.097976 -0.097972 -0.097967 -0.097962   \n",
       "\n",
       "        430       431       432       433       434       435       436  \\\n",
       "0 -0.252181 -0.252170 -0.252158 -0.252147 -0.252135 -0.252124 -0.252112   \n",
       "1 -0.097957 -0.097952 -0.097947 -0.097943 -0.097938 -0.097933 -0.097928   \n",
       "\n",
       "        437       438       439       440       441       442       443  \\\n",
       "0 -0.252101 -0.252089 -0.252078 -0.252067 -0.252055 -0.252044 -0.252033   \n",
       "1 -0.097923 -0.097918 -0.097914 -0.097909 -0.097904 -0.097899 -0.097894   \n",
       "\n",
       "        444       445       446       447       448       449       450  \\\n",
       "0 -0.252021 -0.252010 -0.251999 -0.251988 -0.251976 -0.251965 -0.251954   \n",
       "1 -0.097889 -0.097885 -0.097880 -0.097875 -0.097870 -0.097865 -0.097860   \n",
       "\n",
       "        451       452       453       454       455       456       457  \\\n",
       "0 -0.251943 -0.251932 -0.251921 -0.251909 -0.251898 -0.251887 -0.251876   \n",
       "1 -0.097856 -0.097851 -0.097846 -0.097841 -0.097836 -0.097831 -0.097827   \n",
       "\n",
       "        458       459       460       461       462       463       464  \\\n",
       "0 -0.251865 -0.251854 -0.251843 -0.251832 -0.251821 -0.251811 -0.251800   \n",
       "1 -0.097822 -0.097817 -0.097812 -0.097807 -0.097802 -0.097798 -0.097793   \n",
       "\n",
       "        465       466       467       468       469       470       471  \\\n",
       "0 -0.251789 -0.251778 -0.251767 -0.251756 -0.251746 -0.251735 -0.251724   \n",
       "1 -0.097788 -0.097783 -0.097778 -0.097773 -0.097769 -0.097764 -0.097759   \n",
       "\n",
       "        472       473       474       475       476      477       478  \\\n",
       "0 -0.251713 -0.251703 -0.251692 -0.251681 -0.251671 -0.25166 -0.251649   \n",
       "1 -0.097754 -0.097749 -0.097744 -0.097740 -0.097735 -0.09773 -0.097725   \n",
       "\n",
       "        479       480       481       482       483       484       485  \\\n",
       "0 -0.251639 -0.251628 -0.251618 -0.251607 -0.251597 -0.251586 -0.251576   \n",
       "1 -0.097720 -0.097715 -0.097711 -0.097706 -0.097701 -0.097696 -0.097691   \n",
       "\n",
       "        486       487       488       489       490       491       492  \\\n",
       "0 -0.251565 -0.251555 -0.251544 -0.251534 -0.251524 -0.251513 -0.251503   \n",
       "1 -0.097686 -0.097682 -0.097677 -0.097672 -0.097667 -0.097662 -0.097658   \n",
       "\n",
       "        493       494       495       496       497       498       499  \\\n",
       "0 -0.251493 -0.251483 -0.251472 -0.251462 -0.251452 -0.251442 -0.251431   \n",
       "1 -0.097653 -0.097648 -0.097643 -0.097638 -0.097633 -0.097629 -0.097624   \n",
       "\n",
       "        500       501       502       503       504       505       506  \\\n",
       "0 -0.251421 -0.251411 -0.251401 -0.251391 -0.251381 -0.251371 -0.251361   \n",
       "1 -0.097619 -0.097614 -0.097609 -0.097605 -0.097600 -0.097595 -0.097590   \n",
       "\n",
       "        507       508       509       510       511       512       513  \\\n",
       "0 -0.251351 -0.251341 -0.251331 -0.251321 -0.251311 -0.251301 -0.251291   \n",
       "1 -0.097585 -0.097580 -0.097576 -0.097571 -0.097566 -0.097561 -0.097556   \n",
       "\n",
       "        514       515       516       517       518       519       520  \\\n",
       "0 -0.251281 -0.251271 -0.251261 -0.251252 -0.251242 -0.251232 -0.251222   \n",
       "1 -0.097551 -0.097547 -0.097542 -0.097537 -0.097532 -0.097527 -0.097522   \n",
       "\n",
       "        521       522       523       524       525       526       527  \\\n",
       "0 -0.251213 -0.251203 -0.251193 -0.251183 -0.251174 -0.251164 -0.251155   \n",
       "1 -0.097518 -0.097513 -0.097508 -0.097503 -0.097498 -0.097494 -0.097489   \n",
       "\n",
       "        528       529       530       531       532       533       534  \\\n",
       "0 -0.251145 -0.251135 -0.251126 -0.251116 -0.251107 -0.251097 -0.251088   \n",
       "1 -0.097484 -0.097479 -0.097474 -0.097470 -0.097465 -0.097460 -0.097455   \n",
       "\n",
       "        535       536       537       538       539       540       541  \\\n",
       "0 -0.251078 -0.251069 -0.251059 -0.251050 -0.251041 -0.251031 -0.251022   \n",
       "1 -0.097450 -0.097445 -0.097441 -0.097436 -0.097431 -0.097426 -0.097421   \n",
       "\n",
       "        542       543       544       545       546       547       548  \\\n",
       "0 -0.251013 -0.251003 -0.250994 -0.250985 -0.250975 -0.250966 -0.250957   \n",
       "1 -0.097417 -0.097412 -0.097407 -0.097402 -0.097397 -0.097392 -0.097388   \n",
       "\n",
       "        549       550       551       552       553       554       555  \\\n",
       "0 -0.250948 -0.250939 -0.250929 -0.250920 -0.250911 -0.250902 -0.250893   \n",
       "1 -0.097383 -0.097378 -0.097373 -0.097368 -0.097364 -0.097359 -0.097354   \n",
       "\n",
       "        556       557       558       559       560       561      562  \\\n",
       "0 -0.250884 -0.250875 -0.250866 -0.250857 -0.250848 -0.250839 -0.25083   \n",
       "1 -0.097349 -0.097344 -0.097340 -0.097335 -0.097330 -0.097325 -0.09732   \n",
       "\n",
       "        563       564       565       566       567       568       569  \\\n",
       "0 -0.250821 -0.250812 -0.250803 -0.250794 -0.250786 -0.250777 -0.250768   \n",
       "1 -0.097315 -0.097311 -0.097306 -0.097301 -0.097296 -0.097291 -0.097287   \n",
       "\n",
       "        570       571       572       573       574       575       576  \\\n",
       "0 -0.250759 -0.250750 -0.250742 -0.250733 -0.250724 -0.250716 -0.250707   \n",
       "1 -0.097282 -0.097277 -0.097272 -0.097267 -0.097263 -0.097258 -0.097253   \n",
       "\n",
       "        577       578       579       580       581       582       583  \\\n",
       "0 -0.250698 -0.250690 -0.250681 -0.250672 -0.250664 -0.250655 -0.250647   \n",
       "1 -0.097248 -0.097243 -0.097239 -0.097234 -0.097229 -0.097224 -0.097219   \n",
       "\n",
       "        584      585       586       587       588       589       590  \\\n",
       "0 -0.250638 -0.25063 -0.250621 -0.250613 -0.250604 -0.250596 -0.250587   \n",
       "1 -0.097215 -0.09721 -0.097205 -0.097200 -0.097195 -0.097191 -0.097186   \n",
       "\n",
       "        591       592       593       594       595       596       597  \\\n",
       "0 -0.250579 -0.250571 -0.250562 -0.250554 -0.250546 -0.250537 -0.250529   \n",
       "1 -0.097181 -0.097176 -0.097171 -0.097167 -0.097162 -0.097157 -0.097152   \n",
       "\n",
       "        598       599       600       601       602       603       604  \\\n",
       "0 -0.250521 -0.250513 -0.250504 -0.250496 -0.250488 -0.250480 -0.250472   \n",
       "1 -0.097147 -0.097143 -0.097138 -0.097133 -0.097128 -0.097124 -0.097119   \n",
       "\n",
       "        605       606       607       608       609       610       611  \\\n",
       "0 -0.250464 -0.250455 -0.250447 -0.250439 -0.250431 -0.250423 -0.250415   \n",
       "1 -0.097114 -0.097109 -0.097104 -0.097099 -0.097095 -0.097090 -0.097085   \n",
       "\n",
       "        612       613       614       615       616       617       618  \\\n",
       "0 -0.250407 -0.250399 -0.250391 -0.250383 -0.250375 -0.250367 -0.250360   \n",
       "1 -0.097080 -0.097076 -0.097071 -0.097066 -0.097061 -0.097056 -0.097052   \n",
       "\n",
       "        619       620       621       622       623       624       625  \\\n",
       "0 -0.250352 -0.250344 -0.250336 -0.250328 -0.250320 -0.250313 -0.250305   \n",
       "1 -0.097047 -0.097042 -0.097037 -0.097032 -0.097028 -0.097023 -0.097018   \n",
       "\n",
       "        626       627       628       629       630       631       632  \\\n",
       "0 -0.250297 -0.250289 -0.250282 -0.250274 -0.250266 -0.250259 -0.250251   \n",
       "1 -0.097013 -0.097009 -0.097004 -0.096999 -0.096994 -0.096990 -0.096985   \n",
       "\n",
       "        633       634       635       636       637       638       639  \\\n",
       "0 -0.250243 -0.250236 -0.250228 -0.250221 -0.250213 -0.250206 -0.250198   \n",
       "1 -0.096980 -0.096975 -0.096970 -0.096966 -0.096961 -0.096956 -0.096951   \n",
       "\n",
       "        640       641       642       643       644       645       646  \\\n",
       "0 -0.250191 -0.250183 -0.250176 -0.250168 -0.250161 -0.250154 -0.250146   \n",
       "1 -0.096946 -0.096942 -0.096937 -0.096932 -0.096927 -0.096923 -0.096918   \n",
       "\n",
       "        647       648       649       650       651       652       653  \\\n",
       "0 -0.250139 -0.250131 -0.250124 -0.250117 -0.250110 -0.250102 -0.250095   \n",
       "1 -0.096913 -0.096908 -0.096904 -0.096899 -0.096894 -0.096889 -0.096884   \n",
       "\n",
       "        654       655       656       657       658       659       660  \\\n",
       "0 -0.250088 -0.250081 -0.250073 -0.250066 -0.250059 -0.250052 -0.250045   \n",
       "1 -0.096880 -0.096875 -0.096870 -0.096865 -0.096861 -0.096856 -0.096851   \n",
       "\n",
       "        661       662       663       664       665       666       667  \\\n",
       "0 -0.250038 -0.250031 -0.250024 -0.250016 -0.250009 -0.250002 -0.249995   \n",
       "1 -0.096846 -0.096842 -0.096837 -0.096832 -0.096827 -0.096823 -0.096818   \n",
       "\n",
       "        668       669       670       671       672       673       674  \\\n",
       "0 -0.249988 -0.249981 -0.249974 -0.249968 -0.249961 -0.249954 -0.249947   \n",
       "1 -0.096813 -0.096808 -0.096803 -0.096799 -0.096794 -0.096789 -0.096784   \n",
       "\n",
       "       675       676       677       678       679       680       681  \\\n",
       "0 -0.24994 -0.249933 -0.249926 -0.249920 -0.249913 -0.249906 -0.249899   \n",
       "1 -0.09678 -0.096775 -0.096770 -0.096765 -0.096761 -0.096756 -0.096751   \n",
       "\n",
       "        682       683       684       685       686       687       688  \\\n",
       "0 -0.249892 -0.249886 -0.249879 -0.249872 -0.249866 -0.249859 -0.249852   \n",
       "1 -0.096746 -0.096742 -0.096737 -0.096732 -0.096727 -0.096723 -0.096718   \n",
       "\n",
       "        689       690       691       692       693       694       695  \\\n",
       "0 -0.249846 -0.249839 -0.249833 -0.249826 -0.249819 -0.249813 -0.249806   \n",
       "1 -0.096713 -0.096708 -0.096704 -0.096699 -0.096694 -0.096689 -0.096685   \n",
       "\n",
       "       696       697       698       699       700       701       702  \\\n",
       "0 -0.24980 -0.249793 -0.249787 -0.249780 -0.249774 -0.249768 -0.249761   \n",
       "1 -0.09668 -0.096675 -0.096670 -0.096666 -0.096661 -0.096656 -0.096651   \n",
       "\n",
       "        703       704       705       706       707       708       709  \\\n",
       "0 -0.249755 -0.249749 -0.249742 -0.249736 -0.249730 -0.249723 -0.249717   \n",
       "1 -0.096647 -0.096642 -0.096637 -0.096632 -0.096628 -0.096623 -0.096618   \n",
       "\n",
       "        710       711       712       713       714      715       716  \\\n",
       "0 -0.249711 -0.249704 -0.249698 -0.249692 -0.249686 -0.24968 -0.249673   \n",
       "1 -0.096613 -0.096609 -0.096604 -0.096599 -0.096595 -0.09659 -0.096585   \n",
       "\n",
       "        717       718       719       720       721       722       723  \\\n",
       "0 -0.249667 -0.249661 -0.249655 -0.249649 -0.249643 -0.249637 -0.249631   \n",
       "1 -0.096580 -0.096576 -0.096571 -0.096566 -0.096561 -0.096557 -0.096552   \n",
       "\n",
       "        724       725       726       727       728       729       730  \\\n",
       "0 -0.249625 -0.249619 -0.249613 -0.249607 -0.249601 -0.249595 -0.249589   \n",
       "1 -0.096547 -0.096543 -0.096538 -0.096533 -0.096528 -0.096524 -0.096519   \n",
       "\n",
       "        731       732       733       734       735       736       737  \\\n",
       "0 -0.249583 -0.249577 -0.249571 -0.249565 -0.249560 -0.249554 -0.249548   \n",
       "1 -0.096514 -0.096509 -0.096505 -0.096500 -0.096495 -0.096491 -0.096486   \n",
       "\n",
       "        738       739       740       741       742       743       744  \\\n",
       "0 -0.249542 -0.249536 -0.249531 -0.249525 -0.249519 -0.249513 -0.249508   \n",
       "1 -0.096481 -0.096476 -0.096472 -0.096467 -0.096462 -0.096458 -0.096453   \n",
       "\n",
       "        745       746       747       748       749       750       751  \\\n",
       "0 -0.249502 -0.249496 -0.249491 -0.249485 -0.249479 -0.249474 -0.249468   \n",
       "1 -0.096448 -0.096443 -0.096439 -0.096434 -0.096429 -0.096425 -0.096420   \n",
       "\n",
       "        752       753       754       755       756       757       758  \\\n",
       "0 -0.249463 -0.249457 -0.249452 -0.249446 -0.249441 -0.249435 -0.249430   \n",
       "1 -0.096415 -0.096410 -0.096406 -0.096401 -0.096396 -0.096392 -0.096387   \n",
       "\n",
       "        759       760       761       762       763       764       765  \\\n",
       "0 -0.249424 -0.249419 -0.249413 -0.249408 -0.249403 -0.249397 -0.249392   \n",
       "1 -0.096382 -0.096377 -0.096373 -0.096368 -0.096363 -0.096359 -0.096354   \n",
       "\n",
       "        766       767       768       769       770       771       772  \\\n",
       "0 -0.249386 -0.249381 -0.249376 -0.249371 -0.249365 -0.249360 -0.249355   \n",
       "1 -0.096349 -0.096345 -0.096340 -0.096335 -0.096330 -0.096326 -0.096321   \n",
       "\n",
       "        773       774       775       776       777       778       779  \\\n",
       "0 -0.249349 -0.249344 -0.249339 -0.249334 -0.249329 -0.249324 -0.249318   \n",
       "1 -0.096316 -0.096312 -0.096307 -0.096302 -0.096298 -0.096293 -0.096288   \n",
       "\n",
       "        780       781       782       783       784       785       786  \\\n",
       "0 -0.249313 -0.249308 -0.249303 -0.249298 -0.249293 -0.249288 -0.249283   \n",
       "1 -0.096284 -0.096279 -0.096274 -0.096269 -0.096265 -0.096260 -0.096255   \n",
       "\n",
       "        787       788       789       790       791       792       793  \\\n",
       "0 -0.249278 -0.249273 -0.249268 -0.249263 -0.249258 -0.249253 -0.249248   \n",
       "1 -0.096251 -0.096246 -0.096241 -0.096237 -0.096232 -0.096227 -0.096223   \n",
       "\n",
       "        794       795       796       797       798       799       800  \\\n",
       "0 -0.249243 -0.249238 -0.249233 -0.249229 -0.249224 -0.249219 -0.249214   \n",
       "1 -0.096218 -0.096213 -0.096209 -0.096204 -0.096199 -0.096195 -0.096190   \n",
       "\n",
       "        801       802       803       804       805       806       807  \\\n",
       "0 -0.249209 -0.249205 -0.249200 -0.249195 -0.249190 -0.249186 -0.249181   \n",
       "1 -0.096185 -0.096181 -0.096176 -0.096171 -0.096167 -0.096162 -0.096157   \n",
       "\n",
       "        808       809       810       811       812       813       814  \\\n",
       "0 -0.249176 -0.249171 -0.249167 -0.249162 -0.249158 -0.249153 -0.249148   \n",
       "1 -0.096153 -0.096148 -0.096143 -0.096139 -0.096134 -0.096129 -0.096125   \n",
       "\n",
       "        815       816       817       818       819       820       821  \\\n",
       "0 -0.249144 -0.249139 -0.249135 -0.249130 -0.249126 -0.249121 -0.249116   \n",
       "1 -0.096120 -0.096115 -0.096111 -0.096106 -0.096101 -0.096097 -0.096092   \n",
       "\n",
       "        822       823       824       825       826       827       828  \\\n",
       "0 -0.249112 -0.249108 -0.249103 -0.249099 -0.249094 -0.249090 -0.249085   \n",
       "1 -0.096087 -0.096083 -0.096078 -0.096073 -0.096069 -0.096064 -0.096059   \n",
       "\n",
       "        829       830       831       832       833       834       835  \\\n",
       "0 -0.249081 -0.249077 -0.249072 -0.249068 -0.249064 -0.249059 -0.249055   \n",
       "1 -0.096055 -0.096050 -0.096045 -0.096041 -0.096036 -0.096031 -0.096027   \n",
       "\n",
       "        836       837       838       839       840       841       842  \\\n",
       "0 -0.249051 -0.249047 -0.249042 -0.249038 -0.249034 -0.249030 -0.249025   \n",
       "1 -0.096022 -0.096017 -0.096013 -0.096008 -0.096004 -0.095999 -0.095994   \n",
       "\n",
       "        843       844       845       846       847       848       849  \\\n",
       "0 -0.249021 -0.249017 -0.249013 -0.249009 -0.249005 -0.249001 -0.248997   \n",
       "1 -0.095990 -0.095985 -0.095980 -0.095976 -0.095971 -0.095966 -0.095962   \n",
       "\n",
       "        850       851       852       853       854       855       856  \\\n",
       "0 -0.248993 -0.248988 -0.248984 -0.248980 -0.248976 -0.248972 -0.248968   \n",
       "1 -0.095957 -0.095953 -0.095948 -0.095943 -0.095939 -0.095934 -0.095929   \n",
       "\n",
       "        857      858       859       860       861       862       863  \\\n",
       "0 -0.248964 -0.24896 -0.248957 -0.248953 -0.248949 -0.248945 -0.248941   \n",
       "1 -0.095925 -0.09592 -0.095915 -0.095911 -0.095906 -0.095902 -0.095897   \n",
       "\n",
       "        864       865       866       867       868       869       870  \\\n",
       "0 -0.248937 -0.248933 -0.248929 -0.248926 -0.248922 -0.248918 -0.248914   \n",
       "1 -0.095892 -0.095888 -0.095883 -0.095878 -0.095874 -0.095869 -0.095865   \n",
       "\n",
       "       871       872       873       874       875       876       877  \\\n",
       "0 -0.24891 -0.248907 -0.248903 -0.248899 -0.248895 -0.248892 -0.248888   \n",
       "1 -0.09586 -0.095855 -0.095851 -0.095846 -0.095842 -0.095837 -0.095832   \n",
       "\n",
       "        878       879       880       881       882       883       884  \\\n",
       "0 -0.248884 -0.248881 -0.248877 -0.248873 -0.248870 -0.248866 -0.248863   \n",
       "1 -0.095828 -0.095823 -0.095819 -0.095814 -0.095809 -0.095805 -0.095800   \n",
       "\n",
       "        885       886       887       888       889       890       891  \\\n",
       "0 -0.248859 -0.248855 -0.248852 -0.248848 -0.248845 -0.248841 -0.248838   \n",
       "1 -0.095795 -0.095791 -0.095786 -0.095782 -0.095777 -0.095772 -0.095768   \n",
       "\n",
       "        892       893       894       895       896       897       898  \\\n",
       "0 -0.248834 -0.248831 -0.248827 -0.248824 -0.248820 -0.248817 -0.248814   \n",
       "1 -0.095763 -0.095759 -0.095754 -0.095749 -0.095745 -0.095740 -0.095736   \n",
       "\n",
       "        899       900       901       902       903       904       905  \\\n",
       "0 -0.248810 -0.248807 -0.248804 -0.248800 -0.248797 -0.248793 -0.248790   \n",
       "1 -0.095731 -0.095727 -0.095722 -0.095717 -0.095713 -0.095708 -0.095704   \n",
       "\n",
       "        906       907      908       909       910       911       912  \\\n",
       "0 -0.248787 -0.248784 -0.24878 -0.248777 -0.248774 -0.248771 -0.248767   \n",
       "1 -0.095699 -0.095694 -0.09569 -0.095685 -0.095681 -0.095676 -0.095671   \n",
       "\n",
       "        913       914       915       916       917       918       919  \\\n",
       "0 -0.248764 -0.248761 -0.248758 -0.248755 -0.248751 -0.248748 -0.248745   \n",
       "1 -0.095667 -0.095662 -0.095658 -0.095653 -0.095649 -0.095644 -0.095640   \n",
       "\n",
       "        920       921       922       923       924       925       926  \\\n",
       "0 -0.248742 -0.248739 -0.248736 -0.248733 -0.248730 -0.248727 -0.248724   \n",
       "1 -0.095635 -0.095630 -0.095626 -0.095621 -0.095617 -0.095612 -0.095608   \n",
       "\n",
       "        927       928       929       930       931       932       933  \\\n",
       "0 -0.248721 -0.248718 -0.248715 -0.248712 -0.248709 -0.248706 -0.248703   \n",
       "1 -0.095603 -0.095598 -0.095594 -0.095589 -0.095585 -0.095580 -0.095576   \n",
       "\n",
       "        934       935       936       937       938       939       940  \\\n",
       "0 -0.248700 -0.248697 -0.248694 -0.248691 -0.248688 -0.248686 -0.248683   \n",
       "1 -0.095571 -0.095566 -0.095562 -0.095557 -0.095553 -0.095548 -0.095544   \n",
       "\n",
       "        941       942       943       944       945       946       947  \\\n",
       "0 -0.248680 -0.248677 -0.248674 -0.248671 -0.248669 -0.248666 -0.248663   \n",
       "1 -0.095539 -0.095535 -0.095530 -0.095526 -0.095521 -0.095516 -0.095512   \n",
       "\n",
       "        948       949       950       951       952       953       954  \\\n",
       "0 -0.248660 -0.248658 -0.248655 -0.248652 -0.248650 -0.248647 -0.248644   \n",
       "1 -0.095507 -0.095503 -0.095498 -0.095494 -0.095489 -0.095485 -0.095480   \n",
       "\n",
       "        955       956       957       958       959       960       961  \\\n",
       "0 -0.248642 -0.248639 -0.248636 -0.248634 -0.248631 -0.248628 -0.248626   \n",
       "1 -0.095475 -0.095471 -0.095466 -0.095462 -0.095457 -0.095453 -0.095448   \n",
       "\n",
       "        962       963       964       965       966       967       968  \\\n",
       "0 -0.248623 -0.248621 -0.248618 -0.248616 -0.248613 -0.248611 -0.248608   \n",
       "1 -0.095444 -0.095439 -0.095435 -0.095430 -0.095426 -0.095421 -0.095417   \n",
       "\n",
       "        969       970       971       972       973       974       975  \\\n",
       "0 -0.248606 -0.248603 -0.248601 -0.248598 -0.248596 -0.248593 -0.248591   \n",
       "1 -0.095412 -0.095408 -0.095403 -0.095399 -0.095394 -0.095390 -0.095385   \n",
       "\n",
       "        976       977       978       979       980       981       982  \\\n",
       "0 -0.248589 -0.248586 -0.248584 -0.248581 -0.248579 -0.248577 -0.248574   \n",
       "1 -0.095380 -0.095376 -0.095371 -0.095367 -0.095362 -0.095358 -0.095353   \n",
       "\n",
       "        983       984       985       986       987       988       989  \\\n",
       "0 -0.248572 -0.248570 -0.248567 -0.248565 -0.248563 -0.248561 -0.248558   \n",
       "1 -0.095349 -0.095344 -0.095340 -0.095335 -0.095331 -0.095326 -0.095322   \n",
       "\n",
       "        990       991       992       993       994       995       996  \\\n",
       "0 -0.248556 -0.248554 -0.248552 -0.248550 -0.248547 -0.248545 -0.248543   \n",
       "1 -0.095317 -0.095313 -0.095308 -0.095304 -0.095299 -0.095295 -0.095290   \n",
       "\n",
       "        997       998       999  \n",
       "0 -0.265621 -0.278570 -0.301747  \n",
       "1  0.056804  0.051225 -0.025032  "
      ]
     },
     "execution_count": 745,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_size = 5\n",
    "random_input = np.ones((11, 1000, 5))\n",
    "random_output = model_tst.predict(random_input, batch_size=100)[0, :, :].reshape(1000, 2)\n",
    "random_input[:, 8:, :] = 99\n",
    "random_output2 = model_tst.predict(random_input, batch_size=100)[0, :, :].reshape(1000, 2)\n",
    "random_output3 = model_tst.predict(random_input, batch_size=100)[0, :, :].reshape(1000, 2)\n",
    "diff = random_output - random_output2\n",
    "pd.DataFrame(diff).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.004554</td>\n",
       "      <td>0.030177</td>\n",
       "      <td>0.061376</td>\n",
       "      <td>0.016552</td>\n",
       "      <td>0.068042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055126</td>\n",
       "      <td>0.037456</td>\n",
       "      <td>-0.051067</td>\n",
       "      <td>-0.124175</td>\n",
       "      <td>-0.058899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4         5         6         7         8         9\n",
       "0  0.0  0.0  0.0  0.0  0.0 -0.004554  0.030177  0.061376  0.016552  0.068042\n",
       "1  0.0  0.0  0.0  0.0  0.0  0.055126  0.037456 -0.051067 -0.124175 -0.058899"
      ]
     },
     "execution_count": 738,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff2 = random_output3 - random_output2\n",
    "pd.DataFrame(diff2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.7036 - mse: 0.0865\n",
      "Epoch 2/2\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.6948 - mse: 0.0823\n"
     ]
    }
   ],
   "source": [
    "history = model_tst.fit(np.random.uniform(size=(1152, 128, 53)), np.random.uniform(size=(1152, 128, 2)),\n",
    "    epochs=2,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001110</td>\n",
       "      <td>-0.008770</td>\n",
       "      <td>-0.003744</td>\n",
       "      <td>-0.013602</td>\n",
       "      <td>-0.00649</td>\n",
       "      <td>-0.016743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.020292</td>\n",
       "      <td>-0.014005</td>\n",
       "      <td>-0.016000</td>\n",
       "      <td>-0.018745</td>\n",
       "      <td>-0.01912</td>\n",
       "      <td>-0.021157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3         4         5         6         7        8  \\\n",
       "0  0.0  0.0  0.0  0.0 -0.001110 -0.008770 -0.003744 -0.013602 -0.00649   \n",
       "1  0.0  0.0  0.0  0.0 -0.020292 -0.014005 -0.016000 -0.018745 -0.01912   \n",
       "\n",
       "          9  \n",
       "0 -0.016743  \n",
       "1 -0.021157  "
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_input = np.ones((1, 10, 5))\n",
    "random_output = model_tst.predict(random_input).reshape(10, 2)\n",
    "random_input[:, 4:, :] = 99\n",
    "random_output2 = model_tst.predict(random_input).reshape(10, 2)\n",
    "diff = random_output - random_output2\n",
    "pd.DataFrame(diff).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 2)"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(size=(1, 10, 2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_158 (LSTM)             (None, 10, 2)             64        \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 10, 2)             6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 70\n",
      "Trainable params: 70\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-10 17:32:37.604123: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-10 17:32:37.644558: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-10 17:32:37.858138: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-10 17:32:37.910389: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 767,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input_shape = (10, 5)\n",
    "\n",
    "model_tst = tf.keras.Sequential()\n",
    "\n",
    "model_tst.add(tf.keras.Input(shape=input_shape))\n",
    "model_tst.add(tf.keras.layers.LSTM(2,  return_sequences=True))\n",
    "model_tst.add(tf.keras.layers.Dense(2, activation=\"sigmoid\"))\n",
    "\n",
    "model_tst.summary()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "model_tst.compile(\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        # metrics=[tf.keras.metrics.BinaryCrossentropy()\n",
    "        metrics=[\"mse\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "random_input = np.ones((11, 10, 5))\n",
    "# random_input = np.random.uniform(size=(11, 10, 5))\n",
    "random_input[:, 8:, :] = 99\n",
    "random_output2 = model_tst.predict(random_input, batch_size=1)[0, :, :].reshape(10, 2)\n",
    "random_output3 = model_tst.predict(random_input, batch_size=10)[0, :, :].reshape(10, 2)\n",
    "diff2 = random_output3 - random_output2\n",
    "pd.DataFrame(diff2).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038585</td>\n",
       "      <td>0.075018</td>\n",
       "      <td>0.099508</td>\n",
       "      <td>0.113639</td>\n",
       "      <td>0.120910</td>\n",
       "      <td>0.124124</td>\n",
       "      <td>0.125129</td>\n",
       "      <td>0.125047</td>\n",
       "      <td>-0.006011</td>\n",
       "      <td>-0.134312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025758</td>\n",
       "      <td>0.054718</td>\n",
       "      <td>0.080744</td>\n",
       "      <td>0.102315</td>\n",
       "      <td>0.119503</td>\n",
       "      <td>0.132902</td>\n",
       "      <td>0.143210</td>\n",
       "      <td>0.151065</td>\n",
       "      <td>0.243405</td>\n",
       "      <td>0.303306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.038585  0.075018  0.099508  0.113639  0.120910  0.124124  0.125129   \n",
       "1  0.025758  0.054718  0.080744  0.102315  0.119503  0.132902  0.143210   \n",
       "\n",
       "          7         8         9  \n",
       "0  0.125047 -0.006011 -0.134312  \n",
       "1  0.151065  0.243405  0.303306  "
      ]
     },
     "execution_count": 764,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(out_batch_1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038585</td>\n",
       "      <td>0.075018</td>\n",
       "      <td>0.099508</td>\n",
       "      <td>0.113639</td>\n",
       "      <td>0.120910</td>\n",
       "      <td>0.112897</td>\n",
       "      <td>0.087879</td>\n",
       "      <td>0.096757</td>\n",
       "      <td>0.114014</td>\n",
       "      <td>0.181670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025758</td>\n",
       "      <td>0.054718</td>\n",
       "      <td>0.080744</td>\n",
       "      <td>0.102315</td>\n",
       "      <td>0.119503</td>\n",
       "      <td>0.128028</td>\n",
       "      <td>0.149749</td>\n",
       "      <td>0.158353</td>\n",
       "      <td>0.196497</td>\n",
       "      <td>0.214949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.038585  0.075018  0.099508  0.113639  0.120910  0.112897  0.087879   \n",
       "1  0.025758  0.054718  0.080744  0.102315  0.119503  0.128028  0.149749   \n",
       "\n",
       "          7         8         9  \n",
       "0  0.096757  0.114014  0.181670  \n",
       "1  0.158353  0.196497  0.214949  "
      ]
     },
     "execution_count": 765,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(out_batch_10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_79\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_174 (LSTM)             (None, 10, 2)             64        \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 10, 2)             6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 70\n",
      "Trainable params: 70\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-10 19:02:02.662990: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-10 19:02:02.698816: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-10 19:02:02.908405: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-10 19:02:02.950191: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Setup model\n",
    "input_shape = (10, 5)\n",
    "\n",
    "model_tst = tf.keras.Sequential()\n",
    "\n",
    "model_tst.add(tf.keras.Input(shape=input_shape))\n",
    "model_tst.add(tf.keras.layers.LSTM(100,  return_sequences=True))\n",
    "model_tst.add(tf.keras.layers.Dense(2, activation=\"sigmoid\"))\n",
    "\n",
    "model_tst.summary()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "model_tst.compile(\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        # metrics=[tf.keras.metrics.BinaryCrossentropy()\n",
    "        metrics=[\"mse\"\n",
    "        ]\n",
    ")\n",
    "\n",
    "# Generate step data\n",
    "random_input = np.ones((11, 10, 5))\n",
    "random_input[:, 8:, :] = 2\n",
    "\n",
    "# Predictions\n",
    "random_output2 = model_tst.predict(random_input, batch_size=1)[0, :, :].reshape(10, 2)\n",
    "random_output3 = model_tst.predict(random_input, batch_size=10)[0, :, :].reshape(10, 2)\n",
    "\n",
    "# Compare results\n",
    "diff2 = random_output3 - random_output2\n",
    "pd.DataFrame(diff2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 780,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Visible devices cannot be modified after being initialized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sw/31vsr7sd3dggcqkkgn59gsl80000gn/T/ipykernel_87118/3585259081.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Disable all GPUS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvisible_devices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_visible_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvisible_devices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_type\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'GPU'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/sandbox_tf/lib/python3.8/site-packages/tensorflow/python/framework/config.py\u001b[0m in \u001b[0;36mset_visible_devices\u001b[0;34m(devices, device_type)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRuntime\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0malready\u001b[0m \u001b[0minitialized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m   \"\"\"\n\u001b[0;32m--> 528\u001b[0;31m   \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/sandbox_tf/lib/python3.8/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mset_visible_devices\u001b[0;34m(self, devices, device_type)\u001b[0m\n\u001b[1;32m   1513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m       raise RuntimeError(\n\u001b[0m\u001b[1;32m   1516\u001b[0m           \"Visible devices cannot be modified after being initialized\")\n\u001b[1;32m   1517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Visible devices cannot be modified after being initialized"
     ]
    }
   ],
   "source": [
    "\n",
    "# Disable all GPUS\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "visible_devices = tf.config.get_visible_devices()\n",
    "for device in visible_devices:\n",
    "    assert device.device_type != 'GPU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa35a10c8249ed40525d870f50fb781eeb2f83c906116e51c5b71c9b6b7a9416"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('sandbox_tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
